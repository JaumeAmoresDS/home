[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jaume Amores",
    "section": "",
    "text": "Hello, and welcome to my home page! I currently work as a Principal Data Scientist in Johnson Controls. I have been working on Data Science for more than 20 years, in both academia and industry. I received a Ph.D. in Machine Learning and Computer vision in 2006, and have been in industry for more than 10 years, working both as individual contributor and technical lead in multi-disciplinary teams. I enjoy both the theoretical and practical aspects of Data Science, I like developing high quality software based on best practices, and have experience generating intellectual property.\nApart from my work, I am a dad of a little child and I am interested in early childhood education and health. I intend to write about those subjects in my blog posts, above."
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Jaume Amores",
    "section": "",
    "text": "Hello, and welcome to my home page! I currently work as a Principal Data Scientist in Johnson Controls. I have been working on Data Science for more than 20 years, in both academia and industry. I received a Ph.D. in Machine Learning and Computer vision in 2006, and have been in industry for more than 10 years, working both as individual contributor and technical lead in multi-disciplinary teams. I enjoy both the theoretical and practical aspects of Data Science, I like developing high quality software based on best practices, and have experience generating intellectual property.\nApart from my work, I am a dad of a little child and I am interested in early childhood education and health. I intend to write about those subjects in my blog posts, above."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Jaume Amores",
    "section": "Education",
    "text": "Education\n\nPh.D. in Machine Learning and Computer Vision | Universitat Autonoma de Barcelona | 2006\nM.Sc. in Machine Learning and Computer Vision | Universitat Autonoma de Barcelona | 2003\nB.Sc. in Computer Science | Universitat de Valencia | 2000"
  },
  {
    "objectID": "index.html#open-source",
    "href": "index.html#open-source",
    "title": "Jaume Amores",
    "section": "Open Source",
    "text": "Open Source\nAt the moment I have published one open source library written in python, based on the nbdev+quarto publishing framework. I intend to publish more projects in the future.\n\nnbmodular: link Convert data science notebooks with poor modularity to fully modular notebooks that are automatically exported as python modules."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Hello and welcome to my blog posts. In this website I will be publishing a series of blogs, mostly on health, data science, early childhood education, and software engineering. The blogs will be originally mostly personal notes, which I intend to polish and extend during successive reviews."
  },
  {
    "objectID": "posts.html#introduction",
    "href": "posts.html#introduction",
    "title": "Posts",
    "section": "",
    "text": "Hello and welcome to my blog posts. In this website I will be publishing a series of blogs, mostly on health, data science, early childhood education, and software engineering. The blogs will be originally mostly personal notes, which I intend to polish and extend during successive reviews."
  },
  {
    "objectID": "posts.html#posts",
    "href": "posts.html#posts",
    "title": "Posts",
    "section": "Posts",
    "text": "Posts"
  },
  {
    "objectID": "posts/others/index.html",
    "href": "posts/others/index.html",
    "title": "Other topics",
    "section": "",
    "text": "Podcasts\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/education/project_driven_learning.html",
    "href": "posts/education/project_driven_learning.html",
    "title": "Project-driven learning",
    "section": "",
    "text": "Note: this post is, at this moment, just a draft in progress.\nThe idea of writing this post came initially as a sort-of response / contribution to the excellent essay from Daniel Higginbotham entitled Techniques for Efficiently Learning Programming Languages. Despite the title of the essay, the ideas inside are very much generic for learning to learn basically anything. I will be adding a slighly new perspective which incorporates a “project-driven” approach to learning. This idea is not novel, and can be applied either in conjunction or as an alternative to the ideas expressed in post by D. Higginbotham, depending on what our final goals are. I will also use ideas such as Exploration vs Exploitation from the field of Reinforcement Learning, which I will be explaning for anyone not familiar in this field.\nThe gist of project-based learning is to focus our learning journey on those things that will prove to be important in our daily practice or life in general. It is motivated in part by the fact that, many times, when we learn a new field, e.g., in college, we study many concepts that we will never be using or needing in anyway, and end up completely forgotten. To be more concrete, let us take the field of mathematics as an example.\nGiven the vast amount of things that are actually interesting and relevant for our lives, this approach can be considered sub-optimal in many cases, making us waste a lot of time on topics that are not even connected to those parts that are relevant to us. Having said that, as I explore in this post, it all depends on what the final goal in our learning adventure. For this, I will be using a framework based on the Exploration vs Exploitation trade-off from Reinforcement Learning.\nThe gist of it is that, many times, when we start a learning"
  },
  {
    "objectID": "posts/education/resources_early_childhood_education.html",
    "href": "posts/education/resources_early_childhood_education.html",
    "title": "Resources on Parenting & Early Childhood Education",
    "section": "",
    "text": "The following provides a compact listing about resources and material I found interesting, on both parenting and early childhood education. This includes books, blogs, and podcasts. The intent is to make use of those resources, plus notes collectedfrom our parenting coach, as a source of material for subsequent posts on the topic of parenting and early childhood education. Some of the books are still in my to-read list, which I indicate by an un-checked box. Books in the “in-progress” list are colored in purple, and they are left unchecked if I read just a few chapters."
  },
  {
    "objectID": "posts/education/resources_early_childhood_education.html#parenting",
    "href": "posts/education/resources_early_childhood_education.html#parenting",
    "title": "Resources on Parenting & Early Childhood Education",
    "section": "Parenting",
    "text": "Parenting\n\nBooks\n\nThe Whole-Brain Child: 12 Revolutionary Strategies to Nurture Your Child’s Developing Mind by Daniel J. Siegel and Tina Payne Bryson\nBrain Rules for Baby (Updated and Expanded): How to Raise a Smart and Happy Baby Child from Zero to Five by John Medina\nHow Children Succeed: Grit, Curiosity, and the Hidden Power of Character by Paul Tough\nHow to Talk So Kids Will Listen & Listen So Kids Will Talk by Adele Faber and Elaine Mazlish\nNo-Drama Discipline. The Whole-Brain Way to Calm the Chaos and Nurture Your Child’s Developing Mind by Daniel J. Siegel and Tina Payne Bryson\nParenting from the Inside Out. How A Deeper Self-Understanding Can Help You Raise Children Who Thrive by Daniel J. Siegel and Mary Hartzell\nHow To Get Kids To Say Yes!: Using the Secret Four Color Languages to Get Kids to Listen by\n\n\n\nBlogs and websites\n\n50 OF THE BEST BOOKS FOR PARENTS\nPathways\nResources from Erikson Institute"
  },
  {
    "objectID": "posts/education/resources_early_childhood_education.html#early-childhood-education",
    "href": "posts/education/resources_early_childhood_education.html#early-childhood-education",
    "title": "Resources on Parenting & Early Childhood Education",
    "section": "Early Childhood Education",
    "text": "Early Childhood Education\n\nBooks\n\nWhere’s the Math? Books, Games, and Routines to Spark Children’s Thinking\nTools of the Mind: The Vygotskian Approach to Early Childhood Education (2nd Edition) by elena Bodrova and Deorah J. Leong\nWho Am I in the Lives of Children? An Introduction to Early Childhood Education (10th Edition) by Stephanie Feeney and Eva Moravick\nMind in the Making: The Seven Essential Life Skills Every Child Needs by Ellen Galinsky\nThe Complete Resource Book for Preschoolers: Over 2000 Activities and Ideas (Complete Resource Series) by Pam Schiller and Kay Hastings\n\n\n\nBlogs and websites\n\nEarly Math Collaborative from Erikson Institute\nTHE 40 BEST BOOKS ON EARLY CHILDHOOD EDUCATION"
  },
  {
    "objectID": "posts/education/resources_early_childhood_education.html#general-education",
    "href": "posts/education/resources_early_childhood_education.html#general-education",
    "title": "Resources on Parenting & Early Childhood Education",
    "section": "General Education",
    "text": "General Education\n\nMath Education\n\nBooks\n\nHow I Wish I’d Taught Maths: Lessons learned from research, conversations with experts, and 12 years of mistakes: Reflections on research, conversations with experts, and 12 years of mistakes.\nMathematics for human flourishing\nMathematical mindsets\nChange Is the Only Constant: The Wisdom of Calculus in a Madcap World\nMath with bad drawings\n\n\n\nBlogs and websites\n\n3Blue1Brown"
  },
  {
    "objectID": "posts/education/resources_early_childhood_education.html#learning-to-learn",
    "href": "posts/education/resources_early_childhood_education.html#learning-to-learn",
    "title": "Resources on Parenting & Early Childhood Education",
    "section": "Learning to learn",
    "text": "Learning to learn\n\nBooks\n\nMake It Stick: The Science of Successful Learning by Henry L. Roediger III, Mark A. McDaniel, Peter C. Brown\n\n\n\nBlogs and websites\n\nTechniques for Efficiently Learning Programming Languages. Despite the title, this post provides many useful tips for learning to learn in general.\nRecommendations about how to learn mathematics in an enjoyable way"
  },
  {
    "objectID": "posts/education/parenting_coach.html",
    "href": "posts/education/parenting_coach.html",
    "title": "Parenting coach notes",
    "section": "",
    "text": "Note: this post is just a draft in progress. As of now, it consists of a collection of random notes.\nSome months ago we started to attend parenting coach sessions that would help us learn a suit of strategies for early-childhood parenting that we would like to apply to our son, who is three years old at the moment. This post summarizes some of these strategies and other lessons gathered from those sessions."
  },
  {
    "objectID": "posts/education/parenting_coach.html#following-a-routine",
    "href": "posts/education/parenting_coach.html#following-a-routine",
    "title": "Parenting coach notes",
    "section": "Following a routine",
    "text": "Following a routine\n“I see to it that my child does the following:”\n\n\n\nIn the morning\nIn the evening\n\n\n\n\nDress up\nWash hands\n\n\nGo to the car\nTidy up\n\n\n…\nHave dinner"
  },
  {
    "objectID": "posts/education/parenting_coach.html#general-guidelines",
    "href": "posts/education/parenting_coach.html#general-guidelines",
    "title": "Parenting coach notes",
    "section": "General guidelines",
    "text": "General guidelines\n\nPrevention is better than cure. For instance, if we see that the child might do something wrong, instead of waiting and checking if they actually end up doing it, we might avoid it altogether.\nAvoid non-positive attention-seeking mechanisms. For instance, when asking the child to dress-up, it is important to not ask it repeatedly, or otherwise they might just not do it as a way of obtaining a type of attention that we don’t want to encourage, in which case they might happily ignore our request and expect us to be paying this attention continuously. Instead, a better strategy is to say something like “While you dress up I will be getting ready myself. If you have any difficulties while dressing up, please let me know and I can help you, but I would like to see that you try to do it by yourself first”. It is very important that, even when things are a bit challenging, they are encouraged to try them a few times before seeking help."
  },
  {
    "objectID": "posts/education/parenting_coach.html#q-a",
    "href": "posts/education/parenting_coach.html#q-a",
    "title": "Parenting coach notes",
    "section": "Q & A",
    "text": "Q & A\nTo better reflect the parenting sessions, each one of the discussed topics is placed in a separate a sub-section, and within it we write a Q & A expressed as a dialogue between parents and coach. Parent questions or observations are indicated with P while coach responses with C.\n\nHitting\n\nP: My child is hitting me a lot lately. It seems as if he doesn’t control his emotions.\nC: When this happens, the first thing we need to ask is whether something changed at school or in other domain that might have affected him in some way.\nP: He’s not saying anything, nor his teachers. We are concerned that we wouldn’t notice if something happened at school. One reason for that concern is that all the staff has recently changed and the new staff is not communicating so much.\nC: If he had a behaviour issue, your teachers would certainly tell you. (Then I remember that one teacher told me about one event when our son pushed another kid). I wouldn’t worry about that.\nC: Regarding your initial question, you need to set your boundaries early in the day. Something like “I’m not going to allow you to hit me anymore”. Say it in an “I love you” manner. Use the last example when that happened. We tell him that this is ok because it happened yesterday (for example), so he feels safe when we talk about this. Then we can say something like “I don’t want that because it hurts me.”\n\nThen, if we see the child is going to hit us, we need to stop him/her before that happens (note for self: this falls under the “prevention is better than cure” general guideline, mentioned above). While we stop him, we can say something like - “Hang on, remember we spoke about this early today? I’m not going to allow you to hit me.”\nIf the child stops trying to hit, then we can the following type of dialogue (just an example situation) with our child:\n- What is it that you need?\n- I want to have yoghurt!\n- Hang on, I see. Let's talk about that without hitting. When was the last time you had one?\n- I had it this morning.\n- That's great, you can have it tomorrow morning.\nThis type of dialogue makes the child learn to rationalize cause and effect.\nThe previous dialogue can happen if the child stops to trying to hit. However, they don’t stop, it is better that we leave the room so that they don’t get the opportunity to do it. We say that we are going to leave because we don’t want to allow them to hit us. By doing so we are not allowing the negativity of the behaviour to last for long time.\n\n\nChild talking bad about school\nP: Our son always talks in a negative way about school.\nC: First of all, we have to be careful that there’s nothing concerning at school. Tell the teachers about his feeling and see what they say about this. After that, we also need to be careful to not be continuously asking our child whether or not he had a good day or, more specifically, if there was any issue at school that day. Especially, we need to avoid asking about in a way that provides an image of us being concerned. This might make our child feel uncomfortable, like investigated, and this doesn’t help them having an honest and open communication.\nInstead of that we could simply ask something general like “how was your day” and, after they respond, we can say something like “Let me tell you how was my day” so that they don’t feel the conversation is just about them or about investigating if there was any issue that day.\nRather than investigating on potential negative things, it’s better to talk about positive things. We can ask what activities he had or what games he played and then ask specific details about those (“What was this painting about?” “Do you remember what colors did you use?” or “What animals there were in that jig-saw?” This helps them practice and strengthen their memory, something especially important at early ages."
  },
  {
    "objectID": "posts/health/physical_health.html",
    "href": "posts/health/physical_health.html",
    "title": "Healthspan and lifespan",
    "section": "",
    "text": "Findings regarding best protocols on health."
  },
  {
    "objectID": "posts/health/physical_health.html#sleep",
    "href": "posts/health/physical_health.html#sleep",
    "title": "Healthspan and lifespan",
    "section": "Sleep",
    "text": "Sleep\n\nThe single most important thing seems to be sleeping well, close to eight hours per day.\nCoffee, while having its benefits, needs to be avoided from noon. Ideally, the latest coffee should be taken no later than twelve hours before going to sleep."
  },
  {
    "objectID": "posts/health/physical_health.html#nutrition",
    "href": "posts/health/physical_health.html#nutrition",
    "title": "Healthspan and lifespan",
    "section": "Nutrition",
    "text": "Nutrition\n\nFasting\n\nIntermitent fasting. Specifically, time-restricted fasting, although there are other appropriate forms of intermitent fasting.\nFasting up to one week, three times per year, with keto diet the week before and the week after.\n\nIf fasting is done for more than three days, we might have our sleep affected. To avoid that, we need to take some supplements. I need to find their name.\n\nFasting for up to 48 hours once per month.\n\n\n\nDiet: plant-based with fish\n\nSpecifically, salmon and other blue fishes rich in omega-3\nIt seems to be slightly better than a pure vegan diet, which in turn is better than a meat-based western diet."
  },
  {
    "objectID": "posts/health/physical_health.html#nutrition-to-keep-guts-microbiome-healthy.",
    "href": "posts/health/physical_health.html#nutrition-to-keep-guts-microbiome-healthy.",
    "title": "Healthspan and lifespan",
    "section": "Nutrition to keep gut’s microbiome healthy.",
    "text": "Nutrition to keep gut’s microbiome healthy."
  },
  {
    "objectID": "posts/health/physical_health.html#exercise",
    "href": "posts/health/physical_health.html#exercise",
    "title": "Healthspan and lifespan",
    "section": "Exercise",
    "text": "Exercise\n\nCardio\n\nCardio seems to be the most important exercise of all.\nIn order to avoid removing the benefits of doing cardio, we need to stay on stand-up position at least 50% of the day, rather than sitting. For office jobs, a standing desk is advisable.\nCardio can be done by running, using air-bike, or using stairs, among others.\nWe need to arrive to Zone 2, several days per week. A proxy for this is to achieve the maximum intensity while still being able to keep a conversation while doing the exercise.\nWe need to have a high-intensity exercise at least 10 minutes? (I need to check this) per week. This is exercise would be the next level to zone 2, i.e., not being able to have a conversation while doing it.\n\n\nAvoiding injuries while running\n\nWarmup before running: exercises.\nStrecth after running: exercises\nDo knee exercises\nStart running progressively: plan"
  },
  {
    "objectID": "posts/health/index.html",
    "href": "posts/health/index.html",
    "title": "Health",
    "section": "",
    "text": "Tenacity and will power\n\n\nPotential strategies\n\n\n\n\n\n\n\n\n\n\nTenacity and will power TIL\n\n\nLessons from Huberman podcast\n\n\n\n\n\n\n\n\n\n\nHealthspan and lifespan\n\n\nPhysical health: resources and protocols\n\n\n\n\n\n\n\n\n\n\nAgency and gratitude\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/data_science/hello_world.html",
    "href": "posts/data_science/hello_world.html",
    "title": "Hello World AML pipeline with component",
    "section": "",
    "text": "The purpose of this tutorial is to show how to incrementally build a pipeline from simple and debuggable “Hello World” components. In order to run this at home, you may find it useful to create a free Azure ML subscription as described in the how-to guide I have here\n\n\nIn this section, we will write our code in a notebook and make sure it works well. Then, we will convert it to a script and run it from the terminal. Finally, we will add some logs with MLFlow.\nAlthough not required, as a very first step we can create a environment and kernel following the tutorial in https://learn.microsoft.com/en-gb/azure/machine-learning/tutorial-cloud-workstation?view=azureml-api-2\nFollowing the previous tutorial, create a notebook and type the following hello world code:\n\ndef hello_world (name):\n    \"\"\"Greets the indicated person and the world in general.\"\"\"\n    \n    print (f\"Hello {name} and world\")\n\nhello_world (\"Jaume\")\n\nHello Jaume and world\n\n\nFantastic, the code works ;-). Now, let’s convert it to a script that can be run from terminal. The tutorial above explains how to convert the notebook to a python file. In our case, we will first add an argument parser and then write it to file using the magic cell %%writefile\n\n%%writefile hello_world_core.py\nimport argparse\n\ndef hello_world (name):\n    \"\"\"Greets the indicated person and the world in general.\"\"\"\n    \n    print (f\"Hello {name} and world\")\n\ndef parse_args ():\n    \"\"\"Parses input arguments\"\"\"\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--name\", type=str, help=\"person to greet\")\n    args = parser.parse_args()\n    \n    return args\n\ndef main():\n    \"\"\"Main function of the script.\"\"\"\n    \n    args = parse_args ()\n    hello_world (args.name)\n\nif __name__ == \"__main__\":\n    main()\n\nOverwriting hello_world_core.py\n\n\nNow, we can open up a terminal, as illustrated in the tutorial above, cd to the folder where the script is and run it:\ncd  Users/&lt;my_user&gt;/hello_world\npython hello_world_core.py --name Jaume\n\n\n\n%%writefile hello_world_with_logs.py\nimport mlflow\nfrom hello_world_core import hello_world, parse_args\n\ndef start_logging (args):\n    # set name for logging\n    mlflow.set_experiment(\"Hello World with logging\")\n    mlflow.start_run()\n    mlflow.log_param (\"name to log\", args.name)\n    \ndef finish_logging ():\n    mlflow.end_run ()\n\ndef main():\n    \"\"\"Main function of the script.\"\"\"\n    args = parse_args ()\n    start_logging (args)\n    hello_world (args.name)\n    finish_logging ()\n\nif __name__ == \"__main__\":\n    main()\n\nOverwriting hello_world_with_logs.py\n\n\nLet’s run it and see:\npython hello_world_with_logs.py --name Peter\nHere is the newly created job:\n\nAnd the name passed as argument:\n\nWe start by getting a connection to our Azure ML (AML for short) workspace. We use here a simple connection mechanism that doesn’t require writting your subscription, resource group and workspace details:\n\nfrom azure.ai.ml import MLClient\nfrom azure.identity import DefaultAzureCredential\n\n# authenticate\ncredential = DefaultAzureCredential()\n\n# Get a handle to the workspace\nml_client = MLClient.from_config (\n    credential=credential\n)\n\nFound the config file in: /config.json\n\n\n\n\n\n\nWe now convert the previous script into a job that can be run from the UI.\n\n\n\n# Standard imports\nimport os\n\n# Third-party imports\nimport pandas as pd\n\n# AML imports\nfrom azure.ai.ml import (\n    command,\n    dsl,\n    Input,\n    Output,\n    MLClient\n)\nfrom azure.identity import DefaultAzureCredential\n\n\n\n\nFor the remaining part of this tutorial, we will be needing an ml_client handle. This will allow us to create and use resources from our workspace. The simplest way to get such handle is with the following code:\n\n# authenticate\ncredential = DefaultAzureCredential()\n\n# Get a handle to the workspace\nml_client = MLClient.from_config (\n    credential=credential\n)\n\nFound the config file in: /config.json\n\n\n\n\n\nWe specify a job using the command decorator:\n\njob = command(\n    inputs=dict(\n        name=\"Jaume\", # default value of our parameter\n    ),\n    code=f\"./\",  # location of source code: in this case, the root folder\n    command=\"python hello_world_core.py --name ${{inputs.name}}\",\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"Simplest Hello World\",\n)\n\n\nNote: we indicate as environment “AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest”, which actually contains more libraries than we need, such as sklearn. Simpler environments to use can be found in the “Environments” section of the workspace.\n\n… and submit it using create_or_update from ml_client:\n\nml_client.create_or_update(job)\n\nClass AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nUploading hello_world (8.59 MBs): 100%|██████████| 8591281/8591281 [00:00&lt;00:00, 40584539.30it/s]\n\n\n\n\n\n\n\n\nExperiment\nName\nType\nStatus\nDetails Page\n\n\nhello_world\nclever_spade_sq4jwcg67r\ncommand\nStarting\nLink to Azure Machine Learning studio\n\n\n\n\n\n\nIn the link that appears, we can see the status of the job, which initially is “Queued”. We need to wait until it is completed (and refresh the page to see this). Once it is completed, we can look at the logs:\n\nIn the logs, we can see the messages printed in console:\n\n\n\n\nAbove, we indicated a default value for the input argument name. It would be good to be able to submit jobs with different values for that argument. One way to do that is:\n\nIn the job’s Overview tab, click on “Edit and submit”\n\n\n\nIn the “Training script” section, edit the “Inputs” by clicking on the pencil next to it:\n\n\n\nIn the “Input value” field, type the new value you want for the argument:\n\n\n\nHit Next several times and then Submit.\nIf we go to the jobs section of the workspace, and enter again our job (“helloworld”), we can see that a new job has been submitted:\n\n\nIn its Overview tab, under “See all properties”, we can inspect the json file:\n\n… and see that the new value (Peter) is used in its “parameters” dictionary:\n\nThe std_log.txt for this job shows the new message with Peter:\n\n\n\n\n\n\nhello_world_component = ml_client.create_or_update(job.component)\n\nUploading hello_world (8.58 MBs): 100%|██████████| 8578531/8578531 [00:00&lt;00:00, 21700197.27it/s]\n\n\n\n\n\n# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n@dsl.pipeline(\n    compute=\"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n    description=\"E2E hello world pipeline\",\n)\ndef hello_world_pipeline(\n    pipeline_job_input: str,\n):\n    \"\"\"\n    Hello World pipeline\n    \n    Parameters\n    ----------\n    pipeline_job_input: str\n        Input to pipeline, here name of person to greed.\n    \"\"\"\n    # using data_prep_function like a python call with its own inputs\n    hello_world_job = hello_world_component(\n        name=pipeline_job_input,\n    )\n\n\n# Let's instantiate the pipeline with the parameters of our choice\npipeline = hello_world_pipeline(\n    pipeline_job_input=\"David\",\n)\n\n\npipeline_job = ml_client.jobs.create_or_update(\n    pipeline,\n    # Project's name\n    experiment_name=\"e2e_registered_components\",\n)\nml_client.jobs.stream(pipeline_job.name)\n\nClass AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n\n\nRunId: shy_cabbage_xb9vv4fswl\nWeb View: https://ml.azure.com/runs/shy_cabbage_xb9vv4fswl?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2024-03-26 14:08:19Z] Submitting 1 runs, first five are: 605cf9a7:d9904e2d-3ecb-4ddc-a04d-e2fed4facfe6\n[2024-03-26 14:12:40Z] Completing processing run id d9904e2d-3ecb-4ddc-a04d-e2fed4facfe6.\n\nExecution Summary\n=================\nRunId: shy_cabbage_xb9vv4fswl\nWeb View: https://ml.azure.com/runs/shy_cabbage_xb9vv4fswl?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\n\n\n\n\n\n\njob = command(\n    inputs=dict(\n        name=Input (type=\"string\"),\n    ),\n    code=f\"./\",  # location of source code: in this case, the root folder\n    command=\"python hello_world_core.py --name ${{inputs.name}}\",\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"Hello World witn Input\",\n)\n\nhello_world_component = ml_client.create_or_update(job.component)\n\nUploading hello_world (8.59 MBs): 100%|██████████| 8589758/8589758 [00:00&lt;00:00, 24178345.78it/s]\n\n\n\n\n\n# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\nfrom azure.ai.ml import dsl\n\n@dsl.pipeline(\n    compute=\"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n    description=\"E2E hello world pipeline with input\",\n)\ndef hello_world_pipeline(\n    pipeline_job_input: str,\n):\n    \"\"\"\n    Hello World pipeline\n    \n    Parameters\n    ----------\n    pipeline_job_input: str\n        Input to pipeline, here name of person to greed.\n    \"\"\"\n    # using data_prep_function like a python call with its own inputs\n    hello_world_job = hello_world_component(\n        name=pipeline_job_input,\n    )\n\n\npipeline = hello_world_pipeline(\n    pipeline_job_input=\"Joseph\",\n)\n\npipeline_job = ml_client.jobs.create_or_update(\n    pipeline,\n    # Project's name\n    experiment_name=\"e2e_hello_world_with_input\",\n)\nml_client.jobs.stream(pipeline_job.name)\n\nRunId: olive_plastic_gvnjy01b5s\nWeb View: https://ml.azure.com/runs/olive_plastic_gvnjy01b5s?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2024-03-26 14:38:43Z] Submitting 1 runs, first five are: cd1599c4:ce24c41e-946d-48cd-99b2-70ebde3befb2\n[2024-03-26 14:44:58Z] Completing processing run id ce24c41e-946d-48cd-99b2-70ebde3befb2.\n\nExecution Summary\n=================\nRunId: olive_plastic_gvnjy01b5s\nWeb View: https://ml.azure.com/runs/olive_plastic_gvnjy01b5s?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\n\n\nNotes about Input:\n\nWhen using Input(type=“uri_folder”) or Input(type=“uri_file”), the value passed cannot be a string, it must be an Input type, for example:\n\njob = command(\n    inputs=dict(\n        file_name=Input (type=\"uri_file\"),\n    ),\n    ...\n)\n\npipeline = hello_world_pipeline(\n    pipeline_job_input=Input(path=\"/path/to/file\"),\n)\n\nHowever, when using Input(type=“string”) or Input(type=“number”), the input must be a string or number, not Input\n\njob = command(\n    inputs=dict(\n        name=Input (type=\"string\"),\n    ),\n    ...\n)\n\npipeline = hello_world_pipeline(\n    pipeline_job_input=\"Mary\",\n)\n\nIn the latter case, the input does not appear in the graph of the pipeline, in the UI.\n\n\n\n\n\n# Component definition and registration\njob = command(\n    inputs=dict(\n        name=Input (type=\"uri_file\"),\n    ),\n    code=f\"./\",  # location of source code: in this case, the root folder\n    command=\"python hello_world_core.py --name ${{inputs.name}}\",\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"Hello World with uri_file\",\n)\nhello_world_component = ml_client.create_or_update(job.component)\n\n# Pipeline definition and registration\n@dsl.pipeline(\n    compute=\"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n    description=\"E2E hello world pipeline with input\",\n)\ndef hello_world_pipeline(\n    pipeline_job_input: str,\n):\n    \"\"\"\n    Hello World pipeline\n    \n    Parameters\n    ----------\n    pipeline_job_data_input: str\n        Input to pipeline, here path to file.\n    \"\"\"\n    # using data_prep_function like a python call with its own inputs\n    hello_world_job = hello_world_component(\n        name=pipeline_job_input,\n    )\n\npipeline = hello_world_pipeline(\n    pipeline_job_input=Input(type=\"uri_file\", path=\"./hello_world_core.py\"),\n)\n\npipeline_job = ml_client.jobs.create_or_update(\n    pipeline,\n    # Project's name\n    experiment_name=\"e2e_hello_world_with_uri_file\",\n)\n\n# Pipeline running\nml_client.jobs.stream(pipeline_job.name)\n\nUploading hello_world (8.59 MBs): 100%|██████████| 8588206/8588206 [00:00&lt;00:00, 24482901.98it/s]\n\n\nUploading hello_world_core.py (&lt; 1 MB): 0.00B [00:00, ?B/s] (&lt; 1 MB): 100%|██████████| 514/514 [00:00&lt;00:00, 12.0kB/s]\n\n\n\n\nRunId: great_tail_pw48pry0lj\nWeb View: https://ml.azure.com/runs/great_tail_pw48pry0lj?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2024-03-26 15:06:11Z] Submitting 1 runs, first five are: a08118c5:2099b6ad-fb3a-4cac-9557-c8cf355b8b1b\n[2024-03-26 15:11:50Z] Completing processing run id 2099b6ad-fb3a-4cac-9557-c8cf355b8b1b.\n\nExecution Summary\n=================\nRunId: great_tail_pw48pry0lj\nWeb View: https://ml.azure.com/runs/great_tail_pw48pry0lj?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\n\n\n\n\nIf you click on the “Data” component and inside it click on “Explore”, you can see the contents of the file, since it is a text python file.\n\n\n\n\n\n# Component definition and registration\njob = command(\n    outputs=dict(\n        name=Output (type=\"uri_file\"),\n    ),\n    code=f\"./\",  # location of source code: in this case, the root folder\n    command=\"python hello_world_core.py --name ${{outputs.name}}\",\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"Hello World with uri_file as output\",\n)\nhello_world_component = ml_client.create_or_update(job.component)\n\n# Pipeline definition and registration\n@dsl.pipeline(\n    compute=\"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n    description=\"E2E hello world pipeline with input\",\n)\ndef hello_world_pipeline(\n):\n    # using data_prep_function like a python call with its own inputs\n    hello_world_job = hello_world_component()\n\npipeline = hello_world_pipeline()\n\npipeline_job = ml_client.jobs.create_or_update(\n    pipeline,\n    # Project's name\n    experiment_name=\"e2e_hello_world_with_uri_file_as_output\",\n)\n\n# Pipeline running\nml_client.jobs.stream(pipeline_job.name)\n\nUploading hello_world (9.48 MBs): 100%|██████████| 9483085/9483085 [00:00&lt;00:00, 22969826.09it/s]\n\n\n\n\nRunId: teal_soccer_m9bkcgz2gq\nWeb View: https://ml.azure.com/runs/teal_soccer_m9bkcgz2gq?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2024-03-26 15:36:23Z] Submitting 1 runs, first five are: 528b20ac:27e32a0a-71a0-4bc3-abec-eaeae70ff08e\n[2024-03-26 15:41:30Z] Completing processing run id 27e32a0a-71a0-4bc3-abec-eaeae70ff08e.\n\nExecution Summary\n=================\nRunId: teal_soccer_m9bkcgz2gq\nWeb View: https://ml.azure.com/runs/teal_soccer_m9bkcgz2gq?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\n\n\n\n\n\n\nIn order to have something more meaningful, we create a pipeline with two components. The first one “pre-processes” the input data frame by adding one (or a specified number) to it, storing the output as a csv file. The second component builds a “model” by calculating the mean and standard deviation, and saves it as pickle file.\n\n\nWhenever we have multiple components, a common practice in Azure ML is to have a dedicated subfolder for each one. The subfolder contains the source .py file implementing the component, and may contain a conda yaml file with dependencies that are specific for this component. In our case, we use a pre-built environment so that we don’t need to include any conda yaml file.\n\nos.makedirs (\"preprocessing\", exist_ok=True)\nos.makedirs (\"training\", exist_ok=True)\nos.makedirs (\"data\", exist_ok=True)\n\n\ndf = pd.DataFrame (\n    {\n        \"a\": [1,2,3],\n        \"b\": [4,5,6],\n    },\n)\n\ndf.to_csv (\"data/dummy_input.csv\")\n\n\n\n\n\n%%writefile preprocessing/preprocessing.py\nimport argparse\nimport pandas as pd\n\ndef preprocessing (df, x):\n    \"\"\"Adds `x` to input data frame `df`.\"\"\"\n    \n    print (\"Input\\n\", df)\n    print (f\"Adding {x} to df\")\n    df = df + x\n    print (\"Output\\n\", df)\n    return df\n\ndef parse_args ():\n    \"\"\"Parses input arguments\"\"\"\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--input_data\", type=str, help=\"path to input data frame\")\n    parser.add_argument(\"--preprocessed_data\", type=str, help=\"path to output data frame\")\n    parser.add_argument(\"-x\", type=int, help=\"number to add\")\n    args = parser.parse_args()\n    \n    return args\n\ndef read_and_preprocess (\n    input_data,\n    x,\n    preprocessed_data,\n):\n    df = pd.read_csv (input_data, index_col=0)\n    df = preprocessing (df, x)\n    df.to_csv (preprocessed_data)\n    \ndef main():\n    \"\"\"Main function of the script.\"\"\"\n    \n    args = parse_args ()\n    read_and_preprocess (args.input_data, args.x, args.preprocessed_data)\n\nif __name__ == \"__main__\":\n    main()\n\nOverwriting preprocessing/preprocessing.py\n\n\n\n# Component definition and registration\npreprocessing_command = command(\n    inputs=dict(\n        input_data=Input (type=\"uri_file\"),\n        x=Input (type=\"number\"),\n    ),\n    outputs=dict(\n        preprocessed_data=Output (type=\"uri_file\"),\n    ),\n    code=f\"./preprocessing/\",  # location of source code: in this case, the root folder\n    command=\"python preprocessing.py --input_data ${{inputs.input_data}} -x ${{inputs.x}} --preprocessed_data ${{outputs.preprocessed_data}}\",\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"Pre-processing\",\n)\npreprocessing_component = ml_client.create_or_update(preprocessing_command.component)\n\n\n\n\n\n%%writefile training/training.py\nimport argparse\nimport joblib\nimport pandas as pd\n\ndef train_model (df: pd.DataFrame):\n    \"\"\"Trains a dummy Gaussian model from training set df.\"\"\"\n    \n    print (\"Input\\n\", df)\n    mu = df.mean().values\n    std = df.std().values\n    print (\"mu:\\n\", mu)\n    print (\"std:\\n\", std)\n    return mu, std\n\ndef parse_args ():\n    \"\"\"Parses input arguments\"\"\"\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--preprocessed_data\", type=str, help=\"path to preprocessed data\")\n    parser.add_argument(\"--model\", type=str, help=\"path to built model\")\n    args = parser.parse_args()\n    \n    return args\n\ndef read_and_train (\n    preprocessed_data: str,\n    model_path: str,\n):\n    \"\"\"Reads training data, trains model, and saves it.\"\"\"\n    df = pd.read_csv (preprocessed_data, index_col=0)\n    model = train_model (df)\n    joblib.dump (model, model_path)\n\ndef main():\n    \"\"\"Main function of the script.\"\"\"\n    \n    args = parse_args ()\n    read_and_train (args.preprocessed_data, args.model)\n\nif __name__ == \"__main__\":\n    main()\n\nOverwriting training/training.py\n\n\n\n# Component definition and registration\ntraining_command = command(\n    inputs=dict(\n        preprocessed_data=Input (type=\"uri_file\"),\n    ),\n    outputs=dict(\n        model=Output (type=\"uri_file\"),\n    ),\n    code=f\"./training/\",  # location of source code: in this case, the root folder\n    command=\"python training.py --preprocessed_data ${{inputs.preprocessed_data}} --model ${{outputs.model}}\",\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"Training\",\n)\n\ntraining_component = ml_client.create_or_update(training_command.component)\n\nUploading training (0.0 MBs): 100%|██████████| 1043/1043 [00:00&lt;00:00, 112778.01it/s]\n\n\n\n\n\n\n\nBefore submitting the pipeline job, it is very important to test it first, ideally with some dummy or small dataset. For this purpose, in the component implementation above, we have separated the code related with argument parsing and the rest of the code, which is in encapsulated in a function called read_and_&lt;...&gt;. This way, we can easily write a test pipeline before implementing the final one, as follows:\n\n# We will need to change the code as we iteratively refine it \n# while testing the pipeline. For that purpose, we use the \n# reload module\nfrom importlib import reload \nfrom preprocessing import preprocessing\nfrom training import training\n\nreload (preprocessing)\nreload (training)\n\ndef test_pipeline (\n    pipeline_job_data_input: str,\n    pipeline_job_x: int,\n    pipeline_job_preprocess_output: str,\n    pipeline_job_model_output: str,\n):\n    \"\"\"\n    Tests two component pipeline with preprocessing and training.\n    \n    Parameters\n    ----------\n    pipeline_job_data_input: str\n        Path to input data *file*\n    pipeline_job_x: int\n        Integer to add to input data to convert it to \"preprocessed\" data.\n    pipeline_job_test_input: str\n        Path to (preprocessed) test input *file*\n    pipeline_job_preprocess_output: str\n        Path to preprocessed data *file*, to be used as training.\n        Not present in the final pipeline.\n    pipeline_job_model_output: str\n        Path to model *file*. Not present in the final pipeline.\n    \"\"\"\n    preprocessing.read_and_preprocess (\n        pipeline_job_data_input,\n        pipeline_job_x,\n        pipeline_job_preprocess_output,\n    )\n    training.read_and_train (\n        pipeline_job_preprocess_output,\n        pipeline_job_model_output,\n    )\n\nos.makedirs (\"test_pipeline\", exist_ok=True)\n\ntest_pipeline (\n    pipeline_job_data_input=\"./data/dummy_input.csv\",\n    pipeline_job_x=10,\n    pipeline_job_preprocess_output=\"./test_pipeline/preprocessed_data.csv\",\n    pipeline_job_model_output=\"./test_pipeline/model.pk\"\n)\n\nInput\n    a  b\n0  1  4\n1  2  5\n2  3  6\nAdding 10 to df\nOutput\n     a   b\n0  11  14\n1  12  15\n2  13  16\nInput\n     a   b\n0  11  14\n1  12  15\n2  13  16\nmu:\n [12. 15.]\nstd:\n [1. 1.]\n\n\n\n\n\nNow we are ready to implement and submit our pipeline. The code will be very similar to the test_pipeline implemented above, except for the fact that we don’t need to indicate the outputs that connect one component to the next, since these are automatically populated by AML.\n\n# Pipeline definition and registration\n@dsl.pipeline(\n    compute=\"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n    description=\"E2E hello world pipeline with input\",\n)\ndef two_components_pipeline(\n    pipeline_job_data_input: str,\n    pipeline_job_x: int,\n):\n    \"\"\"\n    Pipeline with two components: preprocessing, and training.\n    \n    Parameters\n    ----------\n    pipeline_job_data_input: str\n        Path to input data *file*\n    pipeline_job_x: int\n        Integer to add to input data to convert it to \"preprocessed\" data.\n    \"\"\"\n    # using data_prep_function like a python call with its own inputs\n    preprocessing_job = preprocessing_component(\n        input_data=pipeline_job_data_input,\n        x=pipeline_job_x,\n    )\n\n    # using train_func like a python call with its own inputs\n    training_job = training_component(\n        preprocessed_data=preprocessing_job.outputs.preprocessed_data,  # note: using outputs from previous step\n    )\n\ntwo_components_pipeline = two_components_pipeline(\n    pipeline_job_data_input=Input(type=\"uri_file\", path=\"./data/dummy_input.csv\"),\n    pipeline_job_x=10,\n)\n\ntwo_components_pipeline_job = ml_client.jobs.create_or_update(\n    two_components_pipeline,\n    # Project's name\n    experiment_name=\"e2e_two_components_pipeline\",\n)\n\n# Pipeline running\nml_client.jobs.stream(two_components_pipeline_job.name)\n\nRunId: quiet_root_nb0c997gsp\nWeb View: https://ml.azure.com/runs/quiet_root_nb0c997gsp?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2024-03-27 10:45:01Z] Submitting 1 runs, first five are: caf1c51e:87b5910c-0e8d-4ca0-a808-f52a94d52b56\n[2024-03-27 10:51:05Z] Completing processing run id 87b5910c-0e8d-4ca0-a808-f52a94d52b56.\n[2024-03-27 10:51:05Z] Submitting 1 runs, first five are: 3d73a420:6c033636-f3d8-4fe2-ba8d-26072210ba05\n[2024-03-27 10:56:25Z] Completing processing run id 6c033636-f3d8-4fe2-ba8d-26072210ba05.\n\nExecution Summary\n=================\nRunId: quiet_root_nb0c997gsp\nWeb View: https://ml.azure.com/runs/quiet_root_nb0c997gsp?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\n\n\nWe can see the created pipeline in the Pipelines section of our workspace:\n\n\n\n\n\nWe can see that:\n\nThe path to the preprocessed data has been automatically set to azureml/49824a8b-967f-4410-84a7-bc18b328a1b6/preprocessed_data, where the file name preprocessed_data is the name of the output given in the component definition:\n\n    outputs=dict(\n        preprocessed_data=Output (type=\"uri_file\"),\n    )\n\nSince this file name doesn’t have an extension, we cannot preview (see arrow 2). However, we can see its contents if we view the file in the datastore, as indicated below:\n\n\nUnfortunately, the content of the file appears in text format, rather than as a table.\n\nWe can also see the content of the outputs by inspecting the logs of the training component:\n\n\n\n\n\n\n\nLet’s try now using outputs of type “uri_folder”. We need to do two changes for this purpose:\n\nIn the component modules, preprocessing/preprocessing.py and training/training.py, the output arguments args.preprocessed_data and args.model will contain a path to a folder where the file is stored. Therefore, when saving the file, we need to append its name to the input path:\n\n#In preprocessing module:\ndf.to_csv (preprocessed_data + \"/preprocessed_data.csv\")\nand\n# In training module:\ndf = pd.read_csv (preprocessed_data + \"/preprocessed_data.csv\", index_col=0)\n\n# later in same module:\njoblib.dump (model, model_path + \"/model.pk\")\n\nIn the definition of the pipeline, we replace the type of the outputs to be “uri_folder”, and the input to the training component to be “uri_folder” as well.\n\n    # In preprocessing component\n    ...    \n    outputs=dict(\n        preprocessed_data=Output (type=\"uri_folder\"),\n    ),\n    ...\n        \n    # In training component\n    ...\n    inputs=dict(\n        preprocessed_data=Input (type=\"uri_folder\"),\n    ),\n    outputs=dict(\n        model=Output (type=\"uri_folder\"),\n    ),\n    ...\nHere we have the final implementation of our components:\n\n\n\n\n%%writefile preprocessing/preprocessing.py\nimport argparse\nimport pandas as pd\n\ndef preprocessing (df, x):\n    \"\"\"Adds `x` to input data frame `df`.\"\"\"\n    \n    print (\"Input\\n\", df)\n    print (f\"Adding {x} to df\")\n    df = df + x\n    print (\"Output\\n\", df)\n    return df\n\ndef parse_args ():\n    \"\"\"Parses input arguments\"\"\"\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--input_data\", type=str, help=\"path to input data *file*\")\n    parser.add_argument(\"--preprocessed_data\", type=str, help=\"path to output data *folder* containing the preprocessed data.\")\n    parser.add_argument(\"-x\", type=int, help=\"number to add\")\n    args = parser.parse_args()\n    \n    return args\n\ndef read_and_preprocess (\n    input_data,\n    x,\n    preprocessed_data,\n):\n    df = pd.read_csv (input_data, index_col=0)\n    df = preprocessing (df, x)\n    df.to_csv (preprocessed_data + \"/preprocessed_data.csv\")\n    \ndef main():\n    \"\"\"Main function of the script.\"\"\"\n    \n    args = parse_args ()\n    read_and_preprocess (args.input_data, args.x, args.preprocessed_data)\n\nif __name__ == \"__main__\":\n    main()\n\nOverwriting preprocessing/preprocessing.py\n\n\n\npreprocessing_command = command(\n    inputs=dict(\n        input_data=Input (type=\"uri_file\"),\n        x=Input (type=\"number\"),\n    ),\n    outputs=dict(\n        preprocessed_data=Output (type=\"uri_folder\"),\n    ),\n    code=f\"./preprocessing/\",  # location of source code: in this case, the root folder\n    command=\"python preprocessing.py --input_data ${{inputs.input_data}} -x ${{inputs.x}} --preprocessed_data ${{outputs.preprocessed_data}}\",\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"Pre-processing\",\n)\npreprocessing_component = ml_client.create_or_update(preprocessing_command.component)\n\n\n\n\n\n%%writefile training/training.py\nimport argparse\nimport joblib\nimport pandas as pd\n\ndef train_model (df: pd.DataFrame):\n    \"\"\"Trains a dummy Gaussian model from training set df.\"\"\"\n    \n    print (\"Input\\n\", df)\n    mu = df.mean().values\n    std = df.std().values\n    print (\"mu:\\n\", mu)\n    print (\"std:\\n\", std)\n    return mu, std\n\ndef parse_args ():\n    \"\"\"Parses input arguments\"\"\"\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--preprocessed_data\", type=str, help=\"path to preprocessed data\")\n    parser.add_argument(\"--model\", type=str, help=\"path to built model\")\n    args = parser.parse_args()\n    \n    return args\n\ndef read_and_train (\n    preprocessed_data: str,\n    model_path: str,\n):\n    \"\"\"Reads training data, trains model, and saves it.\"\"\"\n    df = pd.read_csv (preprocessed_data + \"/preprocessed_data.csv\", index_col=0)\n    model = train_model (df)\n    joblib.dump (model, model_path + \"/model.pk\")\n\ndef main():\n    \"\"\"Main function of the script.\"\"\"\n    \n    args = parse_args ()\n    read_and_train (args.preprocessed_data, args.model)\n\nif __name__ == \"__main__\":\n    main()\n\nOverwriting training/training.py\n\n\n\n# Component definition and registration\ntraining_command = command(\n    inputs=dict(\n        preprocessed_data=Input (type=\"uri_folder\"),\n    ),\n    outputs=dict(\n        model=Output (type=\"uri_folder\"),\n    ),\n    code=f\"./training/\",  # location of source code: in this case, the root folder\n    command=\"python training.py --preprocessed_data ${{inputs.preprocessed_data}} --model ${{outputs.model}}\",\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"Training\",\n)\n\ntraining_component = ml_client.create_or_update(training_command.component)\n\nUploading training (0.0 MBs): 100%|██████████| 1084/1084 [00:00&lt;00:00, 39997.06it/s]\n\n\n\n\n\n\n\nAgain, before submitting the pipeline job, we first test a manually built pipeline. Note that the new pipeline uses paths to folders, and not paths to files, for the outputs:\n\nfrom importlib import reload \nfrom preprocessing import preprocessing\nfrom training import training\n\nreload (preprocessing)\nreload (training)\n\ndef test_pipeline (\n    pipeline_job_data_input: str,\n    pipeline_job_x: int,\n    pipeline_job_preprocess_output: str,\n    pipeline_job_model_output: str,\n):\n    \"\"\"\n    Tests two component pipeline with preprocessing and training.\n    \n    Parameters\n    ----------\n    pipeline_job_data_input: str\n        Path to input data *file*\n    pipeline_job_x: int\n        Integer to add to input data to convert it to \"preprocessed\" data.\n    pipeline_job_test_input: str\n        Path to (preprocessed) test input *file*\n    pipeline_job_preprocess_output: str\n        Path to preprocessed data *folder*, to be used as training.\n        Not present in the final pipeline.\n    pipeline_job_model_output: str\n        Path to model *folder*. Not present in the final pipeline.\n    \"\"\"\n    preprocessing.read_and_preprocess (\n        pipeline_job_data_input,\n        pipeline_job_x,\n        pipeline_job_preprocess_output,\n    )\n    training.read_and_train (\n        pipeline_job_preprocess_output,\n        pipeline_job_model_output,\n    )\n\nos.makedirs (\"test_pipeline\", exist_ok=True)\n\ntest_pipeline (\n    pipeline_job_data_input=\"./data/dummy_input.csv\",\n    pipeline_job_x=10,\n    pipeline_job_preprocess_output=\"./test_pipeline\",\n    pipeline_job_model_output=\"./test_pipeline\"\n)\n\nInput\n    a  b\n0  1  4\n1  2  5\n2  3  6\nAdding 10 to df\nOutput\n     a   b\n0  11  14\n1  12  15\n2  13  16\nInput\n     a   b\n0  11  14\n1  12  15\n2  13  16\nmu:\n [12. 15.]\nstd:\n [1. 1.]\n\n\n… and the implementation of our pipeline:\n\n\n\n\n@dsl.pipeline(\n    compute=\"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n    description=\"E2E hello world pipeline with input\",\n)\ndef two_components_pipeline(\n    pipeline_job_data_input,\n    pipeline_job_x,\n):\n    \"\"\"\n    Pipeline with two components: preprocessing, and training.\n    \n    Parameters\n    ----------\n    pipeline_job_data_input: str\n        Path to input data *file*\n    pipeline_job_x: int\n        Integer to add to input data to convert it to \"preprocessed\" data.\n    \"\"\"\n    # using data_prep_function like a python call with its own inputs\n    preprocessing_job = preprocessing_component(\n        input_data=pipeline_job_data_input,\n        x=pipeline_job_x,\n    )\n\n    # using train_func like a python call with its own inputs\n    training_job = training_component(\n        preprocessed_data=preprocessing_job.outputs.preprocessed_data,  # note: using outputs from previous step\n    )\ntwo_components_pipeline = two_components_pipeline(\n    pipeline_job_data_input=Input(type=\"uri_file\", path=\"./data/dummy_input.csv\"),\n    pipeline_job_x=10,\n)\n\ntwo_components_pipeline_job = ml_client.jobs.create_or_update(\n    two_components_pipeline,\n    # Project's name\n    experiment_name=\"e2e_two_components_pipeline_with_uri_folder\",\n)\n\n# ----------------------------------------------------\n# Pipeline running\n# ----------------------------------------------------\nml_client.jobs.stream(two_components_pipeline_job.name)\n\nRunId: calm_zebra_t3gb5cjnrk\nWeb View: https://ml.azure.com/runs/calm_zebra_t3gb5cjnrk?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2024-03-28 08:45:53Z] Completing processing run id 1ff53a74-943f-4f94-8efd-4b55b345449f.\n[2024-03-28 08:45:54Z] Submitting 1 runs, first five are: e26d0be6:8e0f40eb-e2c3-4feb-a0d0-9882de1daebc\n\nExecution Summary\n=================\nRunId: calm_zebra_t3gb5cjnrk\nWeb View: https://ml.azure.com/runs/calm_zebra_t3gb5cjnrk?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\n\n\nNow, when we go to the “Explore” tab, in the output data of the preprocessed component, we can see the contents of the output preprocessed data in tabular format. We can also see that the extension of the output, which is csv file:\n\n\n\n\n\nWe add now a third component, which takes as input test data, preprocesses it, and uses the model to perform “inference”. For this pipeline, we can reuse the training component from the last section, but we need to slighty modify the preprocessing component to use an additional argument: the name of the output file. This is needed because there will be two outputs: one when preprocessing the training data, and the other when preprocessing the test data.\n\n\n\nos.makedirs (\"inference\", exist_ok=True)\ntest_data = pd.DataFrame (\n    {\n        \"a\": [11., 12.1, 13.1],\n        \"b\": [14.1, 15.1, 16.1],\n    }\n)\ntest_data.to_csv (\"data/dummy_test.csv\")\n\n\n\n\n\n%%writefile preprocessing/preprocessing.py\nimport argparse\nimport pandas as pd\n\ndef preprocessing (df, x):\n    \"\"\"Adds `x` to input data frame `df`.\"\"\"\n    \n    print (\"Input\\n\", df)\n    print (f\"Adding {x} to df\")\n    df = df + x\n    print (\"Output\\n\", df)\n    return df\n\ndef parse_args ():\n    \"\"\"Parses input arguments\"\"\"\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--input_data\", type=str, help=\"path to input data *file*\")\n    parser.add_argument(\"--preprocessed_data\", type=str, help=\"path to output data *folder* containing the preprocessed data.\")\n    parser.add_argument(\"--preprocessed_file_name\", type=str, help=\"name of preprocessed file name.\")\n    parser.add_argument(\"-x\", type=int, help=\"number to add\")\n    args = parser.parse_args()\n    \n    return args\n\ndef read_and_preprocess (\n    input_data: str,\n    preprocessed_data: str,\n    preprocessed_file_name: str,\n    x: int,\n):\n    df = pd.read_csv (input_data, index_col=0)\n    df = preprocessing (df, x)\n    df.to_csv (f\"{preprocessed_data}/{preprocessed_file_name}\")\n    \ndef main():\n    \"\"\"Main function of the script.\"\"\"\n    \n    args = parse_args ()\n    read_and_preprocess (\n        input_data=args.input_data, \n        preprocessed_data=args.preprocessed_data,\n        preprocessed_file_name=args.preprocessed_file_name,\n        x=args.x, \n    )\n\nif __name__ == \"__main__\":\n    main()\n\nOverwriting preprocessing/preprocessing.py\n\n\n\npreprocessing_command = command(\n    inputs=dict(\n        input_data=Input (type=\"uri_file\"),\n        x=Input (type=\"number\"),\n        preprocessed_file_name=Input (type=\"string\"),\n    ),\n    outputs=dict(\n        preprocessed_data=Output (type=\"uri_folder\"),\n    ),\n    code=f\"./preprocessing/\",  # location of source code: in this case, the root folder\n    command=\"python preprocessing.py --input_data ${{inputs.input_data}} -x ${{inputs.x}} --preprocessed_data ${{outputs.preprocessed_data}} --preprocessed_file_name ${{inputs.preprocessed_file_name}}\",\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"Pre-processing\",\n)\npreprocessing_component = ml_client.create_or_update(preprocessing_command.component)\n\nUploading preprocessing (0.0 MBs): 100%|██████████| 1343/1343 [00:00&lt;00:00, 46879.53it/s]\n\n\n\n\nThe preprocessing component doesn’t change from the last pipeline, see “Using uri_folder” section.\n\n\n\n\n%%writefile inference/inference.py\nimport argparse\nimport joblib\nimport pandas as pd\nfrom typing import Tuple\nimport numpy as np\n\ndef inference (\n    model: Tuple[np.ndarray, np.ndarray], \n    df: pd.DataFrame,\n):\n    \"\"\"\n    Runs dummy inference on new data `df`\n    \"\"\"\n    (mu, std) = model\n    z_df = (df - mu) / std\n    print (\"Inference result:\")\n    print (z_df)\n    return z_df\n\ndef parse_args ():\n    \"\"\"Parses input arguments\"\"\"\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--test_data\", type=str, help=\"path to test data *folder*\")\n    parser.add_argument(\"--test_data_file_name\", type=str, help=\"name of test data file name.\")\n    parser.add_argument(\"--model\", type=str, help=\"path to built model *folder*\")\n    parser.add_argument(\"--inference_output\", type=str, help=\"path to inference result *folder*\")\n    args = parser.parse_args()\n    \n    return args\n\ndef read_and_inference (\n    test_data: str,\n    test_data_file_name: str,\n    model_path: str,\n    inference_data: str,\n):\n    \"\"\"\n    Reads test data and model, performs inference, and writes to output inference file.\n    \n    Parameters\n    ----------\n    test_data: str\n        Path to test (preprocessed) data *folder*\n    test_data_file_name: str\n        Name of test data file.\n    model_path: str\n        Path to built model *folder*\n    inference_data: str\n        Path to inference result *folder*\n    \"\"\"\n    df = pd.read_csv (f\"{test_data}/{test_data_file_name}\", index_col=0)\n    model = joblib.load (model_path + \"/model.pk\")\n    z_df = inference (model, df)\n    z_df.to_csv (inference_data + \"/inference_result.csv\")\n\ndef main():\n    \"\"\"Main function of the script.\"\"\"\n    \n    args = parse_args ()\n    read_and_inference (\n        test_data=args.test_data, \n        test_data_file_name=args.test_data_file_name,\n        model_path=args.model, \n        inference_data=args.inference_output,\n    )\n\nif __name__ == \"__main__\":\n    main()\n\nOverwriting inference/inference.py\n\n\n\ninference_command = command(\n    inputs=dict(\n        test_data=Input (type=\"uri_folder\"),\n        test_data_file_name=Input (type=\"string\"),\n        model=Input (type=\"uri_folder\"),\n    ),\n    outputs=dict(\n        inference_output=Output (type=\"uri_folder\"),\n    ),\n    code=f\"./inference/\",  # location of source code: in this case, the root folder\n    command=\"python inference.py \" \n            \"--test_data ${{inputs.test_data}} \"\n            \"--test_data_file_name ${{inputs.test_data_file_name}} \"\n            \"--model ${{inputs.model}} \"\n            \"--inference_output ${{outputs.inference_output}}\",\n\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"inference\",\n)\ninference_component = ml_client.create_or_update(inference_command.component)\n\nUploading inference (0.0 MBs): 100%|██████████| 1912/1912 [00:00&lt;00:00, 63810.98it/s]\n\n\n\n\n\n\n\nBefore submitting the pipeline job, it is very important to test it first, ideally with some dummy or small dataset. For this purpose, in the component implementation above, we have separated the code related with argument parsing and the rest of the code, which is in encapsulated in a function called read_and_&lt;...&gt;. This way, we can easily write a test pipeline before implementing the final one, as follows:\n\n# We will need to change the code as we iteratively refine it \n# while testing the pipeline. For that purpose, we use the \n# reload module\nfrom importlib import reload \nfrom preprocessing import preprocessing\nfrom training import training\nfrom inference import inference\n\nreload (preprocessing)\nreload (training)\nreload (inference)\n\ndef test_pipeline (\n    pipeline_job_data_input: str,\n    pipeline_job_x: int,\n    pipeline_job_test_input: str,\n    pipeline_preprocessed_file_name: str,\n    pipeline_test_file_name: str,\n    \n    # The following parameters are not present in the final pipeline:\n    pipeline_job_preprocess_output: str,\n    pipeline_job_test_output: str,\n    pipeline_job_model_output: str,\n    pipeline_job_inference_output: str,\n):\n    \"\"\"\n    Tests third pipeline: preprocessing, training and inference.\n    \n    Parameters\n    ----------\n    pipeline_job_data_input: str\n        Path to input data *file*\n    pipeline_job_x: int\n        Integer to add to input data to convert it to \"preprocessed\" data.\n    pipeline_job_test_input: str\n        Path to (preprocessed) test input *file*\n    pipeline_job_test_input: str\n        Path to (preprocessed) test input *file*\n    pipeline_preprocessed_file_name: str\n        Name of (preprocessed) input data file.\n    pipeline_test_file_name: str\n        Name of (preprocessed) test data file.\n    pipeline_job_preprocess_output: str\n        Path to preprocessed data *folder*, to be used as training.\n        Not present in the final pipeline.\n    pipeline_job_test_output: str\n        Path to preprocessed test data *folder*, to be used for inferencing.\n        Not present in the final pipeline.\n    pipeline_job_model_output: str\n        Path to model *folder*. Not present in the final pipeline.\n    pipeline_job_inference_output: str\n        Path to inference result *folder*. Not present in the final pipeline.\n    \"\"\"\n    \n    preprocessing.read_and_preprocess (\n        pipeline_job_data_input,\n        pipeline_job_preprocess_output,\n        pipeline_preprocessed_file_name,\n        pipeline_job_x,\n    )\n    preprocessing.read_and_preprocess (\n        pipeline_job_test_input,\n        pipeline_job_test_output,\n        pipeline_test_file_name,\n        pipeline_job_x,\n    )\n    training.read_and_train (\n        pipeline_job_preprocess_output,\n        pipeline_job_model_output,\n    )\n    inference.read_and_inference (\n        test_data=pipeline_job_test_output,\n        test_data_file_name=pipeline_test_file_name,\n        model_path=pipeline_job_model_output,\n        inference_data=pipeline_job_inference_output,\n    )\n\nos.makedirs (\"test_pipeline\", exist_ok=True)\n\ntest_pipeline (\n    pipeline_job_data_input=\"./data/dummy_input.csv\",\n    pipeline_job_x=10,\n    pipeline_job_test_input=\"./data/dummy_test.csv\",\n    pipeline_preprocessed_file_name=\"preprocessed_data.csv\",\n    pipeline_test_file_name=\"preprocessed_test.csv\",\n    \n    # The following parameters are not present in the final pipeline:\n    pipeline_job_preprocess_output=\"./test_pipeline\",\n    pipeline_job_test_output=\"./test_pipeline\",\n    pipeline_job_model_output=\"./test_pipeline\",\n    pipeline_job_inference_output=\"./test_pipeline\",\n)\n\nInput\n    a  b\n0  1  4\n1  2  5\n2  3  6\nAdding 10 to df\nOutput\n     a   b\n0  11  14\n1  12  15\n2  13  16\nInput\n       a     b\n0  11.0  14.1\n1  12.1  15.1\n2  13.1  16.1\nAdding 10 to df\nOutput\n       a     b\n0  21.0  24.1\n1  22.1  25.1\n2  23.1  26.1\nInput\n     a   b\n0  11  14\n1  12  15\n2  13  16\nmu:\n [12. 15.]\nstd:\n [1. 1.]\nInference result:\n      a     b\n0   9.0   9.1\n1  10.1  10.1\n2  11.1  11.1\n\n\n\n\n\n\n@dsl.pipeline(\n    compute=\"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n    description=\"E2E hello world pipeline with input\",\n)\ndef three_components_pipeline(\n    pipeline_job_data_input: str,\n    pipeline_job_x: int,\n    pipeline_job_test_input: str,\n    pipeline_preprocessed_file_name: str,\n    pipeline_test_file_name: str,\n):\n    \"\"\"\n    Third pipeline: preprocessing, training and inference.\n    \n    Parameters\n    ----------\n    pipeline_job_data_input: str\n        Path to input data *file*\n    pipeline_job_x: int\n        Integer to add to input data to convert it to \"preprocessed\" data.\n    pipeline_job_test_input: str\n        Path to (preprocessed) test input *file*\n    pipeline_preprocessed_file_name: str\n        Name of (preprocessed) input data file.\n    pipeline_test_file_name: str\n        Name of (preprocessed) test data file.\n    \"\"\"\n    # using data_prep_function like a python call with its own inputs\n    preprocessing_job = preprocessing_component(\n        input_data=pipeline_job_data_input,\n        x=pipeline_job_x,\n        preprocessed_file_name=pipeline_preprocessed_file_name,\n    )\n    \n    preprocessing_test_job = preprocessing_component(\n        input_data=pipeline_job_test_input,\n        x=pipeline_job_x,\n        preprocessed_file_name=pipeline_test_file_name,\n    )\n\n    # using train_func like a python call with its own inputs\n    training_job = training_component(\n        preprocessed_data=preprocessing_job.outputs.preprocessed_data,  # note: using outputs from previous step\n    )\n    \n    # using train_func like a python call with its own inputs\n    inference_job = inference_component(\n        test_data=preprocessing_test_job.outputs.preprocessed_data,\n        test_data_file_name=pipeline_test_file_name,\n        model=training_job.outputs.model,  # note: using outputs from previous step\n    )\n    \nthree_components_pipeline = three_components_pipeline(\n    pipeline_job_data_input=Input(type=\"uri_file\", path=\"./data/dummy_input.csv\"),\n    pipeline_job_x=10,\n    pipeline_job_test_input=Input(type=\"uri_file\", path=\"./data/dummy_test.csv\"),\n    pipeline_preprocessed_file_name=\"preprocessed_data.csv\",\n    pipeline_test_file_name=\"preprocessed_test_data.csv\",\n)\n\nthree_components_pipeline_job = ml_client.jobs.create_or_update(\n    three_components_pipeline,\n    # Project's name\n    experiment_name=\"e2e_three_components_pipeline_with_uri_folder\",\n)\n\n# ----------------------------------------------------\n# Pipeline running\n# ----------------------------------------------------\nml_client.jobs.stream(three_components_pipeline_job.name)\n\nRunId: calm_rice_3cmmmtc5mf\nWeb View: https://ml.azure.com/runs/calm_rice_3cmmmtc5mf?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2024-03-28 10:15:12Z] Submitting 2 runs, first five are: 47ca85c2:a5595dea-3117-47e9-a99d-186b0c346884,4b1f0180:09aecbcc-aa2c-4cad-b134-eb4837724f58\n[2024-03-28 10:20:13Z] Completing processing run id 09aecbcc-aa2c-4cad-b134-eb4837724f58.\n[2024-03-28 10:20:13Z] Submitting 1 runs, first five are: fece868d:164d1e2a-a5d7-4081-9a68-a07033a3feee\n[2024-03-28 10:20:45Z] Completing processing run id 164d1e2a-a5d7-4081-9a68-a07033a3feee.\n[2024-03-28 10:22:15Z] Completing processing run id a5595dea-3117-47e9-a99d-186b0c346884.\n[2024-03-28 10:22:16Z] Submitting 1 runs, first five are: a9d60b27:6a18ae74-2a3c-4e16-a6cb-a58649235bfe\n[2024-03-28 10:22:52Z] Completing processing run id 6a18ae74-2a3c-4e16-a6cb-a58649235bfe.\n\nExecution Summary\n=================\nRunId: calm_rice_3cmmmtc5mf\nWeb View: https://ml.azure.com/runs/calm_rice_3cmmmtc5mf?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\n\n\nHere we can see the resulting pipeline:\n\n\n\n\n\n\n\nFrom the development of the different pipelines we can extract a few observations that help us create a better refactored pipeline and, at the same time, compile a small set of “design” rules that may help us in future pipelines. In my case, I find it clearer and with less boilerplate to use the following rules:\n\nUse “uri_folder” type for intermediate outputs, and add another parameter next to it containing the output file, something like:\n\ndef pipeline(\n    ...\n    input_folder: str,\n    input_filename: str,\n    ...\n)\n\nUse “_folder” as a prefix for parameters of type uri_folder, “_file” for those of type uri_file, and “_filename” for those indicating names of file names.\nUse the suffix input for those things that are inputs and ouputfor those that are outputs.\nI’m not clear about this one. Many people usually pass a dataframe called df or a numpy array called X, which is passed from one step of the pipeline to the next, without appending words to the name that talk about the content of the dataframe or the array (e.g., use “X” instead of “preprocessed_X” or “inference_result_X”). I tend to find it easier to do a similar thing here for the inputs of intermediate components, when defining the command for those components. Therefore, for the inputs, I would use input_folder rather than preprocessed_training_data_input_folder for indicating the input to the model component. This means that if we replace later the model component with one that works directly on raw (non-preprocessed) data (e.g., because the preprocessing is implicitly done as part of the model), we don’t need to replace the part of the script that parses the arguments to indicate that now the input folder is just training_data_input_folder.\nFor the outputs, it might be useful to add a short prefix to talk about the type of output, so that the pipeline’s diagram shows what is the ouput of each component is. Again, I’m not clear about this one.\nThe exception to the previous rules is when we have more than one input or output folder. In this case, we clearly need to add more words to their names.\nIt is easier to avoid adding pipeline_job_... for each parameter of the pipeline.\n\n\n\n\n\n\n\n\n%%writefile preprocessing/preprocessing.py\nimport argparse\nimport pandas as pd\n\ndef preprocessing (\n    df: pd.DataFrame, \n    x: int\n):\n    \"\"\"Adds `x` to input data frame `df`.\n\n    Parameters\n    ----------\n    df: DataFrame\n        Input data frame \n    x: int\n        Integer to add to df.\n\n    Returns\n    -------\n    DataFrame.\n        Preprocessed data.\n    \"\"\"\n    \n    print (\"Input\\n\", df)\n    print (f\"Adding {x} to df\")\n    df = df + x\n    print (\"Output\\n\", df)\n    return df\n\ndef parse_args ():\n    \"\"\"Parses input arguments\"\"\"\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--input_file\", type=str, help=\"path to input data file\")\n    parser.add_argument(\"--output_folder\", type=str, help=\"path to output data folder containing the preprocessed data.\")\n    parser.add_argument(\"--output_filename\", type=str, help=\"name of file containing the output, preprocessed, data.\")\n    parser.add_argument(\"-x\", type=int, help=\"number to add to input data for preprocessing it.\")\n    args = parser.parse_args()\n    \n    return args\n\ndef read_and_preprocess (\n    input_file: str,\n    output_folder: str,\n    output_filename: str,\n    x: int,\n):\n    \"\"\"Reads input data, preprocesses it, and writes result as csv file in disk.\n\n    Parameters\n    ----------\n    input_file: str\n        Path to input data file.\n    output_folder: str\n        Path to output data folder containing the preprocessed data.\n    output_filename: str\n        Name of file containing the output, preprocessed, data.\n    x: int\n        Number to add to input data for preprocessing it.\n    \"\"\"\n    df = pd.read_csv (input_file, index_col=0)\n    df = preprocessing (df, x)\n    df.to_csv (f\"{output_folder}/{output_filename}\")\n    \ndef main():\n    \"\"\"Main function of the script.\"\"\"\n    \n    args = parse_args ()\n    read_and_preprocess (\n        input_file=args.input_file, \n        output_folder=args.output_folder,\n        output_filename=args.output_filename,\n        x=args.x, \n    )\n\nif __name__ == \"__main__\":\n    main()\n\nOverwriting preprocessing/preprocessing.py\n\n\n\npreprocessing_command = command(\n    inputs=dict(\n        input_file=Input (type=\"uri_file\"),\n        x=Input (type=\"number\"),\n        output_filename=Input (type=\"string\"),\n    ),\n    outputs=dict(\n        output_folder=Output (type=\"uri_folder\"),\n    ),\n    code=f\"./preprocessing/\",  # location of source code: in this case, the root folder\n    command=\"python preprocessing.py \"\n        \"--input_file ${{inputs.input_file}} \"\n        \"-x ${{inputs.x}} \"\n        \"--output_folder ${{outputs.output_folder}} \"\n        \"--output_filename ${{inputs.output_filename}}\",\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"Pre-processing\",\n)\npreprocessing_component = ml_client.create_or_update(preprocessing_command.component)\n\nUploading preprocessing (0.0 MBs): 100%|██████████| 1985/1985 [00:00&lt;00:00, 64070.90it/s]\n\n\n\n\n\n\n\n\n%%writefile training/training.py\nimport argparse\nimport joblib\nimport pandas as pd\n\ndef train_model (df: pd.DataFrame):\n    \"\"\"Trains a dummy Gaussian model from training set df.\n    \n    Parameters\n    ----------\n    df: DataFrame\n        Input data frame\n    \n    Returns\n    -------\n    np.ndarray\n        Average across rows, one per column.\n    np.ndarray\n        Standard deviation across rows, one per column.\n    \"\"\"\n    \n    print (\"Input\\n\", df)\n    mu = df.mean().values\n    std = df.std().values\n    print (\"mu:\\n\", mu)\n    print (\"std:\\n\", std)\n    return mu, std\n\ndef parse_args ():\n    \"\"\"Parses input arguments\"\"\"\n    \n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\n        \"--input_folder\", \n        type=str, \n        help=\"path to preprocessed training data folder, \"\n             \"containing training set file.\"\n    )\n    parser.add_argument(\n        \"--input_filename\", \n        type=str, \n        help=\"name of file containing preprocessed, training data.\"\n    )\n    parser.add_argument(\n        \"--output_folder\", \n        type=str, \n        help=\"path to output *folder* containing the trained model.\"\n    )\n    parser.add_argument(\n        \"--output_filename\", \n        type=str, \n        help=\"name of file containing the trained model.\"\n    )\n    args = parser.parse_args()\n    \n    return args\n\ndef read_and_train (\n    input_folder: str,\n    input_filename: str,\n    output_folder: str,\n    output_filename: str,\n):\n    \"\"\"Reads training data, trains model, and saves it.\n    \n    Parameters\n    ----------\n    input_folder: str\n        Path to preprocessed training data folder containing training set file.\n    input_filename: str\n        Name of file containing preprocessed, training data.\n    output_folder: str\n        Path to output folder containing the trained model.\n    output_filename: str\n        Name of file containing the trained model.\n    \"\"\"\n    \n    df = pd.read_csv (f\"{input_folder}/{input_filename}\", index_col=0)\n    model = train_model (df)\n    joblib.dump (model, f\"{output_folder}/{output_filename}\")\n\ndef main():\n    \"\"\"Main function of the script.\"\"\"\n    \n    args = parse_args ()\n    read_and_train (\n        args.input_folder, \n        args.input_filename,\n        args.output_folder,\n        args.output_filename,\n    )\n\nif __name__ == \"__main__\":\n    main()\n\nOverwriting training/training.py\n\n\n\n# Component definition and registration\ntraining_command = command(\n    inputs=dict(\n        input_folder=Input (type=\"uri_folder\"),\n        input_filename=Input (type=\"string\"),\n        output_filename=Input (type=\"string\"),\n    ),\n    outputs=dict(\n        output_folder=Output (type=\"uri_folder\"),\n    ),\n    code=f\"./training/\",  # location of source code: in this case, the root folder\n    command=\"python training.py \"\n        \"--input_folder ${{inputs.input_folder}} \"\n        \"--input_filename ${{inputs.input_filename}} \"\n        \"--output_folder ${{outputs.output_folder}} \"\n        \"--output_filename ${{inputs.output_filename}}\",\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"Training\",\n)\n\ntraining_component = ml_client.create_or_update(training_command.component)\n\nUploading training (0.0 MBs): 100%|██████████| 2309/2309 [00:00&lt;00:00, 50917.16it/s]\n\n\n\n\n\n\n\n\n%%writefile inference/inference.py\nimport argparse\nfrom typing import Tuple\nimport joblib\nimport pandas as pd\nfrom sklearn.metrics import DistanceMetric\nimport numpy as np\n\ndef inference (\n    model: Tuple[np.ndarray, np.ndarray], \n    df: pd.DataFrame,\n):\n    \"\"\"Runs dummy inference on new data `df`.\n\n    Parameters\n    ----------\n    model: Tople (np.ndarray, np.ndarray)\n        Average across rows (one per column), and \n        standard deviation across rows (one per column).\n    df: DataFrame\n        Test data frame on which to perform inference.\n    \n    Returns\n    -------\n    DataFrame\n        One column dataframe giving an approximation of the Mahalanobis distance \n        between each row vector and the mean vector, assuming that the covariance \n        matrix is diagonal. The negative of the scores obtained can be considered \n        as a sort of prediction probability for each row of belonging to the Gaussian \n        class estimated from the training data. In this sense this function provides\n        inference about how \"normal\" the test samples are. \n    \"\"\"\n    (mu, std) = model\n    dist = DistanceMetric.get_metric('mahalanobis', V=np.diag(std**2))\n    ndims = df.shape[1]\n    mah_dist = dist.pairwise (mu.reshape(1, ndims), df)\n    mah_dist = pd.DataFrame (mah_dist.ravel(), columns=[\"distance\"])\n    print (\"Inference result:\")\n    print (mah_dist)\n    return mah_dist\n\ndef parse_args ():\n    \"\"\"Parses input arguments\"\"\"\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--preprocessed_input_folder\", \n        type=str, \n        help=\"path to input, preprocessed, test data folder, \"\n             \"containing file on which to perform inference.\"\n    )\n    parser.add_argument(\n        \"--preprocessed_input_filename\", \n        type=str, \n        help=\"name of file containing the input, preprocessed, test data.\"\n    )\n    parser.add_argument(\n        \"--model_input_folder\", \n        type=str, \n        help=\"path to model folder.\"\n    )\n    parser.add_argument(\n        \"--model_input_filename\", \n        type=str, \n        help=\"name of model file.\"\n    )\n    parser.add_argument(\n        \"--output_folder\", \n        type=str, \n        help=\"path to output data *folder* with inference results.\"\n    )\n    parser.add_argument(\n        \"--output_filename\", \n        type=str, \n        help=\"name of file containing the output data with inference results.\"\n    )\n    \n    args = parser.parse_args()\n\n    args = parser.parse_args()\n    \n    return args\n\ndef read_and_inference (\n    preprocessed_input_folder: str,\n    preprocessed_input_filename: str,\n    model_input_folder: str,\n    model_input_filename: str,\n    output_folder: str,    \n    output_filename: str,\n):\n    \"\"\"\n    Reads test data and model, performs inference, and writes to output inference file.\n    \n    Parameters\n    ----------\n    preprocessed_input_folder: str\n        Path to test (preprocessed) data folder.\n    preprocessed_input_filename: str\n        Name of test data file.\n    model_input_folder: str\n        Path to built model folder.\n    model_input_filename: str\n        Path to inference result folder.\n    output_folder: str\n        Path to output data folder with inference results.\n    output_filename: str\n        Name of file containing the output data with inference results.\n    \"\"\"\n    df = pd.read_csv (f\"{preprocessed_input_folder}/{preprocessed_input_filename}\", index_col=0)\n    model = joblib.load (f\"{model_input_folder}/{model_input_filename}\")\n    z_df = inference (model, df)\n    z_df.to_csv (f\"{output_folder}/{output_filename}\")\n\ndef main():\n    \"\"\"Main function of the script.\"\"\"\n    \n    args = parse_args ()\n    read_and_inference (\n        preprocessed_input_folder=args.preprocessed_input_folder,\n        preprocessed_input_filename=args.preprocessed_input_filename,\n        model_input_folder=args.model_input_folder,\n        model_input_filename=args.model_input_filename,\n        output_folder=args.output_folder,\n        output_filename=args.output_filename,\n    )\n\nif __name__ == \"__main__\":\n    main()\n\nOverwriting inference/inference.py\n\n\n\ninference_command = command(\n    inputs=dict(\n        preprocessed_input_folder=Input (type=\"uri_folder\"),\n        preprocessed_input_filename=Input (type=\"string\"),\n        model_input_folder=Input (type=\"uri_folder\"),\n        model_input_filename=Input (type=\"string\"),\n        output_filename=Input (type=\"string\"),\n    ),\n    outputs=dict(\n        output_folder=Output (type=\"uri_folder\"),\n    ),\n    code=f\"./inference/\",  # location of source code: in this case, the root folder\n    command=\"python inference.py \" \n        \"--preprocessed_input_folder ${{inputs.preprocessed_input_folder}} \"\n        \"--preprocessed_input_filename ${{inputs.preprocessed_input_filename}} \"\n        \"--model_input_folder ${{inputs.model_input_folder}} \"\n        \"--model_input_filename ${{inputs.model_input_filename}} \"\n        \"--output_folder ${{outputs.output_folder}} \"\n        \"--output_filename ${{inputs.output_filename}} \",\n\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"inference\",\n)\ninference_component = ml_client.create_or_update(inference_command.component)\n\nUploading inference (0.0 MBs): 100%|██████████| 4046/4046 [00:00&lt;00:00, 151355.71it/s]\n\n\n\n\n\n\n\nBefore submitting the pipeline job, it is very important to test it first, ideally with some dummy or small dataset. For this purpose, in the component implementation above, we have separated the code related with argument parsing and the rest of the code, which is in encapsulated in a function called read_and_&lt;...&gt;. This way, we can easily write a test pipeline before implementing the final one, as follows:\n\n# We will need to change the code as we iteratively refine it \n# while testing the pipeline. For that purpose, we use the \n# reload module\nfrom importlib import reload \nfrom preprocessing import preprocessing\nfrom training import training\nfrom inference import inference\n\nreload (preprocessing)\nreload (training)\nreload (inference)\n\ndef test_pipeline (\n    # Preprocessing component parameters, first component:\n    preprocessing_training_input_file: str,\n    preprocessing_training_output_folder: str, # Not present in final pipeline\n    preprocessing_training_output_filename: str,\n    x: int,\n    \n    # Preprocessing component parameters, second component:\n    preprocessing_test_input_file: str,\n    preprocessing_test_output_folder: str, # Not present in final pipeline\n    preprocessing_test_output_filename: str,\n    \n    # Training component parameters:\n    # input_folder: this is preprocessing_training_output_folder\n    # input_filename: this is preprocessing_training_output_filename\n    training_output_folder: str, # Not present in final pipeline\n    training_output_filename: str, \n    \n    # Inference component parameters:\n    # preprocessed_input_folder: this is preprocessing_test_output_folder\n    # preprocessed_input_filename: this is preprocessing_test_output_filename\n    # model_input_folder: this is training_output_folder\n    # model_input_filename: this is training_output_filename\n    inference_output_folder: str, # Not present in final pipeline\n    inference_output_filename: str,\n):\n    \"\"\"\n    Tests third pipeline: preprocessing, training and inference.\n    \n    Parameters\n    ----------\n    preprocessing_training_input_file: str\n        Path to file containing training data to be preprocessed.\n    preprocessing_training_output_folder: str\n        Path to folder containing the preprocessed, training data file.\n        Not present in final pipeline.\n    preprocessing_training_output_filename: str\n        Name of file containing the preprocessed, training data.\n    x: int\n        Number to add to input data for preprocessing it.\n    preprocessing_test_input_file: str\n        Path to file containing test data to be preprocessed.\n    preprocessing_test_output_folder: str\n        Path to folder containing the preprocessed, test data file.\n        Not present in final pipeline.\n    preprocessing_test_output_filename: str\n        Name of file containing the preprocessed, test data.\n    training_output_folder: str\n        Path to output folder containing the trained model.\n        Not present in final pipeline.\n    training_output_filename: str\n        Name of file containing the trained model.\n    inference_output_folder: str\n        Path to output data folder with inference results.\n        Not present in final pipeline.\n    inference_output_filename: str\n        Name of file containing the output data with inference results.\n    \"\"\"\n    \n    preprocessing.read_and_preprocess (\n        input_file=preprocessing_training_input_file,\n        output_folder=preprocessing_training_output_folder, # Not present in final component\n        output_filename=preprocessing_training_output_filename,\n        x=x,\n    )\n    preprocessing.read_and_preprocess (\n        input_file=preprocessing_test_input_file,\n        output_folder=preprocessing_test_output_folder,\n        output_filename=preprocessing_test_output_filename,\n        x=x,\n    )\n    training.read_and_train (\n        input_folder=preprocessing_training_output_folder,\n        input_filename=preprocessing_training_output_filename,\n        output_folder=training_output_folder,\n        output_filename=training_output_filename,\n    )\n    inference.read_and_inference (\n        preprocessed_input_folder=preprocessing_test_output_folder,\n        preprocessed_input_filename=preprocessing_test_output_filename,\n        model_input_folder=training_output_folder,\n        model_input_filename=training_output_filename,\n        output_folder=inference_output_folder,\n        output_filename=inference_output_filename,\n    )\n\nos.makedirs (\"test_pipeline\", exist_ok=True)\n\ntest_pipeline (\n    # first preprocessing component\n    preprocessing_training_input_file=\"./data/dummy_input.csv\",\n    preprocessing_training_output_folder=\"./test_pipeline\", # Not present in final pipeline\n    preprocessing_training_output_filename=\"preprocessed_data.csv\",\n    x=10,\n    \n    # second preprocessing component\n    preprocessing_test_input_file=\"./data/dummy_test.csv\",\n    preprocessing_test_output_folder=\"./test_pipeline\", # Not present in final pipeline\n    preprocessing_test_output_filename=\"preprocessed_test.csv\",\n    \n    # Training component parameters:\n    training_output_folder=\"./test_pipeline\", # Not present in final pipeline\n    training_output_filename=\"model.pk\",\n    \n    # Inference component parameters:\n    inference_output_folder=\"./test_pipeline\", # Not present in final pipeline\n    inference_output_filename=\"inference_result.csv\",\n)\n\nInput\n    a  b\n0  1  4\n1  2  5\n2  3  6\nAdding 10 to df\nOutput\n     a   b\n0  11  14\n1  12  15\n2  13  16\nInput\n       a     b\n0  11.0  14.1\n1  12.1  15.1\n2  13.1  16.1\nAdding 10 to df\nOutput\n       a     b\n0  21.0  24.1\n1  22.1  25.1\n2  23.1  26.1\nInput\n     a   b\n0  11  14\n1  12  15\n2  13  16\nmu:\n [12. 15.]\nstd:\n [1. 1.]\nInference result:\n    distance\n0  12.798828\n1  14.283557\n2  15.697771\n\n\n\n\n\n\n@dsl.pipeline(\n    compute=\"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n    description=\"E2E hello world pipeline with input\",\n)\ndef three_components_pipeline(\n    # Preprocessing component parameters, first component:\n    preprocessing_training_input_file: str,\n    preprocessing_training_output_filename: str,\n    x: int,\n    \n    # Preprocessing component parameters, second component:\n    preprocessing_test_input_file: str,\n    preprocessing_test_output_filename: str,\n    \n    # Training component parameters:\n    training_output_filename: str, \n    \n    # Inference component parameters:\n    inference_output_filename: str,\n):\n    \"\"\"\n    Third pipeline: preprocessing, training and inference.\n    \n    Parameters\n    ----------\n    preprocessing_training_input_file: str\n        Path to file containing training data to be preprocessed.\n    preprocessing_training_output_filename: str\n        Name of file containing the preprocessed, training data.\n    x: int\n        Number to add to input data for preprocessing it.\n    preprocessing_test_input_file: str\n        Path to file containing test data to be preprocessed.\n    preprocessing_test_output_filename: str\n        Name of file containing the preprocessed, test data.\n    training_output_filename: str\n        Name of file containing the trained model.\n    inference_output_filename: str\n        Name of file containing the output data with inference results.\n    \"\"\"\n    # using data_prep_function like a python call with its own inputs\n    preprocessing_training_job = preprocessing_component(\n        input_file=preprocessing_training_input_file,\n        #output_folder: automatically determined\n        output_filename=preprocessing_training_output_filename,\n        x=x,\n    )\n    preprocessing_test_job = preprocessing_component(\n        input_file=preprocessing_test_input_file,\n        #output_folder: automatically determined\n        output_filename=preprocessing_test_output_filename,\n        x=x,\n    )\n    training_job = training_component(\n        input_folder=preprocessing_training_job.outputs.output_folder,\n        input_filename=preprocessing_training_output_filename,\n        #output_folder: automatically determined\n        output_filename=training_output_filename,\n    )\n    inference_job = inference_component(\n        preprocessed_input_folder=preprocessing_test_job.outputs.output_folder,\n        preprocessed_input_filename=preprocessing_test_output_filename,\n        model_input_folder=training_job.outputs.output_folder,\n        model_input_filename=training_output_filename,\n        #output_folder: automatically determined\n        output_filename=inference_output_filename,\n    )\n    \nthree_components_pipeline = three_components_pipeline(\n    # first preprocessing component\n    preprocessing_training_input_file=Input(type=\"uri_file\", path=\"./data/dummy_input.csv\"),\n    preprocessing_training_output_filename=\"preprocessed_training_data.csv\",\n    x=10,\n    \n    # second preprocessing component\n    preprocessing_test_input_file=Input(type=\"uri_file\", path=\"./data/dummy_test.csv\"),\n    preprocessing_test_output_filename=\"preprocessed_test_data.csv\",\n    \n    # Training component parameters:\n    training_output_filename=\"model.pk\",\n    \n    # Inference component parameters:\n    inference_output_filename=\"inference_results.csv\",\n)\n\nthree_components_pipeline_job = ml_client.jobs.create_or_update(\n    three_components_pipeline,\n    # Project's name\n    experiment_name=\"e2e_three_components_refactored\",\n)\n\n# ----------------------------------------------------\n# Pipeline running\n# ----------------------------------------------------\nml_client.jobs.stream(three_components_pipeline_job.name)\n\nClass AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n\n\nRunId: busy_toe_03bv5yshzh\nWeb View: https://ml.azure.com/runs/busy_toe_03bv5yshzh?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2024-03-29 08:45:11Z] Submitting 2 runs, first five are: f39c6442:de45cd94-3bf9-4405-9b9f-59782424a8fb,feeb5198:0b543053-d140-4699-9c29-1ec2e755898e\n[2024-03-29 08:50:08Z] Completing processing run id de45cd94-3bf9-4405-9b9f-59782424a8fb.\n[2024-03-29 08:50:33Z] Completing processing run id 0b543053-d140-4699-9c29-1ec2e755898e.\n[2024-03-29 08:50:34Z] Submitting 1 runs, first five are: 5731fc43:46a9ab31-8a68-42ee-a377-a5e84cbad37d\n[2024-03-29 08:55:36Z] Completing processing run id 46a9ab31-8a68-42ee-a377-a5e84cbad37d.\n[2024-03-29 08:55:36Z] Submitting 1 runs, first five are: 5af83f98:fd50a27d-6687-431a-88d3-b8ba3d8799f4\n[2024-03-29 08:56:12Z] Execution of experiment failed, update experiment status and cancel running nodes.\n\nExecution Summary\n=================\nRunId: busy_toe_03bv5yshzh\nWeb View: https://ml.azure.com/runs/busy_toe_03bv5yshzh?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\n\nJobException: Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has failed child jobs. Failed nodes: /inference_job. For more details and logs, please go to the job detail page and check the child jobs.\",\n        \"message_format\": \"Pipeline has failed child jobs. {0}\",\n        \"message_parameters\": {},\n        \"reference_code\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"eastus2\",\n    \"location\": \"eastus2\",\n    \"time\": \"2024-03-29T08:56:11.952518Z\",\n    \"component_name\": \"\"\n} \n\n\nHere we can see the resulting pipeline:"
  },
  {
    "objectID": "posts/data_science/hello_world.html#starting-development-with-a-notebook",
    "href": "posts/data_science/hello_world.html#starting-development-with-a-notebook",
    "title": "Hello World AML pipeline with component",
    "section": "",
    "text": "In this section, we will write our code in a notebook and make sure it works well. Then, we will convert it to a script and run it from the terminal. Finally, we will add some logs with MLFlow.\nAlthough not required, as a very first step we can create a environment and kernel following the tutorial in https://learn.microsoft.com/en-gb/azure/machine-learning/tutorial-cloud-workstation?view=azureml-api-2\nFollowing the previous tutorial, create a notebook and type the following hello world code:\n\ndef hello_world (name):\n    \"\"\"Greets the indicated person and the world in general.\"\"\"\n    \n    print (f\"Hello {name} and world\")\n\nhello_world (\"Jaume\")\n\nHello Jaume and world\n\n\nFantastic, the code works ;-). Now, let’s convert it to a script that can be run from terminal. The tutorial above explains how to convert the notebook to a python file. In our case, we will first add an argument parser and then write it to file using the magic cell %%writefile\n\n%%writefile hello_world_core.py\nimport argparse\n\ndef hello_world (name):\n    \"\"\"Greets the indicated person and the world in general.\"\"\"\n    \n    print (f\"Hello {name} and world\")\n\ndef parse_args ():\n    \"\"\"Parses input arguments\"\"\"\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--name\", type=str, help=\"person to greet\")\n    args = parser.parse_args()\n    \n    return args\n\ndef main():\n    \"\"\"Main function of the script.\"\"\"\n    \n    args = parse_args ()\n    hello_world (args.name)\n\nif __name__ == \"__main__\":\n    main()\n\nOverwriting hello_world_core.py\n\n\nNow, we can open up a terminal, as illustrated in the tutorial above, cd to the folder where the script is and run it:\ncd  Users/&lt;my_user&gt;/hello_world\npython hello_world_core.py --name Jaume\n\n\n\n%%writefile hello_world_with_logs.py\nimport mlflow\nfrom hello_world_core import hello_world, parse_args\n\ndef start_logging (args):\n    # set name for logging\n    mlflow.set_experiment(\"Hello World with logging\")\n    mlflow.start_run()\n    mlflow.log_param (\"name to log\", args.name)\n    \ndef finish_logging ():\n    mlflow.end_run ()\n\ndef main():\n    \"\"\"Main function of the script.\"\"\"\n    args = parse_args ()\n    start_logging (args)\n    hello_world (args.name)\n    finish_logging ()\n\nif __name__ == \"__main__\":\n    main()\n\nOverwriting hello_world_with_logs.py\n\n\nLet’s run it and see:\npython hello_world_with_logs.py --name Peter\nHere is the newly created job:\n\nAnd the name passed as argument:\n\nWe start by getting a connection to our Azure ML (AML for short) workspace. We use here a simple connection mechanism that doesn’t require writting your subscription, resource group and workspace details:\n\nfrom azure.ai.ml import MLClient\nfrom azure.identity import DefaultAzureCredential\n\n# authenticate\ncredential = DefaultAzureCredential()\n\n# Get a handle to the workspace\nml_client = MLClient.from_config (\n    credential=credential\n)\n\nFound the config file in: /config.json"
  },
  {
    "objectID": "posts/data_science/hello_world.html#running-script-as-a-job",
    "href": "posts/data_science/hello_world.html#running-script-as-a-job",
    "title": "Hello World AML pipeline with component",
    "section": "",
    "text": "We now convert the previous script into a job that can be run from the UI.\n\n\n\n# Standard imports\nimport os\n\n# Third-party imports\nimport pandas as pd\n\n# AML imports\nfrom azure.ai.ml import (\n    command,\n    dsl,\n    Input,\n    Output,\n    MLClient\n)\nfrom azure.identity import DefaultAzureCredential\n\n\n\n\nFor the remaining part of this tutorial, we will be needing an ml_client handle. This will allow us to create and use resources from our workspace. The simplest way to get such handle is with the following code:\n\n# authenticate\ncredential = DefaultAzureCredential()\n\n# Get a handle to the workspace\nml_client = MLClient.from_config (\n    credential=credential\n)\n\nFound the config file in: /config.json\n\n\n\n\n\nWe specify a job using the command decorator:\n\njob = command(\n    inputs=dict(\n        name=\"Jaume\", # default value of our parameter\n    ),\n    code=f\"./\",  # location of source code: in this case, the root folder\n    command=\"python hello_world_core.py --name ${{inputs.name}}\",\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"Simplest Hello World\",\n)\n\n\nNote: we indicate as environment “AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest”, which actually contains more libraries than we need, such as sklearn. Simpler environments to use can be found in the “Environments” section of the workspace.\n\n… and submit it using create_or_update from ml_client:\n\nml_client.create_or_update(job)\n\nClass AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nUploading hello_world (8.59 MBs): 100%|██████████| 8591281/8591281 [00:00&lt;00:00, 40584539.30it/s]\n\n\n\n\n\n\n\n\nExperiment\nName\nType\nStatus\nDetails Page\n\n\nhello_world\nclever_spade_sq4jwcg67r\ncommand\nStarting\nLink to Azure Machine Learning studio\n\n\n\n\n\n\nIn the link that appears, we can see the status of the job, which initially is “Queued”. We need to wait until it is completed (and refresh the page to see this). Once it is completed, we can look at the logs:\n\nIn the logs, we can see the messages printed in console:\n\n\n\n\nAbove, we indicated a default value for the input argument name. It would be good to be able to submit jobs with different values for that argument. One way to do that is:\n\nIn the job’s Overview tab, click on “Edit and submit”\n\n\n\nIn the “Training script” section, edit the “Inputs” by clicking on the pencil next to it:\n\n\n\nIn the “Input value” field, type the new value you want for the argument:\n\n\n\nHit Next several times and then Submit.\nIf we go to the jobs section of the workspace, and enter again our job (“helloworld”), we can see that a new job has been submitted:\n\n\nIn its Overview tab, under “See all properties”, we can inspect the json file:\n\n… and see that the new value (Peter) is used in its “parameters” dictionary:\n\nThe std_log.txt for this job shows the new message with Peter:"
  },
  {
    "objectID": "posts/data_science/hello_world.html#creating-single-component-pipeline",
    "href": "posts/data_science/hello_world.html#creating-single-component-pipeline",
    "title": "Hello World AML pipeline with component",
    "section": "",
    "text": "hello_world_component = ml_client.create_or_update(job.component)\n\nUploading hello_world (8.58 MBs): 100%|██████████| 8578531/8578531 [00:00&lt;00:00, 21700197.27it/s]\n\n\n\n\n\n# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n@dsl.pipeline(\n    compute=\"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n    description=\"E2E hello world pipeline\",\n)\ndef hello_world_pipeline(\n    pipeline_job_input: str,\n):\n    \"\"\"\n    Hello World pipeline\n    \n    Parameters\n    ----------\n    pipeline_job_input: str\n        Input to pipeline, here name of person to greed.\n    \"\"\"\n    # using data_prep_function like a python call with its own inputs\n    hello_world_job = hello_world_component(\n        name=pipeline_job_input,\n    )\n\n\n# Let's instantiate the pipeline with the parameters of our choice\npipeline = hello_world_pipeline(\n    pipeline_job_input=\"David\",\n)\n\n\npipeline_job = ml_client.jobs.create_or_update(\n    pipeline,\n    # Project's name\n    experiment_name=\"e2e_registered_components\",\n)\nml_client.jobs.stream(pipeline_job.name)\n\nClass AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n\n\nRunId: shy_cabbage_xb9vv4fswl\nWeb View: https://ml.azure.com/runs/shy_cabbage_xb9vv4fswl?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2024-03-26 14:08:19Z] Submitting 1 runs, first five are: 605cf9a7:d9904e2d-3ecb-4ddc-a04d-e2fed4facfe6\n[2024-03-26 14:12:40Z] Completing processing run id d9904e2d-3ecb-4ddc-a04d-e2fed4facfe6.\n\nExecution Summary\n=================\nRunId: shy_cabbage_xb9vv4fswl\nWeb View: https://ml.azure.com/runs/shy_cabbage_xb9vv4fswl?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\n\n\n\n\n\n\njob = command(\n    inputs=dict(\n        name=Input (type=\"string\"),\n    ),\n    code=f\"./\",  # location of source code: in this case, the root folder\n    command=\"python hello_world_core.py --name ${{inputs.name}}\",\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"Hello World witn Input\",\n)\n\nhello_world_component = ml_client.create_or_update(job.component)\n\nUploading hello_world (8.59 MBs): 100%|██████████| 8589758/8589758 [00:00&lt;00:00, 24178345.78it/s]\n\n\n\n\n\n# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\nfrom azure.ai.ml import dsl\n\n@dsl.pipeline(\n    compute=\"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n    description=\"E2E hello world pipeline with input\",\n)\ndef hello_world_pipeline(\n    pipeline_job_input: str,\n):\n    \"\"\"\n    Hello World pipeline\n    \n    Parameters\n    ----------\n    pipeline_job_input: str\n        Input to pipeline, here name of person to greed.\n    \"\"\"\n    # using data_prep_function like a python call with its own inputs\n    hello_world_job = hello_world_component(\n        name=pipeline_job_input,\n    )\n\n\npipeline = hello_world_pipeline(\n    pipeline_job_input=\"Joseph\",\n)\n\npipeline_job = ml_client.jobs.create_or_update(\n    pipeline,\n    # Project's name\n    experiment_name=\"e2e_hello_world_with_input\",\n)\nml_client.jobs.stream(pipeline_job.name)\n\nRunId: olive_plastic_gvnjy01b5s\nWeb View: https://ml.azure.com/runs/olive_plastic_gvnjy01b5s?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2024-03-26 14:38:43Z] Submitting 1 runs, first five are: cd1599c4:ce24c41e-946d-48cd-99b2-70ebde3befb2\n[2024-03-26 14:44:58Z] Completing processing run id ce24c41e-946d-48cd-99b2-70ebde3befb2.\n\nExecution Summary\n=================\nRunId: olive_plastic_gvnjy01b5s\nWeb View: https://ml.azure.com/runs/olive_plastic_gvnjy01b5s?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\n\n\nNotes about Input:\n\nWhen using Input(type=“uri_folder”) or Input(type=“uri_file”), the value passed cannot be a string, it must be an Input type, for example:\n\njob = command(\n    inputs=dict(\n        file_name=Input (type=\"uri_file\"),\n    ),\n    ...\n)\n\npipeline = hello_world_pipeline(\n    pipeline_job_input=Input(path=\"/path/to/file\"),\n)\n\nHowever, when using Input(type=“string”) or Input(type=“number”), the input must be a string or number, not Input\n\njob = command(\n    inputs=dict(\n        name=Input (type=\"string\"),\n    ),\n    ...\n)\n\npipeline = hello_world_pipeline(\n    pipeline_job_input=\"Mary\",\n)\n\nIn the latter case, the input does not appear in the graph of the pipeline, in the UI.\n\n\n\n\n\n# Component definition and registration\njob = command(\n    inputs=dict(\n        name=Input (type=\"uri_file\"),\n    ),\n    code=f\"./\",  # location of source code: in this case, the root folder\n    command=\"python hello_world_core.py --name ${{inputs.name}}\",\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"Hello World with uri_file\",\n)\nhello_world_component = ml_client.create_or_update(job.component)\n\n# Pipeline definition and registration\n@dsl.pipeline(\n    compute=\"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n    description=\"E2E hello world pipeline with input\",\n)\ndef hello_world_pipeline(\n    pipeline_job_input: str,\n):\n    \"\"\"\n    Hello World pipeline\n    \n    Parameters\n    ----------\n    pipeline_job_data_input: str\n        Input to pipeline, here path to file.\n    \"\"\"\n    # using data_prep_function like a python call with its own inputs\n    hello_world_job = hello_world_component(\n        name=pipeline_job_input,\n    )\n\npipeline = hello_world_pipeline(\n    pipeline_job_input=Input(type=\"uri_file\", path=\"./hello_world_core.py\"),\n)\n\npipeline_job = ml_client.jobs.create_or_update(\n    pipeline,\n    # Project's name\n    experiment_name=\"e2e_hello_world_with_uri_file\",\n)\n\n# Pipeline running\nml_client.jobs.stream(pipeline_job.name)\n\nUploading hello_world (8.59 MBs): 100%|██████████| 8588206/8588206 [00:00&lt;00:00, 24482901.98it/s]\n\n\nUploading hello_world_core.py (&lt; 1 MB): 0.00B [00:00, ?B/s] (&lt; 1 MB): 100%|██████████| 514/514 [00:00&lt;00:00, 12.0kB/s]\n\n\n\n\nRunId: great_tail_pw48pry0lj\nWeb View: https://ml.azure.com/runs/great_tail_pw48pry0lj?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2024-03-26 15:06:11Z] Submitting 1 runs, first five are: a08118c5:2099b6ad-fb3a-4cac-9557-c8cf355b8b1b\n[2024-03-26 15:11:50Z] Completing processing run id 2099b6ad-fb3a-4cac-9557-c8cf355b8b1b.\n\nExecution Summary\n=================\nRunId: great_tail_pw48pry0lj\nWeb View: https://ml.azure.com/runs/great_tail_pw48pry0lj?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\n\n\n\n\nIf you click on the “Data” component and inside it click on “Explore”, you can see the contents of the file, since it is a text python file.\n\n\n\n\n\n# Component definition and registration\njob = command(\n    outputs=dict(\n        name=Output (type=\"uri_file\"),\n    ),\n    code=f\"./\",  # location of source code: in this case, the root folder\n    command=\"python hello_world_core.py --name ${{outputs.name}}\",\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"Hello World with uri_file as output\",\n)\nhello_world_component = ml_client.create_or_update(job.component)\n\n# Pipeline definition and registration\n@dsl.pipeline(\n    compute=\"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n    description=\"E2E hello world pipeline with input\",\n)\ndef hello_world_pipeline(\n):\n    # using data_prep_function like a python call with its own inputs\n    hello_world_job = hello_world_component()\n\npipeline = hello_world_pipeline()\n\npipeline_job = ml_client.jobs.create_or_update(\n    pipeline,\n    # Project's name\n    experiment_name=\"e2e_hello_world_with_uri_file_as_output\",\n)\n\n# Pipeline running\nml_client.jobs.stream(pipeline_job.name)\n\nUploading hello_world (9.48 MBs): 100%|██████████| 9483085/9483085 [00:00&lt;00:00, 22969826.09it/s]\n\n\n\n\nRunId: teal_soccer_m9bkcgz2gq\nWeb View: https://ml.azure.com/runs/teal_soccer_m9bkcgz2gq?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2024-03-26 15:36:23Z] Submitting 1 runs, first five are: 528b20ac:27e32a0a-71a0-4bc3-abec-eaeae70ff08e\n[2024-03-26 15:41:30Z] Completing processing run id 27e32a0a-71a0-4bc3-abec-eaeae70ff08e.\n\nExecution Summary\n=================\nRunId: teal_soccer_m9bkcgz2gq\nWeb View: https://ml.azure.com/runs/teal_soccer_m9bkcgz2gq?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld"
  },
  {
    "objectID": "posts/data_science/hello_world.html#pipeline-with-two-components",
    "href": "posts/data_science/hello_world.html#pipeline-with-two-components",
    "title": "Hello World AML pipeline with component",
    "section": "",
    "text": "In order to have something more meaningful, we create a pipeline with two components. The first one “pre-processes” the input data frame by adding one (or a specified number) to it, storing the output as a csv file. The second component builds a “model” by calculating the mean and standard deviation, and saves it as pickle file.\n\n\nWhenever we have multiple components, a common practice in Azure ML is to have a dedicated subfolder for each one. The subfolder contains the source .py file implementing the component, and may contain a conda yaml file with dependencies that are specific for this component. In our case, we use a pre-built environment so that we don’t need to include any conda yaml file.\n\nos.makedirs (\"preprocessing\", exist_ok=True)\nos.makedirs (\"training\", exist_ok=True)\nos.makedirs (\"data\", exist_ok=True)\n\n\ndf = pd.DataFrame (\n    {\n        \"a\": [1,2,3],\n        \"b\": [4,5,6],\n    },\n)\n\ndf.to_csv (\"data/dummy_input.csv\")\n\n\n\n\n\n%%writefile preprocessing/preprocessing.py\nimport argparse\nimport pandas as pd\n\ndef preprocessing (df, x):\n    \"\"\"Adds `x` to input data frame `df`.\"\"\"\n    \n    print (\"Input\\n\", df)\n    print (f\"Adding {x} to df\")\n    df = df + x\n    print (\"Output\\n\", df)\n    return df\n\ndef parse_args ():\n    \"\"\"Parses input arguments\"\"\"\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--input_data\", type=str, help=\"path to input data frame\")\n    parser.add_argument(\"--preprocessed_data\", type=str, help=\"path to output data frame\")\n    parser.add_argument(\"-x\", type=int, help=\"number to add\")\n    args = parser.parse_args()\n    \n    return args\n\ndef read_and_preprocess (\n    input_data,\n    x,\n    preprocessed_data,\n):\n    df = pd.read_csv (input_data, index_col=0)\n    df = preprocessing (df, x)\n    df.to_csv (preprocessed_data)\n    \ndef main():\n    \"\"\"Main function of the script.\"\"\"\n    \n    args = parse_args ()\n    read_and_preprocess (args.input_data, args.x, args.preprocessed_data)\n\nif __name__ == \"__main__\":\n    main()\n\nOverwriting preprocessing/preprocessing.py\n\n\n\n# Component definition and registration\npreprocessing_command = command(\n    inputs=dict(\n        input_data=Input (type=\"uri_file\"),\n        x=Input (type=\"number\"),\n    ),\n    outputs=dict(\n        preprocessed_data=Output (type=\"uri_file\"),\n    ),\n    code=f\"./preprocessing/\",  # location of source code: in this case, the root folder\n    command=\"python preprocessing.py --input_data ${{inputs.input_data}} -x ${{inputs.x}} --preprocessed_data ${{outputs.preprocessed_data}}\",\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"Pre-processing\",\n)\npreprocessing_component = ml_client.create_or_update(preprocessing_command.component)\n\n\n\n\n\n%%writefile training/training.py\nimport argparse\nimport joblib\nimport pandas as pd\n\ndef train_model (df: pd.DataFrame):\n    \"\"\"Trains a dummy Gaussian model from training set df.\"\"\"\n    \n    print (\"Input\\n\", df)\n    mu = df.mean().values\n    std = df.std().values\n    print (\"mu:\\n\", mu)\n    print (\"std:\\n\", std)\n    return mu, std\n\ndef parse_args ():\n    \"\"\"Parses input arguments\"\"\"\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--preprocessed_data\", type=str, help=\"path to preprocessed data\")\n    parser.add_argument(\"--model\", type=str, help=\"path to built model\")\n    args = parser.parse_args()\n    \n    return args\n\ndef read_and_train (\n    preprocessed_data: str,\n    model_path: str,\n):\n    \"\"\"Reads training data, trains model, and saves it.\"\"\"\n    df = pd.read_csv (preprocessed_data, index_col=0)\n    model = train_model (df)\n    joblib.dump (model, model_path)\n\ndef main():\n    \"\"\"Main function of the script.\"\"\"\n    \n    args = parse_args ()\n    read_and_train (args.preprocessed_data, args.model)\n\nif __name__ == \"__main__\":\n    main()\n\nOverwriting training/training.py\n\n\n\n# Component definition and registration\ntraining_command = command(\n    inputs=dict(\n        preprocessed_data=Input (type=\"uri_file\"),\n    ),\n    outputs=dict(\n        model=Output (type=\"uri_file\"),\n    ),\n    code=f\"./training/\",  # location of source code: in this case, the root folder\n    command=\"python training.py --preprocessed_data ${{inputs.preprocessed_data}} --model ${{outputs.model}}\",\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"Training\",\n)\n\ntraining_component = ml_client.create_or_update(training_command.component)\n\nUploading training (0.0 MBs): 100%|██████████| 1043/1043 [00:00&lt;00:00, 112778.01it/s]\n\n\n\n\n\n\n\nBefore submitting the pipeline job, it is very important to test it first, ideally with some dummy or small dataset. For this purpose, in the component implementation above, we have separated the code related with argument parsing and the rest of the code, which is in encapsulated in a function called read_and_&lt;...&gt;. This way, we can easily write a test pipeline before implementing the final one, as follows:\n\n# We will need to change the code as we iteratively refine it \n# while testing the pipeline. For that purpose, we use the \n# reload module\nfrom importlib import reload \nfrom preprocessing import preprocessing\nfrom training import training\n\nreload (preprocessing)\nreload (training)\n\ndef test_pipeline (\n    pipeline_job_data_input: str,\n    pipeline_job_x: int,\n    pipeline_job_preprocess_output: str,\n    pipeline_job_model_output: str,\n):\n    \"\"\"\n    Tests two component pipeline with preprocessing and training.\n    \n    Parameters\n    ----------\n    pipeline_job_data_input: str\n        Path to input data *file*\n    pipeline_job_x: int\n        Integer to add to input data to convert it to \"preprocessed\" data.\n    pipeline_job_test_input: str\n        Path to (preprocessed) test input *file*\n    pipeline_job_preprocess_output: str\n        Path to preprocessed data *file*, to be used as training.\n        Not present in the final pipeline.\n    pipeline_job_model_output: str\n        Path to model *file*. Not present in the final pipeline.\n    \"\"\"\n    preprocessing.read_and_preprocess (\n        pipeline_job_data_input,\n        pipeline_job_x,\n        pipeline_job_preprocess_output,\n    )\n    training.read_and_train (\n        pipeline_job_preprocess_output,\n        pipeline_job_model_output,\n    )\n\nos.makedirs (\"test_pipeline\", exist_ok=True)\n\ntest_pipeline (\n    pipeline_job_data_input=\"./data/dummy_input.csv\",\n    pipeline_job_x=10,\n    pipeline_job_preprocess_output=\"./test_pipeline/preprocessed_data.csv\",\n    pipeline_job_model_output=\"./test_pipeline/model.pk\"\n)\n\nInput\n    a  b\n0  1  4\n1  2  5\n2  3  6\nAdding 10 to df\nOutput\n     a   b\n0  11  14\n1  12  15\n2  13  16\nInput\n     a   b\n0  11  14\n1  12  15\n2  13  16\nmu:\n [12. 15.]\nstd:\n [1. 1.]\n\n\n\n\n\nNow we are ready to implement and submit our pipeline. The code will be very similar to the test_pipeline implemented above, except for the fact that we don’t need to indicate the outputs that connect one component to the next, since these are automatically populated by AML.\n\n# Pipeline definition and registration\n@dsl.pipeline(\n    compute=\"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n    description=\"E2E hello world pipeline with input\",\n)\ndef two_components_pipeline(\n    pipeline_job_data_input: str,\n    pipeline_job_x: int,\n):\n    \"\"\"\n    Pipeline with two components: preprocessing, and training.\n    \n    Parameters\n    ----------\n    pipeline_job_data_input: str\n        Path to input data *file*\n    pipeline_job_x: int\n        Integer to add to input data to convert it to \"preprocessed\" data.\n    \"\"\"\n    # using data_prep_function like a python call with its own inputs\n    preprocessing_job = preprocessing_component(\n        input_data=pipeline_job_data_input,\n        x=pipeline_job_x,\n    )\n\n    # using train_func like a python call with its own inputs\n    training_job = training_component(\n        preprocessed_data=preprocessing_job.outputs.preprocessed_data,  # note: using outputs from previous step\n    )\n\ntwo_components_pipeline = two_components_pipeline(\n    pipeline_job_data_input=Input(type=\"uri_file\", path=\"./data/dummy_input.csv\"),\n    pipeline_job_x=10,\n)\n\ntwo_components_pipeline_job = ml_client.jobs.create_or_update(\n    two_components_pipeline,\n    # Project's name\n    experiment_name=\"e2e_two_components_pipeline\",\n)\n\n# Pipeline running\nml_client.jobs.stream(two_components_pipeline_job.name)\n\nRunId: quiet_root_nb0c997gsp\nWeb View: https://ml.azure.com/runs/quiet_root_nb0c997gsp?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2024-03-27 10:45:01Z] Submitting 1 runs, first five are: caf1c51e:87b5910c-0e8d-4ca0-a808-f52a94d52b56\n[2024-03-27 10:51:05Z] Completing processing run id 87b5910c-0e8d-4ca0-a808-f52a94d52b56.\n[2024-03-27 10:51:05Z] Submitting 1 runs, first five are: 3d73a420:6c033636-f3d8-4fe2-ba8d-26072210ba05\n[2024-03-27 10:56:25Z] Completing processing run id 6c033636-f3d8-4fe2-ba8d-26072210ba05.\n\nExecution Summary\n=================\nRunId: quiet_root_nb0c997gsp\nWeb View: https://ml.azure.com/runs/quiet_root_nb0c997gsp?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\n\n\nWe can see the created pipeline in the Pipelines section of our workspace:\n\n\n\n\n\nWe can see that:\n\nThe path to the preprocessed data has been automatically set to azureml/49824a8b-967f-4410-84a7-bc18b328a1b6/preprocessed_data, where the file name preprocessed_data is the name of the output given in the component definition:\n\n    outputs=dict(\n        preprocessed_data=Output (type=\"uri_file\"),\n    )\n\nSince this file name doesn’t have an extension, we cannot preview (see arrow 2). However, we can see its contents if we view the file in the datastore, as indicated below:\n\n\nUnfortunately, the content of the file appears in text format, rather than as a table.\n\nWe can also see the content of the outputs by inspecting the logs of the training component:"
  },
  {
    "objectID": "posts/data_science/hello_world.html#using-uri_folder",
    "href": "posts/data_science/hello_world.html#using-uri_folder",
    "title": "Hello World AML pipeline with component",
    "section": "",
    "text": "Let’s try now using outputs of type “uri_folder”. We need to do two changes for this purpose:\n\nIn the component modules, preprocessing/preprocessing.py and training/training.py, the output arguments args.preprocessed_data and args.model will contain a path to a folder where the file is stored. Therefore, when saving the file, we need to append its name to the input path:\n\n#In preprocessing module:\ndf.to_csv (preprocessed_data + \"/preprocessed_data.csv\")\nand\n# In training module:\ndf = pd.read_csv (preprocessed_data + \"/preprocessed_data.csv\", index_col=0)\n\n# later in same module:\njoblib.dump (model, model_path + \"/model.pk\")\n\nIn the definition of the pipeline, we replace the type of the outputs to be “uri_folder”, and the input to the training component to be “uri_folder” as well.\n\n    # In preprocessing component\n    ...    \n    outputs=dict(\n        preprocessed_data=Output (type=\"uri_folder\"),\n    ),\n    ...\n        \n    # In training component\n    ...\n    inputs=dict(\n        preprocessed_data=Input (type=\"uri_folder\"),\n    ),\n    outputs=dict(\n        model=Output (type=\"uri_folder\"),\n    ),\n    ...\nHere we have the final implementation of our components:\n\n\n\n\n%%writefile preprocessing/preprocessing.py\nimport argparse\nimport pandas as pd\n\ndef preprocessing (df, x):\n    \"\"\"Adds `x` to input data frame `df`.\"\"\"\n    \n    print (\"Input\\n\", df)\n    print (f\"Adding {x} to df\")\n    df = df + x\n    print (\"Output\\n\", df)\n    return df\n\ndef parse_args ():\n    \"\"\"Parses input arguments\"\"\"\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--input_data\", type=str, help=\"path to input data *file*\")\n    parser.add_argument(\"--preprocessed_data\", type=str, help=\"path to output data *folder* containing the preprocessed data.\")\n    parser.add_argument(\"-x\", type=int, help=\"number to add\")\n    args = parser.parse_args()\n    \n    return args\n\ndef read_and_preprocess (\n    input_data,\n    x,\n    preprocessed_data,\n):\n    df = pd.read_csv (input_data, index_col=0)\n    df = preprocessing (df, x)\n    df.to_csv (preprocessed_data + \"/preprocessed_data.csv\")\n    \ndef main():\n    \"\"\"Main function of the script.\"\"\"\n    \n    args = parse_args ()\n    read_and_preprocess (args.input_data, args.x, args.preprocessed_data)\n\nif __name__ == \"__main__\":\n    main()\n\nOverwriting preprocessing/preprocessing.py\n\n\n\npreprocessing_command = command(\n    inputs=dict(\n        input_data=Input (type=\"uri_file\"),\n        x=Input (type=\"number\"),\n    ),\n    outputs=dict(\n        preprocessed_data=Output (type=\"uri_folder\"),\n    ),\n    code=f\"./preprocessing/\",  # location of source code: in this case, the root folder\n    command=\"python preprocessing.py --input_data ${{inputs.input_data}} -x ${{inputs.x}} --preprocessed_data ${{outputs.preprocessed_data}}\",\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"Pre-processing\",\n)\npreprocessing_component = ml_client.create_or_update(preprocessing_command.component)\n\n\n\n\n\n%%writefile training/training.py\nimport argparse\nimport joblib\nimport pandas as pd\n\ndef train_model (df: pd.DataFrame):\n    \"\"\"Trains a dummy Gaussian model from training set df.\"\"\"\n    \n    print (\"Input\\n\", df)\n    mu = df.mean().values\n    std = df.std().values\n    print (\"mu:\\n\", mu)\n    print (\"std:\\n\", std)\n    return mu, std\n\ndef parse_args ():\n    \"\"\"Parses input arguments\"\"\"\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--preprocessed_data\", type=str, help=\"path to preprocessed data\")\n    parser.add_argument(\"--model\", type=str, help=\"path to built model\")\n    args = parser.parse_args()\n    \n    return args\n\ndef read_and_train (\n    preprocessed_data: str,\n    model_path: str,\n):\n    \"\"\"Reads training data, trains model, and saves it.\"\"\"\n    df = pd.read_csv (preprocessed_data + \"/preprocessed_data.csv\", index_col=0)\n    model = train_model (df)\n    joblib.dump (model, model_path + \"/model.pk\")\n\ndef main():\n    \"\"\"Main function of the script.\"\"\"\n    \n    args = parse_args ()\n    read_and_train (args.preprocessed_data, args.model)\n\nif __name__ == \"__main__\":\n    main()\n\nOverwriting training/training.py\n\n\n\n# Component definition and registration\ntraining_command = command(\n    inputs=dict(\n        preprocessed_data=Input (type=\"uri_folder\"),\n    ),\n    outputs=dict(\n        model=Output (type=\"uri_folder\"),\n    ),\n    code=f\"./training/\",  # location of source code: in this case, the root folder\n    command=\"python training.py --preprocessed_data ${{inputs.preprocessed_data}} --model ${{outputs.model}}\",\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"Training\",\n)\n\ntraining_component = ml_client.create_or_update(training_command.component)\n\nUploading training (0.0 MBs): 100%|██████████| 1084/1084 [00:00&lt;00:00, 39997.06it/s]\n\n\n\n\n\n\n\nAgain, before submitting the pipeline job, we first test a manually built pipeline. Note that the new pipeline uses paths to folders, and not paths to files, for the outputs:\n\nfrom importlib import reload \nfrom preprocessing import preprocessing\nfrom training import training\n\nreload (preprocessing)\nreload (training)\n\ndef test_pipeline (\n    pipeline_job_data_input: str,\n    pipeline_job_x: int,\n    pipeline_job_preprocess_output: str,\n    pipeline_job_model_output: str,\n):\n    \"\"\"\n    Tests two component pipeline with preprocessing and training.\n    \n    Parameters\n    ----------\n    pipeline_job_data_input: str\n        Path to input data *file*\n    pipeline_job_x: int\n        Integer to add to input data to convert it to \"preprocessed\" data.\n    pipeline_job_test_input: str\n        Path to (preprocessed) test input *file*\n    pipeline_job_preprocess_output: str\n        Path to preprocessed data *folder*, to be used as training.\n        Not present in the final pipeline.\n    pipeline_job_model_output: str\n        Path to model *folder*. Not present in the final pipeline.\n    \"\"\"\n    preprocessing.read_and_preprocess (\n        pipeline_job_data_input,\n        pipeline_job_x,\n        pipeline_job_preprocess_output,\n    )\n    training.read_and_train (\n        pipeline_job_preprocess_output,\n        pipeline_job_model_output,\n    )\n\nos.makedirs (\"test_pipeline\", exist_ok=True)\n\ntest_pipeline (\n    pipeline_job_data_input=\"./data/dummy_input.csv\",\n    pipeline_job_x=10,\n    pipeline_job_preprocess_output=\"./test_pipeline\",\n    pipeline_job_model_output=\"./test_pipeline\"\n)\n\nInput\n    a  b\n0  1  4\n1  2  5\n2  3  6\nAdding 10 to df\nOutput\n     a   b\n0  11  14\n1  12  15\n2  13  16\nInput\n     a   b\n0  11  14\n1  12  15\n2  13  16\nmu:\n [12. 15.]\nstd:\n [1. 1.]\n\n\n… and the implementation of our pipeline:\n\n\n\n\n@dsl.pipeline(\n    compute=\"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n    description=\"E2E hello world pipeline with input\",\n)\ndef two_components_pipeline(\n    pipeline_job_data_input,\n    pipeline_job_x,\n):\n    \"\"\"\n    Pipeline with two components: preprocessing, and training.\n    \n    Parameters\n    ----------\n    pipeline_job_data_input: str\n        Path to input data *file*\n    pipeline_job_x: int\n        Integer to add to input data to convert it to \"preprocessed\" data.\n    \"\"\"\n    # using data_prep_function like a python call with its own inputs\n    preprocessing_job = preprocessing_component(\n        input_data=pipeline_job_data_input,\n        x=pipeline_job_x,\n    )\n\n    # using train_func like a python call with its own inputs\n    training_job = training_component(\n        preprocessed_data=preprocessing_job.outputs.preprocessed_data,  # note: using outputs from previous step\n    )\ntwo_components_pipeline = two_components_pipeline(\n    pipeline_job_data_input=Input(type=\"uri_file\", path=\"./data/dummy_input.csv\"),\n    pipeline_job_x=10,\n)\n\ntwo_components_pipeline_job = ml_client.jobs.create_or_update(\n    two_components_pipeline,\n    # Project's name\n    experiment_name=\"e2e_two_components_pipeline_with_uri_folder\",\n)\n\n# ----------------------------------------------------\n# Pipeline running\n# ----------------------------------------------------\nml_client.jobs.stream(two_components_pipeline_job.name)\n\nRunId: calm_zebra_t3gb5cjnrk\nWeb View: https://ml.azure.com/runs/calm_zebra_t3gb5cjnrk?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2024-03-28 08:45:53Z] Completing processing run id 1ff53a74-943f-4f94-8efd-4b55b345449f.\n[2024-03-28 08:45:54Z] Submitting 1 runs, first five are: e26d0be6:8e0f40eb-e2c3-4feb-a0d0-9882de1daebc\n\nExecution Summary\n=================\nRunId: calm_zebra_t3gb5cjnrk\nWeb View: https://ml.azure.com/runs/calm_zebra_t3gb5cjnrk?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\n\n\nNow, when we go to the “Explore” tab, in the output data of the preprocessed component, we can see the contents of the output preprocessed data in tabular format. We can also see that the extension of the output, which is csv file:"
  },
  {
    "objectID": "posts/data_science/hello_world.html#pipeline-with-three-components",
    "href": "posts/data_science/hello_world.html#pipeline-with-three-components",
    "title": "Hello World AML pipeline with component",
    "section": "",
    "text": "We add now a third component, which takes as input test data, preprocesses it, and uses the model to perform “inference”. For this pipeline, we can reuse the training component from the last section, but we need to slighty modify the preprocessing component to use an additional argument: the name of the output file. This is needed because there will be two outputs: one when preprocessing the training data, and the other when preprocessing the test data.\n\n\n\nos.makedirs (\"inference\", exist_ok=True)\ntest_data = pd.DataFrame (\n    {\n        \"a\": [11., 12.1, 13.1],\n        \"b\": [14.1, 15.1, 16.1],\n    }\n)\ntest_data.to_csv (\"data/dummy_test.csv\")\n\n\n\n\n\n%%writefile preprocessing/preprocessing.py\nimport argparse\nimport pandas as pd\n\ndef preprocessing (df, x):\n    \"\"\"Adds `x` to input data frame `df`.\"\"\"\n    \n    print (\"Input\\n\", df)\n    print (f\"Adding {x} to df\")\n    df = df + x\n    print (\"Output\\n\", df)\n    return df\n\ndef parse_args ():\n    \"\"\"Parses input arguments\"\"\"\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--input_data\", type=str, help=\"path to input data *file*\")\n    parser.add_argument(\"--preprocessed_data\", type=str, help=\"path to output data *folder* containing the preprocessed data.\")\n    parser.add_argument(\"--preprocessed_file_name\", type=str, help=\"name of preprocessed file name.\")\n    parser.add_argument(\"-x\", type=int, help=\"number to add\")\n    args = parser.parse_args()\n    \n    return args\n\ndef read_and_preprocess (\n    input_data: str,\n    preprocessed_data: str,\n    preprocessed_file_name: str,\n    x: int,\n):\n    df = pd.read_csv (input_data, index_col=0)\n    df = preprocessing (df, x)\n    df.to_csv (f\"{preprocessed_data}/{preprocessed_file_name}\")\n    \ndef main():\n    \"\"\"Main function of the script.\"\"\"\n    \n    args = parse_args ()\n    read_and_preprocess (\n        input_data=args.input_data, \n        preprocessed_data=args.preprocessed_data,\n        preprocessed_file_name=args.preprocessed_file_name,\n        x=args.x, \n    )\n\nif __name__ == \"__main__\":\n    main()\n\nOverwriting preprocessing/preprocessing.py\n\n\n\npreprocessing_command = command(\n    inputs=dict(\n        input_data=Input (type=\"uri_file\"),\n        x=Input (type=\"number\"),\n        preprocessed_file_name=Input (type=\"string\"),\n    ),\n    outputs=dict(\n        preprocessed_data=Output (type=\"uri_folder\"),\n    ),\n    code=f\"./preprocessing/\",  # location of source code: in this case, the root folder\n    command=\"python preprocessing.py --input_data ${{inputs.input_data}} -x ${{inputs.x}} --preprocessed_data ${{outputs.preprocessed_data}} --preprocessed_file_name ${{inputs.preprocessed_file_name}}\",\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"Pre-processing\",\n)\npreprocessing_component = ml_client.create_or_update(preprocessing_command.component)\n\nUploading preprocessing (0.0 MBs): 100%|██████████| 1343/1343 [00:00&lt;00:00, 46879.53it/s]\n\n\n\n\nThe preprocessing component doesn’t change from the last pipeline, see “Using uri_folder” section.\n\n\n\n\n%%writefile inference/inference.py\nimport argparse\nimport joblib\nimport pandas as pd\nfrom typing import Tuple\nimport numpy as np\n\ndef inference (\n    model: Tuple[np.ndarray, np.ndarray], \n    df: pd.DataFrame,\n):\n    \"\"\"\n    Runs dummy inference on new data `df`\n    \"\"\"\n    (mu, std) = model\n    z_df = (df - mu) / std\n    print (\"Inference result:\")\n    print (z_df)\n    return z_df\n\ndef parse_args ():\n    \"\"\"Parses input arguments\"\"\"\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--test_data\", type=str, help=\"path to test data *folder*\")\n    parser.add_argument(\"--test_data_file_name\", type=str, help=\"name of test data file name.\")\n    parser.add_argument(\"--model\", type=str, help=\"path to built model *folder*\")\n    parser.add_argument(\"--inference_output\", type=str, help=\"path to inference result *folder*\")\n    args = parser.parse_args()\n    \n    return args\n\ndef read_and_inference (\n    test_data: str,\n    test_data_file_name: str,\n    model_path: str,\n    inference_data: str,\n):\n    \"\"\"\n    Reads test data and model, performs inference, and writes to output inference file.\n    \n    Parameters\n    ----------\n    test_data: str\n        Path to test (preprocessed) data *folder*\n    test_data_file_name: str\n        Name of test data file.\n    model_path: str\n        Path to built model *folder*\n    inference_data: str\n        Path to inference result *folder*\n    \"\"\"\n    df = pd.read_csv (f\"{test_data}/{test_data_file_name}\", index_col=0)\n    model = joblib.load (model_path + \"/model.pk\")\n    z_df = inference (model, df)\n    z_df.to_csv (inference_data + \"/inference_result.csv\")\n\ndef main():\n    \"\"\"Main function of the script.\"\"\"\n    \n    args = parse_args ()\n    read_and_inference (\n        test_data=args.test_data, \n        test_data_file_name=args.test_data_file_name,\n        model_path=args.model, \n        inference_data=args.inference_output,\n    )\n\nif __name__ == \"__main__\":\n    main()\n\nOverwriting inference/inference.py\n\n\n\ninference_command = command(\n    inputs=dict(\n        test_data=Input (type=\"uri_folder\"),\n        test_data_file_name=Input (type=\"string\"),\n        model=Input (type=\"uri_folder\"),\n    ),\n    outputs=dict(\n        inference_output=Output (type=\"uri_folder\"),\n    ),\n    code=f\"./inference/\",  # location of source code: in this case, the root folder\n    command=\"python inference.py \" \n            \"--test_data ${{inputs.test_data}} \"\n            \"--test_data_file_name ${{inputs.test_data_file_name}} \"\n            \"--model ${{inputs.model}} \"\n            \"--inference_output ${{outputs.inference_output}}\",\n\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"inference\",\n)\ninference_component = ml_client.create_or_update(inference_command.component)\n\nUploading inference (0.0 MBs): 100%|██████████| 1912/1912 [00:00&lt;00:00, 63810.98it/s]\n\n\n\n\n\n\n\nBefore submitting the pipeline job, it is very important to test it first, ideally with some dummy or small dataset. For this purpose, in the component implementation above, we have separated the code related with argument parsing and the rest of the code, which is in encapsulated in a function called read_and_&lt;...&gt;. This way, we can easily write a test pipeline before implementing the final one, as follows:\n\n# We will need to change the code as we iteratively refine it \n# while testing the pipeline. For that purpose, we use the \n# reload module\nfrom importlib import reload \nfrom preprocessing import preprocessing\nfrom training import training\nfrom inference import inference\n\nreload (preprocessing)\nreload (training)\nreload (inference)\n\ndef test_pipeline (\n    pipeline_job_data_input: str,\n    pipeline_job_x: int,\n    pipeline_job_test_input: str,\n    pipeline_preprocessed_file_name: str,\n    pipeline_test_file_name: str,\n    \n    # The following parameters are not present in the final pipeline:\n    pipeline_job_preprocess_output: str,\n    pipeline_job_test_output: str,\n    pipeline_job_model_output: str,\n    pipeline_job_inference_output: str,\n):\n    \"\"\"\n    Tests third pipeline: preprocessing, training and inference.\n    \n    Parameters\n    ----------\n    pipeline_job_data_input: str\n        Path to input data *file*\n    pipeline_job_x: int\n        Integer to add to input data to convert it to \"preprocessed\" data.\n    pipeline_job_test_input: str\n        Path to (preprocessed) test input *file*\n    pipeline_job_test_input: str\n        Path to (preprocessed) test input *file*\n    pipeline_preprocessed_file_name: str\n        Name of (preprocessed) input data file.\n    pipeline_test_file_name: str\n        Name of (preprocessed) test data file.\n    pipeline_job_preprocess_output: str\n        Path to preprocessed data *folder*, to be used as training.\n        Not present in the final pipeline.\n    pipeline_job_test_output: str\n        Path to preprocessed test data *folder*, to be used for inferencing.\n        Not present in the final pipeline.\n    pipeline_job_model_output: str\n        Path to model *folder*. Not present in the final pipeline.\n    pipeline_job_inference_output: str\n        Path to inference result *folder*. Not present in the final pipeline.\n    \"\"\"\n    \n    preprocessing.read_and_preprocess (\n        pipeline_job_data_input,\n        pipeline_job_preprocess_output,\n        pipeline_preprocessed_file_name,\n        pipeline_job_x,\n    )\n    preprocessing.read_and_preprocess (\n        pipeline_job_test_input,\n        pipeline_job_test_output,\n        pipeline_test_file_name,\n        pipeline_job_x,\n    )\n    training.read_and_train (\n        pipeline_job_preprocess_output,\n        pipeline_job_model_output,\n    )\n    inference.read_and_inference (\n        test_data=pipeline_job_test_output,\n        test_data_file_name=pipeline_test_file_name,\n        model_path=pipeline_job_model_output,\n        inference_data=pipeline_job_inference_output,\n    )\n\nos.makedirs (\"test_pipeline\", exist_ok=True)\n\ntest_pipeline (\n    pipeline_job_data_input=\"./data/dummy_input.csv\",\n    pipeline_job_x=10,\n    pipeline_job_test_input=\"./data/dummy_test.csv\",\n    pipeline_preprocessed_file_name=\"preprocessed_data.csv\",\n    pipeline_test_file_name=\"preprocessed_test.csv\",\n    \n    # The following parameters are not present in the final pipeline:\n    pipeline_job_preprocess_output=\"./test_pipeline\",\n    pipeline_job_test_output=\"./test_pipeline\",\n    pipeline_job_model_output=\"./test_pipeline\",\n    pipeline_job_inference_output=\"./test_pipeline\",\n)\n\nInput\n    a  b\n0  1  4\n1  2  5\n2  3  6\nAdding 10 to df\nOutput\n     a   b\n0  11  14\n1  12  15\n2  13  16\nInput\n       a     b\n0  11.0  14.1\n1  12.1  15.1\n2  13.1  16.1\nAdding 10 to df\nOutput\n       a     b\n0  21.0  24.1\n1  22.1  25.1\n2  23.1  26.1\nInput\n     a   b\n0  11  14\n1  12  15\n2  13  16\nmu:\n [12. 15.]\nstd:\n [1. 1.]\nInference result:\n      a     b\n0   9.0   9.1\n1  10.1  10.1\n2  11.1  11.1\n\n\n\n\n\n\n@dsl.pipeline(\n    compute=\"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n    description=\"E2E hello world pipeline with input\",\n)\ndef three_components_pipeline(\n    pipeline_job_data_input: str,\n    pipeline_job_x: int,\n    pipeline_job_test_input: str,\n    pipeline_preprocessed_file_name: str,\n    pipeline_test_file_name: str,\n):\n    \"\"\"\n    Third pipeline: preprocessing, training and inference.\n    \n    Parameters\n    ----------\n    pipeline_job_data_input: str\n        Path to input data *file*\n    pipeline_job_x: int\n        Integer to add to input data to convert it to \"preprocessed\" data.\n    pipeline_job_test_input: str\n        Path to (preprocessed) test input *file*\n    pipeline_preprocessed_file_name: str\n        Name of (preprocessed) input data file.\n    pipeline_test_file_name: str\n        Name of (preprocessed) test data file.\n    \"\"\"\n    # using data_prep_function like a python call with its own inputs\n    preprocessing_job = preprocessing_component(\n        input_data=pipeline_job_data_input,\n        x=pipeline_job_x,\n        preprocessed_file_name=pipeline_preprocessed_file_name,\n    )\n    \n    preprocessing_test_job = preprocessing_component(\n        input_data=pipeline_job_test_input,\n        x=pipeline_job_x,\n        preprocessed_file_name=pipeline_test_file_name,\n    )\n\n    # using train_func like a python call with its own inputs\n    training_job = training_component(\n        preprocessed_data=preprocessing_job.outputs.preprocessed_data,  # note: using outputs from previous step\n    )\n    \n    # using train_func like a python call with its own inputs\n    inference_job = inference_component(\n        test_data=preprocessing_test_job.outputs.preprocessed_data,\n        test_data_file_name=pipeline_test_file_name,\n        model=training_job.outputs.model,  # note: using outputs from previous step\n    )\n    \nthree_components_pipeline = three_components_pipeline(\n    pipeline_job_data_input=Input(type=\"uri_file\", path=\"./data/dummy_input.csv\"),\n    pipeline_job_x=10,\n    pipeline_job_test_input=Input(type=\"uri_file\", path=\"./data/dummy_test.csv\"),\n    pipeline_preprocessed_file_name=\"preprocessed_data.csv\",\n    pipeline_test_file_name=\"preprocessed_test_data.csv\",\n)\n\nthree_components_pipeline_job = ml_client.jobs.create_or_update(\n    three_components_pipeline,\n    # Project's name\n    experiment_name=\"e2e_three_components_pipeline_with_uri_folder\",\n)\n\n# ----------------------------------------------------\n# Pipeline running\n# ----------------------------------------------------\nml_client.jobs.stream(three_components_pipeline_job.name)\n\nRunId: calm_rice_3cmmmtc5mf\nWeb View: https://ml.azure.com/runs/calm_rice_3cmmmtc5mf?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2024-03-28 10:15:12Z] Submitting 2 runs, first five are: 47ca85c2:a5595dea-3117-47e9-a99d-186b0c346884,4b1f0180:09aecbcc-aa2c-4cad-b134-eb4837724f58\n[2024-03-28 10:20:13Z] Completing processing run id 09aecbcc-aa2c-4cad-b134-eb4837724f58.\n[2024-03-28 10:20:13Z] Submitting 1 runs, first five are: fece868d:164d1e2a-a5d7-4081-9a68-a07033a3feee\n[2024-03-28 10:20:45Z] Completing processing run id 164d1e2a-a5d7-4081-9a68-a07033a3feee.\n[2024-03-28 10:22:15Z] Completing processing run id a5595dea-3117-47e9-a99d-186b0c346884.\n[2024-03-28 10:22:16Z] Submitting 1 runs, first five are: a9d60b27:6a18ae74-2a3c-4e16-a6cb-a58649235bfe\n[2024-03-28 10:22:52Z] Completing processing run id 6a18ae74-2a3c-4e16-a6cb-a58649235bfe.\n\nExecution Summary\n=================\nRunId: calm_rice_3cmmmtc5mf\nWeb View: https://ml.azure.com/runs/calm_rice_3cmmmtc5mf?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\n\n\nHere we can see the resulting pipeline:"
  },
  {
    "objectID": "posts/data_science/hello_world.html#refactoring",
    "href": "posts/data_science/hello_world.html#refactoring",
    "title": "Hello World AML pipeline with component",
    "section": "",
    "text": "From the development of the different pipelines we can extract a few observations that help us create a better refactored pipeline and, at the same time, compile a small set of “design” rules that may help us in future pipelines. In my case, I find it clearer and with less boilerplate to use the following rules:\n\nUse “uri_folder” type for intermediate outputs, and add another parameter next to it containing the output file, something like:\n\ndef pipeline(\n    ...\n    input_folder: str,\n    input_filename: str,\n    ...\n)\n\nUse “_folder” as a prefix for parameters of type uri_folder, “_file” for those of type uri_file, and “_filename” for those indicating names of file names.\nUse the suffix input for those things that are inputs and ouputfor those that are outputs.\nI’m not clear about this one. Many people usually pass a dataframe called df or a numpy array called X, which is passed from one step of the pipeline to the next, without appending words to the name that talk about the content of the dataframe or the array (e.g., use “X” instead of “preprocessed_X” or “inference_result_X”). I tend to find it easier to do a similar thing here for the inputs of intermediate components, when defining the command for those components. Therefore, for the inputs, I would use input_folder rather than preprocessed_training_data_input_folder for indicating the input to the model component. This means that if we replace later the model component with one that works directly on raw (non-preprocessed) data (e.g., because the preprocessing is implicitly done as part of the model), we don’t need to replace the part of the script that parses the arguments to indicate that now the input folder is just training_data_input_folder.\nFor the outputs, it might be useful to add a short prefix to talk about the type of output, so that the pipeline’s diagram shows what is the ouput of each component is. Again, I’m not clear about this one.\nThe exception to the previous rules is when we have more than one input or output folder. In this case, we clearly need to add more words to their names.\nIt is easier to avoid adding pipeline_job_... for each parameter of the pipeline."
  },
  {
    "objectID": "posts/data_science/hello_world.html#final-pipeline",
    "href": "posts/data_science/hello_world.html#final-pipeline",
    "title": "Hello World AML pipeline with component",
    "section": "",
    "text": "%%writefile preprocessing/preprocessing.py\nimport argparse\nimport pandas as pd\n\ndef preprocessing (\n    df: pd.DataFrame, \n    x: int\n):\n    \"\"\"Adds `x` to input data frame `df`.\n\n    Parameters\n    ----------\n    df: DataFrame\n        Input data frame \n    x: int\n        Integer to add to df.\n\n    Returns\n    -------\n    DataFrame.\n        Preprocessed data.\n    \"\"\"\n    \n    print (\"Input\\n\", df)\n    print (f\"Adding {x} to df\")\n    df = df + x\n    print (\"Output\\n\", df)\n    return df\n\ndef parse_args ():\n    \"\"\"Parses input arguments\"\"\"\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--input_file\", type=str, help=\"path to input data file\")\n    parser.add_argument(\"--output_folder\", type=str, help=\"path to output data folder containing the preprocessed data.\")\n    parser.add_argument(\"--output_filename\", type=str, help=\"name of file containing the output, preprocessed, data.\")\n    parser.add_argument(\"-x\", type=int, help=\"number to add to input data for preprocessing it.\")\n    args = parser.parse_args()\n    \n    return args\n\ndef read_and_preprocess (\n    input_file: str,\n    output_folder: str,\n    output_filename: str,\n    x: int,\n):\n    \"\"\"Reads input data, preprocesses it, and writes result as csv file in disk.\n\n    Parameters\n    ----------\n    input_file: str\n        Path to input data file.\n    output_folder: str\n        Path to output data folder containing the preprocessed data.\n    output_filename: str\n        Name of file containing the output, preprocessed, data.\n    x: int\n        Number to add to input data for preprocessing it.\n    \"\"\"\n    df = pd.read_csv (input_file, index_col=0)\n    df = preprocessing (df, x)\n    df.to_csv (f\"{output_folder}/{output_filename}\")\n    \ndef main():\n    \"\"\"Main function of the script.\"\"\"\n    \n    args = parse_args ()\n    read_and_preprocess (\n        input_file=args.input_file, \n        output_folder=args.output_folder,\n        output_filename=args.output_filename,\n        x=args.x, \n    )\n\nif __name__ == \"__main__\":\n    main()\n\nOverwriting preprocessing/preprocessing.py\n\n\n\npreprocessing_command = command(\n    inputs=dict(\n        input_file=Input (type=\"uri_file\"),\n        x=Input (type=\"number\"),\n        output_filename=Input (type=\"string\"),\n    ),\n    outputs=dict(\n        output_folder=Output (type=\"uri_folder\"),\n    ),\n    code=f\"./preprocessing/\",  # location of source code: in this case, the root folder\n    command=\"python preprocessing.py \"\n        \"--input_file ${{inputs.input_file}} \"\n        \"-x ${{inputs.x}} \"\n        \"--output_folder ${{outputs.output_folder}} \"\n        \"--output_filename ${{inputs.output_filename}}\",\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"Pre-processing\",\n)\npreprocessing_component = ml_client.create_or_update(preprocessing_command.component)\n\nUploading preprocessing (0.0 MBs): 100%|██████████| 1985/1985 [00:00&lt;00:00, 64070.90it/s]\n\n\n\n\n\n\n\n\n%%writefile training/training.py\nimport argparse\nimport joblib\nimport pandas as pd\n\ndef train_model (df: pd.DataFrame):\n    \"\"\"Trains a dummy Gaussian model from training set df.\n    \n    Parameters\n    ----------\n    df: DataFrame\n        Input data frame\n    \n    Returns\n    -------\n    np.ndarray\n        Average across rows, one per column.\n    np.ndarray\n        Standard deviation across rows, one per column.\n    \"\"\"\n    \n    print (\"Input\\n\", df)\n    mu = df.mean().values\n    std = df.std().values\n    print (\"mu:\\n\", mu)\n    print (\"std:\\n\", std)\n    return mu, std\n\ndef parse_args ():\n    \"\"\"Parses input arguments\"\"\"\n    \n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\n        \"--input_folder\", \n        type=str, \n        help=\"path to preprocessed training data folder, \"\n             \"containing training set file.\"\n    )\n    parser.add_argument(\n        \"--input_filename\", \n        type=str, \n        help=\"name of file containing preprocessed, training data.\"\n    )\n    parser.add_argument(\n        \"--output_folder\", \n        type=str, \n        help=\"path to output *folder* containing the trained model.\"\n    )\n    parser.add_argument(\n        \"--output_filename\", \n        type=str, \n        help=\"name of file containing the trained model.\"\n    )\n    args = parser.parse_args()\n    \n    return args\n\ndef read_and_train (\n    input_folder: str,\n    input_filename: str,\n    output_folder: str,\n    output_filename: str,\n):\n    \"\"\"Reads training data, trains model, and saves it.\n    \n    Parameters\n    ----------\n    input_folder: str\n        Path to preprocessed training data folder containing training set file.\n    input_filename: str\n        Name of file containing preprocessed, training data.\n    output_folder: str\n        Path to output folder containing the trained model.\n    output_filename: str\n        Name of file containing the trained model.\n    \"\"\"\n    \n    df = pd.read_csv (f\"{input_folder}/{input_filename}\", index_col=0)\n    model = train_model (df)\n    joblib.dump (model, f\"{output_folder}/{output_filename}\")\n\ndef main():\n    \"\"\"Main function of the script.\"\"\"\n    \n    args = parse_args ()\n    read_and_train (\n        args.input_folder, \n        args.input_filename,\n        args.output_folder,\n        args.output_filename,\n    )\n\nif __name__ == \"__main__\":\n    main()\n\nOverwriting training/training.py\n\n\n\n# Component definition and registration\ntraining_command = command(\n    inputs=dict(\n        input_folder=Input (type=\"uri_folder\"),\n        input_filename=Input (type=\"string\"),\n        output_filename=Input (type=\"string\"),\n    ),\n    outputs=dict(\n        output_folder=Output (type=\"uri_folder\"),\n    ),\n    code=f\"./training/\",  # location of source code: in this case, the root folder\n    command=\"python training.py \"\n        \"--input_folder ${{inputs.input_folder}} \"\n        \"--input_filename ${{inputs.input_filename}} \"\n        \"--output_folder ${{outputs.output_folder}} \"\n        \"--output_filename ${{inputs.output_filename}}\",\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"Training\",\n)\n\ntraining_component = ml_client.create_or_update(training_command.component)\n\nUploading training (0.0 MBs): 100%|██████████| 2309/2309 [00:00&lt;00:00, 50917.16it/s]\n\n\n\n\n\n\n\n\n%%writefile inference/inference.py\nimport argparse\nfrom typing import Tuple\nimport joblib\nimport pandas as pd\nfrom sklearn.metrics import DistanceMetric\nimport numpy as np\n\ndef inference (\n    model: Tuple[np.ndarray, np.ndarray], \n    df: pd.DataFrame,\n):\n    \"\"\"Runs dummy inference on new data `df`.\n\n    Parameters\n    ----------\n    model: Tople (np.ndarray, np.ndarray)\n        Average across rows (one per column), and \n        standard deviation across rows (one per column).\n    df: DataFrame\n        Test data frame on which to perform inference.\n    \n    Returns\n    -------\n    DataFrame\n        One column dataframe giving an approximation of the Mahalanobis distance \n        between each row vector and the mean vector, assuming that the covariance \n        matrix is diagonal. The negative of the scores obtained can be considered \n        as a sort of prediction probability for each row of belonging to the Gaussian \n        class estimated from the training data. In this sense this function provides\n        inference about how \"normal\" the test samples are. \n    \"\"\"\n    (mu, std) = model\n    dist = DistanceMetric.get_metric('mahalanobis', V=np.diag(std**2))\n    ndims = df.shape[1]\n    mah_dist = dist.pairwise (mu.reshape(1, ndims), df)\n    mah_dist = pd.DataFrame (mah_dist.ravel(), columns=[\"distance\"])\n    print (\"Inference result:\")\n    print (mah_dist)\n    return mah_dist\n\ndef parse_args ():\n    \"\"\"Parses input arguments\"\"\"\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--preprocessed_input_folder\", \n        type=str, \n        help=\"path to input, preprocessed, test data folder, \"\n             \"containing file on which to perform inference.\"\n    )\n    parser.add_argument(\n        \"--preprocessed_input_filename\", \n        type=str, \n        help=\"name of file containing the input, preprocessed, test data.\"\n    )\n    parser.add_argument(\n        \"--model_input_folder\", \n        type=str, \n        help=\"path to model folder.\"\n    )\n    parser.add_argument(\n        \"--model_input_filename\", \n        type=str, \n        help=\"name of model file.\"\n    )\n    parser.add_argument(\n        \"--output_folder\", \n        type=str, \n        help=\"path to output data *folder* with inference results.\"\n    )\n    parser.add_argument(\n        \"--output_filename\", \n        type=str, \n        help=\"name of file containing the output data with inference results.\"\n    )\n    \n    args = parser.parse_args()\n\n    args = parser.parse_args()\n    \n    return args\n\ndef read_and_inference (\n    preprocessed_input_folder: str,\n    preprocessed_input_filename: str,\n    model_input_folder: str,\n    model_input_filename: str,\n    output_folder: str,    \n    output_filename: str,\n):\n    \"\"\"\n    Reads test data and model, performs inference, and writes to output inference file.\n    \n    Parameters\n    ----------\n    preprocessed_input_folder: str\n        Path to test (preprocessed) data folder.\n    preprocessed_input_filename: str\n        Name of test data file.\n    model_input_folder: str\n        Path to built model folder.\n    model_input_filename: str\n        Path to inference result folder.\n    output_folder: str\n        Path to output data folder with inference results.\n    output_filename: str\n        Name of file containing the output data with inference results.\n    \"\"\"\n    df = pd.read_csv (f\"{preprocessed_input_folder}/{preprocessed_input_filename}\", index_col=0)\n    model = joblib.load (f\"{model_input_folder}/{model_input_filename}\")\n    z_df = inference (model, df)\n    z_df.to_csv (f\"{output_folder}/{output_filename}\")\n\ndef main():\n    \"\"\"Main function of the script.\"\"\"\n    \n    args = parse_args ()\n    read_and_inference (\n        preprocessed_input_folder=args.preprocessed_input_folder,\n        preprocessed_input_filename=args.preprocessed_input_filename,\n        model_input_folder=args.model_input_folder,\n        model_input_filename=args.model_input_filename,\n        output_folder=args.output_folder,\n        output_filename=args.output_filename,\n    )\n\nif __name__ == \"__main__\":\n    main()\n\nOverwriting inference/inference.py\n\n\n\ninference_command = command(\n    inputs=dict(\n        preprocessed_input_folder=Input (type=\"uri_folder\"),\n        preprocessed_input_filename=Input (type=\"string\"),\n        model_input_folder=Input (type=\"uri_folder\"),\n        model_input_filename=Input (type=\"string\"),\n        output_filename=Input (type=\"string\"),\n    ),\n    outputs=dict(\n        output_folder=Output (type=\"uri_folder\"),\n    ),\n    code=f\"./inference/\",  # location of source code: in this case, the root folder\n    command=\"python inference.py \" \n        \"--preprocessed_input_folder ${{inputs.preprocessed_input_folder}} \"\n        \"--preprocessed_input_filename ${{inputs.preprocessed_input_filename}} \"\n        \"--model_input_folder ${{inputs.model_input_folder}} \"\n        \"--model_input_filename ${{inputs.model_input_filename}} \"\n        \"--output_folder ${{outputs.output_folder}} \"\n        \"--output_filename ${{inputs.output_filename}} \",\n\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n    display_name=\"inference\",\n)\ninference_component = ml_client.create_or_update(inference_command.component)\n\nUploading inference (0.0 MBs): 100%|██████████| 4046/4046 [00:00&lt;00:00, 151355.71it/s]\n\n\n\n\n\n\n\nBefore submitting the pipeline job, it is very important to test it first, ideally with some dummy or small dataset. For this purpose, in the component implementation above, we have separated the code related with argument parsing and the rest of the code, which is in encapsulated in a function called read_and_&lt;...&gt;. This way, we can easily write a test pipeline before implementing the final one, as follows:\n\n# We will need to change the code as we iteratively refine it \n# while testing the pipeline. For that purpose, we use the \n# reload module\nfrom importlib import reload \nfrom preprocessing import preprocessing\nfrom training import training\nfrom inference import inference\n\nreload (preprocessing)\nreload (training)\nreload (inference)\n\ndef test_pipeline (\n    # Preprocessing component parameters, first component:\n    preprocessing_training_input_file: str,\n    preprocessing_training_output_folder: str, # Not present in final pipeline\n    preprocessing_training_output_filename: str,\n    x: int,\n    \n    # Preprocessing component parameters, second component:\n    preprocessing_test_input_file: str,\n    preprocessing_test_output_folder: str, # Not present in final pipeline\n    preprocessing_test_output_filename: str,\n    \n    # Training component parameters:\n    # input_folder: this is preprocessing_training_output_folder\n    # input_filename: this is preprocessing_training_output_filename\n    training_output_folder: str, # Not present in final pipeline\n    training_output_filename: str, \n    \n    # Inference component parameters:\n    # preprocessed_input_folder: this is preprocessing_test_output_folder\n    # preprocessed_input_filename: this is preprocessing_test_output_filename\n    # model_input_folder: this is training_output_folder\n    # model_input_filename: this is training_output_filename\n    inference_output_folder: str, # Not present in final pipeline\n    inference_output_filename: str,\n):\n    \"\"\"\n    Tests third pipeline: preprocessing, training and inference.\n    \n    Parameters\n    ----------\n    preprocessing_training_input_file: str\n        Path to file containing training data to be preprocessed.\n    preprocessing_training_output_folder: str\n        Path to folder containing the preprocessed, training data file.\n        Not present in final pipeline.\n    preprocessing_training_output_filename: str\n        Name of file containing the preprocessed, training data.\n    x: int\n        Number to add to input data for preprocessing it.\n    preprocessing_test_input_file: str\n        Path to file containing test data to be preprocessed.\n    preprocessing_test_output_folder: str\n        Path to folder containing the preprocessed, test data file.\n        Not present in final pipeline.\n    preprocessing_test_output_filename: str\n        Name of file containing the preprocessed, test data.\n    training_output_folder: str\n        Path to output folder containing the trained model.\n        Not present in final pipeline.\n    training_output_filename: str\n        Name of file containing the trained model.\n    inference_output_folder: str\n        Path to output data folder with inference results.\n        Not present in final pipeline.\n    inference_output_filename: str\n        Name of file containing the output data with inference results.\n    \"\"\"\n    \n    preprocessing.read_and_preprocess (\n        input_file=preprocessing_training_input_file,\n        output_folder=preprocessing_training_output_folder, # Not present in final component\n        output_filename=preprocessing_training_output_filename,\n        x=x,\n    )\n    preprocessing.read_and_preprocess (\n        input_file=preprocessing_test_input_file,\n        output_folder=preprocessing_test_output_folder,\n        output_filename=preprocessing_test_output_filename,\n        x=x,\n    )\n    training.read_and_train (\n        input_folder=preprocessing_training_output_folder,\n        input_filename=preprocessing_training_output_filename,\n        output_folder=training_output_folder,\n        output_filename=training_output_filename,\n    )\n    inference.read_and_inference (\n        preprocessed_input_folder=preprocessing_test_output_folder,\n        preprocessed_input_filename=preprocessing_test_output_filename,\n        model_input_folder=training_output_folder,\n        model_input_filename=training_output_filename,\n        output_folder=inference_output_folder,\n        output_filename=inference_output_filename,\n    )\n\nos.makedirs (\"test_pipeline\", exist_ok=True)\n\ntest_pipeline (\n    # first preprocessing component\n    preprocessing_training_input_file=\"./data/dummy_input.csv\",\n    preprocessing_training_output_folder=\"./test_pipeline\", # Not present in final pipeline\n    preprocessing_training_output_filename=\"preprocessed_data.csv\",\n    x=10,\n    \n    # second preprocessing component\n    preprocessing_test_input_file=\"./data/dummy_test.csv\",\n    preprocessing_test_output_folder=\"./test_pipeline\", # Not present in final pipeline\n    preprocessing_test_output_filename=\"preprocessed_test.csv\",\n    \n    # Training component parameters:\n    training_output_folder=\"./test_pipeline\", # Not present in final pipeline\n    training_output_filename=\"model.pk\",\n    \n    # Inference component parameters:\n    inference_output_folder=\"./test_pipeline\", # Not present in final pipeline\n    inference_output_filename=\"inference_result.csv\",\n)\n\nInput\n    a  b\n0  1  4\n1  2  5\n2  3  6\nAdding 10 to df\nOutput\n     a   b\n0  11  14\n1  12  15\n2  13  16\nInput\n       a     b\n0  11.0  14.1\n1  12.1  15.1\n2  13.1  16.1\nAdding 10 to df\nOutput\n       a     b\n0  21.0  24.1\n1  22.1  25.1\n2  23.1  26.1\nInput\n     a   b\n0  11  14\n1  12  15\n2  13  16\nmu:\n [12. 15.]\nstd:\n [1. 1.]\nInference result:\n    distance\n0  12.798828\n1  14.283557\n2  15.697771\n\n\n\n\n\n\n@dsl.pipeline(\n    compute=\"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n    description=\"E2E hello world pipeline with input\",\n)\ndef three_components_pipeline(\n    # Preprocessing component parameters, first component:\n    preprocessing_training_input_file: str,\n    preprocessing_training_output_filename: str,\n    x: int,\n    \n    # Preprocessing component parameters, second component:\n    preprocessing_test_input_file: str,\n    preprocessing_test_output_filename: str,\n    \n    # Training component parameters:\n    training_output_filename: str, \n    \n    # Inference component parameters:\n    inference_output_filename: str,\n):\n    \"\"\"\n    Third pipeline: preprocessing, training and inference.\n    \n    Parameters\n    ----------\n    preprocessing_training_input_file: str\n        Path to file containing training data to be preprocessed.\n    preprocessing_training_output_filename: str\n        Name of file containing the preprocessed, training data.\n    x: int\n        Number to add to input data for preprocessing it.\n    preprocessing_test_input_file: str\n        Path to file containing test data to be preprocessed.\n    preprocessing_test_output_filename: str\n        Name of file containing the preprocessed, test data.\n    training_output_filename: str\n        Name of file containing the trained model.\n    inference_output_filename: str\n        Name of file containing the output data with inference results.\n    \"\"\"\n    # using data_prep_function like a python call with its own inputs\n    preprocessing_training_job = preprocessing_component(\n        input_file=preprocessing_training_input_file,\n        #output_folder: automatically determined\n        output_filename=preprocessing_training_output_filename,\n        x=x,\n    )\n    preprocessing_test_job = preprocessing_component(\n        input_file=preprocessing_test_input_file,\n        #output_folder: automatically determined\n        output_filename=preprocessing_test_output_filename,\n        x=x,\n    )\n    training_job = training_component(\n        input_folder=preprocessing_training_job.outputs.output_folder,\n        input_filename=preprocessing_training_output_filename,\n        #output_folder: automatically determined\n        output_filename=training_output_filename,\n    )\n    inference_job = inference_component(\n        preprocessed_input_folder=preprocessing_test_job.outputs.output_folder,\n        preprocessed_input_filename=preprocessing_test_output_filename,\n        model_input_folder=training_job.outputs.output_folder,\n        model_input_filename=training_output_filename,\n        #output_folder: automatically determined\n        output_filename=inference_output_filename,\n    )\n    \nthree_components_pipeline = three_components_pipeline(\n    # first preprocessing component\n    preprocessing_training_input_file=Input(type=\"uri_file\", path=\"./data/dummy_input.csv\"),\n    preprocessing_training_output_filename=\"preprocessed_training_data.csv\",\n    x=10,\n    \n    # second preprocessing component\n    preprocessing_test_input_file=Input(type=\"uri_file\", path=\"./data/dummy_test.csv\"),\n    preprocessing_test_output_filename=\"preprocessed_test_data.csv\",\n    \n    # Training component parameters:\n    training_output_filename=\"model.pk\",\n    \n    # Inference component parameters:\n    inference_output_filename=\"inference_results.csv\",\n)\n\nthree_components_pipeline_job = ml_client.jobs.create_or_update(\n    three_components_pipeline,\n    # Project's name\n    experiment_name=\"e2e_three_components_refactored\",\n)\n\n# ----------------------------------------------------\n# Pipeline running\n# ----------------------------------------------------\nml_client.jobs.stream(three_components_pipeline_job.name)\n\nClass AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n\n\nRunId: busy_toe_03bv5yshzh\nWeb View: https://ml.azure.com/runs/busy_toe_03bv5yshzh?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2024-03-29 08:45:11Z] Submitting 2 runs, first five are: f39c6442:de45cd94-3bf9-4405-9b9f-59782424a8fb,feeb5198:0b543053-d140-4699-9c29-1ec2e755898e\n[2024-03-29 08:50:08Z] Completing processing run id de45cd94-3bf9-4405-9b9f-59782424a8fb.\n[2024-03-29 08:50:33Z] Completing processing run id 0b543053-d140-4699-9c29-1ec2e755898e.\n[2024-03-29 08:50:34Z] Submitting 1 runs, first five are: 5731fc43:46a9ab31-8a68-42ee-a377-a5e84cbad37d\n[2024-03-29 08:55:36Z] Completing processing run id 46a9ab31-8a68-42ee-a377-a5e84cbad37d.\n[2024-03-29 08:55:36Z] Submitting 1 runs, first five are: 5af83f98:fd50a27d-6687-431a-88d3-b8ba3d8799f4\n[2024-03-29 08:56:12Z] Execution of experiment failed, update experiment status and cancel running nodes.\n\nExecution Summary\n=================\nRunId: busy_toe_03bv5yshzh\nWeb View: https://ml.azure.com/runs/busy_toe_03bv5yshzh?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n\n\nJobException: Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has failed child jobs. Failed nodes: /inference_job. For more details and logs, please go to the job detail page and check the child jobs.\",\n        \"message_format\": \"Pipeline has failed child jobs. {0}\",\n        \"message_parameters\": {},\n        \"reference_code\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"eastus2\",\n    \"location\": \"eastus2\",\n    \"time\": \"2024-03-29T08:56:11.952518Z\",\n    \"component_name\": \"\"\n} \n\n\nHere we can see the resulting pipeline:"
  },
  {
    "objectID": "posts/data_science/index.html",
    "href": "posts/data_science/index.html",
    "title": "Data Science",
    "section": "",
    "text": "Hello World AML pipeline with component\n\n\nExploring AML through Hello World components.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/health/tenacity_and_will_power.html",
    "href": "posts/health/tenacity_and_will_power.html",
    "title": "Tenacity and will power",
    "section": "",
    "text": "This post is just a series of short notes I wrote for myself, on general strategies to achieve tenacity / will power goals."
  },
  {
    "objectID": "posts/health/tenacity_and_will_power.html#introduction",
    "href": "posts/health/tenacity_and_will_power.html#introduction",
    "title": "Tenacity and will power",
    "section": "",
    "text": "This post is just a series of short notes I wrote for myself, on general strategies to achieve tenacity / will power goals."
  },
  {
    "objectID": "posts/health/tenacity_and_will_power.html#strategies",
    "href": "posts/health/tenacity_and_will_power.html#strategies",
    "title": "Tenacity and will power",
    "section": "Strategies",
    "text": "Strategies\nThese are the (evolving) strategies I would like to follow:\n\nClearly write goal objectives before trying. Dieting example: always write I will be eating that day,before eating. This is what I can eat today: menu with 2000 calories for example, 30% fat, 25% protein, 10% sugar.\n\nWrite down times: time when I start breakfast, and duration (end time). Time when I start lunch (if I do it), and duration (end time). Total eating window time (end lunch - beginning breakfast). It should always be lower than 8 hours. No snacks in between.\nWrite down all the coffees and teas, and their times.\nExtra: may be also the regular drinks (e.g., water), to see how this affects sleep.\nAt least something at coarse level, that has just a reasonable accuracy.\n\nDieting or other goals: Always write down what I have eaten (or whatever the goal was). That’s even more important than meeting objectives. I need to know, if I don’t meet them, why I haven’t and what could’ve done better or changed for meeting them. Write this down as well, every time.\n\nAbout writing down what I eat, it doesn’t need to be super-accurate. But at least something at coarse level, that has just a reasonable accuracy.\n\nSleep objectives. If I don’t meet them, what should I’ve done better, what subset of objectives (e.g., number of coffees) I didn’t meet and which strategies I could’ve followed to meet them. Always have empathy and kindness towards myself, not be harsh, but do not stop trying.\nBuy all the necessary wearables: sleep, glucose, lactose and maybe heart rate (because of its relantionship with will power).\nContinue reading, for instance papers about super-agers and how they get will power. Write down a blog summarizing my findings.\nIzan:\n\nwrite down notes from Anita and publish them.\nlook for strategies on savings, and publish them."
  },
  {
    "objectID": "posts/health/huberman_podcast_will_power.html",
    "href": "posts/health/huberman_podcast_will_power.html",
    "title": "Tenacity and will power TIL",
    "section": "",
    "text": "This post is an in-progress Today-I-Learned (TIL) on the awesome Huberman podcast “How to Increase Tenacity & Will Power”"
  },
  {
    "objectID": "posts/health/huberman_podcast_will_power.html#introduction",
    "href": "posts/health/huberman_podcast_will_power.html#introduction",
    "title": "Tenacity and will power TIL",
    "section": "",
    "text": "This post is an in-progress Today-I-Learned (TIL) on the awesome Huberman podcast “How to Increase Tenacity & Will Power”"
  },
  {
    "objectID": "posts/health/huberman_podcast_will_power.html#short-notes-lessons-learned",
    "href": "posts/health/huberman_podcast_will_power.html#short-notes-lessons-learned",
    "title": "Tenacity and will power TIL",
    "section": "Short notes / lessons learned",
    "text": "Short notes / lessons learned\n\nExperts differ on whether Tenacity & Will Power are constant and limited in the individual. I interpret this as whether or not an individual is able to increase their maximum capacity of tenacity & will-power. If the answer is no, this means that each person can only have X amount of tenacity & will power, and nothing they do can increase this amount. That is not the same as saying that our tenacity & will power is constant and cannot change. It can increase throghout life through practice, but it cannot exceed a certain limit, which is fixed. It is this limit or upper bound what is fixed and cannot change.\nOn the previous topic, Dr. Huberman tends to lean towards the camp that, indeed, the maximum achievable Tenacity & Will Power is constant and cannot change.\nOne way of increasing Tenacity & Will Power is through exercise / practice. This means, in a nutshell, to do tasks that one doesn’t want to do, or, conversely don’t engage in activities or behaviours that one would like to engage in, and that are presumably unhealthy or not recommendable in some sense. Huberman informally refers to these tasks as micro-sucks, i.e., things that one doesn’t really feel like doing at some point, but that they are safe and, I interpret, not extremely challenging.\nBiological remarks:\n\nIncreasing testosterone seems to be related with increasing Tenacity & Will Power.\nThere is a specific region of the brain that is strongly associated with Tenacity & Will Power. The bigger this region grows, the more Tenacity & Will Power one has. So-called super-agers (or centenarians, as Dr. Attia calls them) tend to have such regions much bigger than the rest of people. And they tend to engage in activities that they would normally don’t feel like engaging in (at least at the beginning, e.g., things out of their comfort-zone and that they do not feel particularly inclined to do), and avoid unheatlhy or not-recommendable things that they normally desire. Another way of looking at this is that elderly people (in their 60s to 90s) that do not cease trying to improve, evolve, and explore unchartered and mildly not-attractive territories, show in some sense a strong desire of living longer while growing personally, and this usually translates into them living longer. It looks to me, to some extent, as a sort-of self-fulfilling wish."
  },
  {
    "objectID": "posts/health/agency_and_gratitude.html",
    "href": "posts/health/agency_and_gratitude.html",
    "title": "Agency and gratitude",
    "section": "",
    "text": "I would like to start contributing following the framework from Dr. Paul Conti. The basic idea is to do things that make me feel good and can help other people at the same time. This can be small things as:\n\nCommenting on interesting reads.\nContributing to open source: my personal project nbmodular, exercism, clojure data science community, and others.\nDo clojure pipeline ala dsblocks.\nReach out to people that are influencing me and ask them questions that may be of interest to others as well, e.g., in twitter, youtube, etc. Examples: David Sinclair, Andrew Huberman, and Peter Attia.\n\nHuberman: difficult task after tenacy task. Did they try to wait until drive/motivation/dopamine comes back to baseline (from the experimented valley)? Are the results as strong in that case? Idea basically is to separate the experiments for long time.\nSinclair: some of the suggested strategies require a lot of will power and tenacity. Do you know of mental strategies that can help us achieve those goals? Tricks such as drinking tea or coffee can be quite effective, but I wonder about strategies focused on dieting, which involve a large amount of tenacity.\nFast.ai: nbdev_export done, link to test tutorials, include examples / demos.\nFinish tenacity podcast and maybe re-listen Paul Conti podcasts book\nFor each X personal things, do one thing that may help (even just a bit) to the community. - Can be for my son - for others Try to find best strategy here, something that makes me feel good and works.\nPotential next project.\n\nLearn R language.\nApplied Statistics for Data Science: null hypothesis tests, etc.\nVisualization techniques and libraries: blogs, kaggle notebooks, other material.\n\nReveal.js / quarto notebooks.\n\nPapers discussed by Huberman, Attia and maybe Sinclair: analyze them from an statistics point of view and write a blog summarizing the findings. Have a list of blogs about this subject.\n\nPotential pitfalls, which ones are stronger from a statistics viewpoint.\nDo rating. Can even be a website with ratings / open review about this type of papers, automatic review that includes ChatGPT type of analysis, etc. With the AI piece we may be able to automatically find sources references and more context information (opinions, rebuttals, etc.) for these papers.\n\nFind kaggle projects, mostly focused on medicine, that can be good to participate in.\n\nAlso for Kaggle Days and social interaction.\n\nCommit myself to write a blog or something everytime I learn something, e.g., TIL like Simon Willison - see his github’s TIL"
  },
  {
    "objectID": "posts/education/learning_visually.html",
    "href": "posts/education/learning_visually.html",
    "title": "Learning visually",
    "section": "",
    "text": "I recently joined the amazing exercism community. Exercism is an online free coding platform that encourages learning programming languages through practice, free mentorship by volunteers, and exposure to similar solutions once our own solution has been submitted and passes the tests. Each exercise comes with associated learning material, which is a concise description of the concepts that are strictly required for attempting the exercise. However, the focus is always to encourage the students to learn through problem solving, while autonomously investigating the bits that are required. This follows closely recent research on education and neuroscience.\nI would like to share related thoughts on learning through practice, visual learning, and mathematics or related disciplines. The following are just short notes on this matter, that can serve both as a high-level roadmap to a potential long-term project, and contain some references that I would like to keep in mind and expand in the future.\nI found exercism to be a nicely designed platform that, being open source, could be extended to be applied to other domains beyond programming languages, and in particular to math problems. We could then use libraries to describe the problems visually, similar to what 3blue1brown does. By presenting math problems as both visual puzzles and in its original text form, we can make the problem more appealing, following the the ideas of Jo Boaler, e.g., in Mathematical mindsets, and visual mathematics. It would be nice if the student could manipulate the problem visually (i.e., manipulating and transforming the graphical elements with a mouse), and if the problem could be presented not only in 2D but also in 3D (using Virtual Reality, ideally), so that the student is able to also manipulate the objects in a sort-of “physical” world (be it through a game that emulates this world, or through gadgets like glasses and gloves, to have a better sensorial experience, although this would only be accessible by people able to buy those gadgets, unfortunately).\nMichael Nielsen has written very nice essays on these lines, under the title “Tools for Thought”. In particular, his essays Toward an exploratory medium for mathematics, Thought as a Technology, How can we develop transformative tools for thought? and Reinventing Explanation. Similar ideas are recently being developed by many scientists and educators and put in practice in journals such as Distill. I will be including those references in here as I find them, with a brief description, to serve as sources for the future."
  },
  {
    "objectID": "posts/education/index.html",
    "href": "posts/education/index.html",
    "title": "Education and Parenting",
    "section": "",
    "text": "Learning visually\n\n\nShort notes on learning by practice and visualization.\n\n\n\n\n\n\n\n\n\n\nParenting coach notes\n\n\n\n\n\n\n\n\n\n\n\n\n\nResources on Parenting & Early Childhood Education\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject-driven learning\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/others/podcasts.html",
    "href": "posts/others/podcasts.html",
    "title": "Podcasts",
    "section": "",
    "text": "Note: this post is, at this moment, just a draft in progress.\nI have been a podcast and audio-book listener for the last few years. In this post, I would like to list and briefly describe the podcasts that I find interesting, and point to relevant sources."
  },
  {
    "objectID": "posts/others/podcasts.html#list-of-podcasts",
    "href": "posts/others/podcasts.html#list-of-podcasts",
    "title": "Podcasts",
    "section": "List of podcasts",
    "text": "List of podcasts\n\nGeneral\n\nLex Fridman Podcast: long-format (&gt;= 3h) interviews with interesting people of all types: literature, education, health, science, religion, politics… It started focusing on AI (the domain he is expert on), then expanded to science and software in general, then to everything. Very interesting guests IMO, most of them previously unknown by me.\nJoe Rogan Experience: Similar to Lex Fridman podcast. i found many interesting personalities in this podcast, and Joe has this capacity of making every conversation a fascinating one, and showing respect and interest towards points of view that are quite different from his own ones. However, there are few things that make me get away from it sometimes: he tends to bring many UFC fighters and comedians, which are the two things he is expert on. I like martial arts, but this high bias towards this type of guests is not ideal for me. He also tends to criticize previous guests when they are not there, even if he doesn’t do any criticism when the guest is there (at least not during the time I listen, sometimes I stop after one or two hours, and I should wait until the end to be sure about this).\n\n\n\nHealth\n\nHuberman Lab Podcast: mental and physical health, performance, and well-being in general. Dr. Andrew Huberman is a neuroscientist and professor at the Standford School of Medicine. This podcast combines interviews with guests, many of them well-known scientists and pioneers in their field, with discussions about where he covers specific topics that he and his team have been researching for the podcast.\nThe Peter Attia Drive Podcast: focused on latest research for increasing longevity, healthspan and lifespan. Dr. Peter Attia is the author of Outlive, and has a dedicated practice for studying and researching causes and solutions of age-related diseases, and how to stay healthy until the last years of our life.\nDr Chaterjee: mostly interviews with guests from the health sector, occasionally having well-known guests from other domains, usually with some relationship with health and well-being in general.\n\n\n\nEducation\n\nMr Barton Maths Podcast: education in mathematics (mostly secondary education), but extrapolable to education in general, as it talks about research in education, which in turn comes from fields such as neuroscience and psychology, and is applicable to many fields beyond mathematics.\n3blue1brown podcast: interviews, mostly with mathematicians. Even if you don’t like maths (which to be honest it is not something I’m really an expert on), i find the conversations very interesting, and amenable for non-mathematicians like myself.\n\n\n\nParenting\n\nRasing Good Humans: I listened to some of the episodes but still have to listen to more.\nParent-Driven Development: just listened to a couple of podcasts. Informal conversations with parents involved in technology and software in particular: how they teach their children, what strategies they use to keep a good work-life balance, etc.\n\n\n\nMisc.\n\nThe Rest is History: Great podcast about both recent and ancient history, with in-depth coverage of different curious and interesting events, personalities, and cultures.\nThe Rest is Money: I discovered this through the Rest is History. It is a different way to follow recent developments, here focused on the ones having impact on the economy.\nGlobal News Podcast from the BBC. It has two episodes per day, 30 min each."
  },
  {
    "objectID": "additional/data_science/aml/free_subscription.html",
    "href": "additional/data_science/aml/free_subscription.html",
    "title": "Hello World in Azure ML Pipelines",
    "section": "",
    "text": "Go to : https://azure.microsoft.com/en-gb/free/\n\n\n\n\nGo home: https://portal.azure.com/?quickstart=true#home\nOn the top, in the text search box, type “azure machine learning”\n\n\n\nOn the left, click on the + button, and select create workspace\n\n\n\nNote: As part of creating the workspace, you need to create other resources, like the resource group and storage. I created a resource group with the name “helloworld”, and it populated the remaining fields for me. It is very important to avoid underscores, just letters and digits. The first time I did it, I used “hello_world” and after that I tried to change the name and it kept failing, even without underscore. It seems that after logging out and in again, and choosing an appropriate name without underscore, it succeeds.\n\n\nGo to workspace by clicking on the link that appears on the right, with name “Studio web URL”:\n\n\n\n\n\n\nIn my case, I just selected the cheapest recommended option, and named it “jaumecpu”."
  },
  {
    "objectID": "additional/data_science/aml/free_subscription.html#steps",
    "href": "additional/data_science/aml/free_subscription.html#steps",
    "title": "Hello World in Azure ML Pipelines",
    "section": "",
    "text": "Go to : https://azure.microsoft.com/en-gb/free/\n\n\n\n\nGo home: https://portal.azure.com/?quickstart=true#home\nOn the top, in the text search box, type “azure machine learning”\n\n\n\nOn the left, click on the + button, and select create workspace\n\n\n\nNote: As part of creating the workspace, you need to create other resources, like the resource group and storage. I created a resource group with the name “helloworld”, and it populated the remaining fields for me. It is very important to avoid underscores, just letters and digits. The first time I did it, I used “hello_world” and after that I tried to change the name and it kept failing, even without underscore. It seems that after logging out and in again, and choosing an appropriate name without underscore, it succeeds.\n\n\nGo to workspace by clicking on the link that appears on the right, with name “Studio web URL”:\n\n\n\n\n\n\nIn my case, I just selected the cheapest recommended option, and named it “jaumecpu”."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]