<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.555">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Jaume Amores - Playing with MLflow</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Jaume Amores - Playing with MLflow">
<meta property="og:description" content="Personal page">
<meta property="og:site_name" content="Jaume Amores">
<meta property="og:locale" content="en_US">
<meta name="twitter:title" content="Jaume Amores - Playing with MLflow">
<meta name="twitter:description" content="Personal page">
<meta name="twitter:creator" content="@JaumeEire">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Jaume Amores</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../papers.pdf"> 
<span class="menu-text">Papers</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Playing with MLflow</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="initial-imports" class="level2">
<h2 class="anchored" data-anchor-id="initial-imports">Initial imports</h2>
<div id="e9103813-05a4-445a-b450-130d3659a757" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Standard imports</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Third-party imports</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># AML imports</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> azure.ai.ml <span class="im">import</span> command, MLClient</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> azure.identity <span class="im">import</span> DefaultAzureCredential</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="additional-imports" class="level3">
<h3 class="anchored" data-anchor-id="additional-imports">Additional imports</h3>
<div id="754a6a26-07a1-4f53-b6f7-55fd446427ae" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">### Multiple-metric and dataset imports</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_recall_curve, classification_report</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlflow</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlflow.entities <span class="im">import</span> Metric</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlflow.tracking <span class="im">import</span> MlflowClient</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlflow.models <span class="im">import</span> infer_signature</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="connecting" class="level2">
<h2 class="anchored" data-anchor-id="connecting">Connecting</h2>
<div id="be218f5a-9c61-45c4-a156-c0277de9a20e" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># authenticate</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>credential <span class="op">=</span> DefaultAzureCredential()</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a handle to the workspace</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>ml_client <span class="op">=</span> MLClient.from_config (</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    credential<span class="op">=</span>credential</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Found the config file in: /config.json</code></pre>
</div>
</div>
</section>
<section id="logging-details" class="level2">
<h2 class="anchored" data-anchor-id="logging-details">Logging details</h2>
<p>First experiment: see why the MLflow job created by hello world notebook does not log code, etc. I assume this is because we need to use <code>command</code> and create a job through to the <code>ml_client</code>, as explained in the hello world notebook, under section <code>running script as a job</code>:</p>
<div id="5786b54c-db18-49ef-8211-491fca2933fb" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>writefile hello_world_with_logs.py</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlflow</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> hello_world_core <span class="im">import</span> hello_world, parse_args</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> start_logging (args):</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># set name for logging</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    mlflow.set_experiment(<span class="st">"Hello World with logging"</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    mlflow.start_run()</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    mlflow.log_param (<span class="st">"name to log"</span>, args.name)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> finish_logging ():</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    mlflow.end_run ()</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main():</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Main function of the script."""</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    args <span class="op">=</span> parse_args ()</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    start_logging (args)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    hello_world (args.name)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    finish_logging ()</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    main()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Writing hello_world_with_logs.py</code></pre>
</div>
</div>
<div id="57bf15af-a771-47c5-a3d1-65d07605bd83" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># configure job</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>job <span class="op">=</span> command(</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span><span class="bu">dict</span>(</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        name<span class="op">=</span><span class="st">"Jaume"</span>, <span class="co"># default value of our parameter</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    code<span class="op">=</span><span class="ss">f"./"</span>,  <span class="co"># location of source code: in this case, the root folder</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    command<span class="op">=</span><span class="st">"python hello_world_with_logs.py --name $</span><span class="sc">{{</span><span class="st">inputs.name</span><span class="sc">}}</span><span class="st">"</span>,</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    environment<span class="op">=</span><span class="st">"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest"</span>,</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    display_name<span class="op">=</span><span class="st">"Hello World with logging and job"</span>,</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co"># submit job</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>ml_client.create_or_update(job)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Found the config file in: /config.json
Uploading data_science (12.66 MBs): 100%|██████████| 12658976/12658976 [00:00&lt;00:00, 18557482.88it/s]

</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="6">
<table class="caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">Experiment</td>
<td data-quarto-table-cell-role="th">Name</td>
<td data-quarto-table-cell-role="th">Type</td>
<td data-quarto-table-cell-role="th">Status</td>
<td data-quarto-table-cell-role="th">Details Page</td>
</tr>
<tr class="even">
<td>data_science</td>
<td>jolly_malanga_wgt7b8mb36</td>
<td>command</td>
<td>Starting</td>
<td><a href="https://ml.azure.com/runs/jolly_malanga_wgt7b8mb36?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld&amp;tid=369b25b1-777a-484a-8b5b-52d79bc83015" target="_blank" rel="noopener">Link to Azure Machine Learning studio</a></td>
</tr>
</tbody>
</table>
</div>
</div>
<section id="result" class="level3">
<h3 class="anchored" data-anchor-id="result">Result</h3>
<p>In the previous example there is one error: it seems that we cannot indicate an experiment name unless it is the same as the one indicated in the command function. Since we didn’t indicate any experiment name in that function, we try to do it now:</p>
</section>
<section id="fixing-error" class="level3">
<h3 class="anchored" data-anchor-id="fixing-error">Fixing error</h3>
<div id="ffe2f6d0-c79c-4d7d-8f00-702bc6fb3b79" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>job <span class="op">=</span> command(</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span><span class="bu">dict</span>(</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>        name<span class="op">=</span><span class="st">"Jaume"</span>, <span class="co"># default value of our parameter</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    code<span class="op">=</span><span class="ss">f"./"</span>,  <span class="co"># location of source code: in this case, the root folder</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    command<span class="op">=</span><span class="st">"python hello_world_with_logs.py --name $</span><span class="sc">{{</span><span class="st">inputs.name</span><span class="sc">}}</span><span class="st">"</span>,</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    environment<span class="op">=</span><span class="st">"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest"</span>,</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    display_name<span class="op">=</span><span class="st">"Hello World with logging and job"</span>,</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    experiment_name<span class="op">=</span><span class="st">"Hello World with logging"</span>,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co"># submit job</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>ml_client.create_or_update(job)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Uploading data_science (12.66 MBs): 100%|██████████| 12664739/12664739 [00:00&lt;00:00, 18366160.15it/s]

</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="7">
<table class="caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">Experiment</td>
<td data-quarto-table-cell-role="th">Name</td>
<td data-quarto-table-cell-role="th">Type</td>
<td data-quarto-table-cell-role="th">Status</td>
<td data-quarto-table-cell-role="th">Details Page</td>
</tr>
<tr class="even">
<td>Hello World with logging</td>
<td>joyful_brick_2zb5xmvktl</td>
<td>command</td>
<td>Starting</td>
<td><a href="https://ml.azure.com/runs/joyful_brick_2zb5xmvktl?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld&amp;tid=369b25b1-777a-484a-8b5b-52d79bc83015" target="_blank" rel="noopener">Link to Azure Machine Learning studio</a></td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="result-1" class="level3">
<h3 class="anchored" data-anchor-id="result-1">Result</h3>
<ul>
<li>A job is created with experiment name “Hello world with logging” and latest job name “Hello World with logging and job”, which is the display_name indicated in the command function (see screenshot below)</li>
<li>Both code and logs are stored as part of this job.</li>
</ul>
<p><img src="playing_with_mlflow_files/figure-html/4541e532-9152-45e4-8061-9468ba27687c-1-f51300be-5b90-42c1-9977-7c14fc8bdcf9.png" class="img-fluid"></p>
<div id="1e36f9c6-1e28-4664-9ae4-a21dfc8a326a" class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="logging-experiments" class="level2">
<h2 class="anchored" data-anchor-id="logging-experiments">Logging experiments</h2>
<p>Links:</p>
<p>https://mlflow.org/docs/2.0.0/tracking.html#logging-functions</p>
<p>https://mlflow.org/docs/2.0.0/tracking.html#managing-experiments-and-runs-with-the-tracking-service-api</p>
<div id="f6a44609-7c3b-48ca-a294-d93602c41113" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>writefile hello_world_experiments.py</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlflow</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> hello_world_core <span class="im">import</span> hello_world, parse_args</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main():</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Main function of the script."""</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    names <span class="op">=</span> [<span class="st">"John"</span>, <span class="st">"Mary"</span>, <span class="st">"Ana"</span>]</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx, name <span class="kw">in</span> <span class="bu">enumerate</span>(names):</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>        mlflow.create_experiment (<span class="bu">str</span>(idx))</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>        mlflow.start_run()</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        mlflow.log_param (<span class="st">"name to log"</span>, name)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>        mlflow.log_metric (<span class="st">"length"</span>, <span class="bu">len</span>(name))</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>        mlflow.end_run ()</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>        hello_world (name)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    main()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Writing hello_world_experiments.py</code></pre>
</div>
</div>
<div id="07213771-41b5-4866-8b0f-f50b23951a19" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Standard imports</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Third-party imports</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># AML imports</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> azure.ai.ml <span class="im">import</span> command, MLClient</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> azure.identity <span class="im">import</span> DefaultAzureCredential</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co"># authenticate</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>credential <span class="op">=</span> DefaultAzureCredential()</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a handle to the workspace</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>ml_client <span class="op">=</span> MLClient.from_config (</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    credential<span class="op">=</span>credential</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="co"># configure job</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>job <span class="op">=</span> command(</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    code<span class="op">=</span><span class="ss">f"./"</span>,  <span class="co"># location of source code: in this case, the root folder</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    command<span class="op">=</span><span class="st">"python hello_world_experiments.py"</span>,</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>    environment<span class="op">=</span><span class="st">"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest"</span>,</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    display_name<span class="op">=</span><span class="st">"Hello World with experiments"</span>,</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a><span class="co"># submit job</span></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>ml_client.create_or_update(job)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Found the config file in: /config.json
Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.
Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.
Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.
Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.
Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.
Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.
Uploading data_science (12.75 MBs): 100%|██████████| 12749662/12749662 [00:00&lt;00:00, 15505064.12it/s]

</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="2">
<table class="caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">Experiment</td>
<td data-quarto-table-cell-role="th">Name</td>
<td data-quarto-table-cell-role="th">Type</td>
<td data-quarto-table-cell-role="th">Status</td>
<td data-quarto-table-cell-role="th">Details Page</td>
</tr>
<tr class="even">
<td>data_science</td>
<td>witty_glove_syh5ltdkh6</td>
<td>command</td>
<td>Starting</td>
<td><a href="https://ml.azure.com/runs/witty_glove_syh5ltdkh6?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld&amp;tid=369b25b1-777a-484a-8b5b-52d79bc83015" target="_blank" rel="noopener">Link to Azure Machine Learning studio</a></td>
</tr>
</tbody>
</table>
</div>
</div>
<section id="result-2" class="level3">
<h3 class="anchored" data-anchor-id="result-2">Result</h3>
<ul>
<li>A job “Hello World with experiments” is created under the experiment name “data_science”. It seems this is the default name of the experiment if we don’t indicate in the <code>command</code> function (see screenshot)</li>
</ul>
<p><img src="playing_with_mlflow_files/figure-html/ac7ebcc3-7983-4615-b539-f0d0ad30063c-1-8acb6d58-f2c6-4b09-b41e-24d66b1fe80d.png" class="img-fluid"></p>
<ul>
<li>The job has code and logs registered</li>
</ul>
</section>
</section>
<section id="start_run-receives-experiment" class="level2">
<h2 class="anchored" data-anchor-id="start_run-receives-experiment">start_run receives experiment</h2>
<div id="1d4f191c-e42a-4158-993b-bef6f6495149" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>writefile hello_world_experiments.py</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlflow</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> hello_world_core <span class="im">import</span> hello_world, parse_args</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main():</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Main function of the script."""</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    names <span class="op">=</span> [<span class="st">"John"</span>, <span class="st">"Mary"</span>, <span class="st">"Ana"</span>]</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx, name <span class="kw">in</span> <span class="bu">enumerate</span>(names):</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        experiment <span class="op">=</span> mlflow.create_experiment (<span class="bu">str</span>(idx))</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        mlflow.start_run(experiment)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>        mlflow.log_param (<span class="st">"name to log"</span>, name)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>        mlflow.log_metric (<span class="st">"length"</span>, <span class="bu">len</span>(name))</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        mlflow.end_run ()</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        hello_world (name)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>    main()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Overwriting hello_world_experiments.py</code></pre>
</div>
</div>
<div id="237aeb7e-0876-46e7-8e0f-0c417d742794" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># configure job</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>job <span class="op">=</span> command(</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    code<span class="op">=</span><span class="ss">f"./"</span>,  <span class="co"># location of source code: in this case, the root folder</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    command<span class="op">=</span><span class="st">"python hello_world_experiments.py"</span>,</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    environment<span class="op">=</span><span class="st">"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest"</span>,</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    display_name<span class="op">=</span><span class="st">"Hello World with experiments 2"</span>,</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co"># submit job</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>ml_client.create_or_update(job)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Uploading data_science (12.73 MBs): 100%|██████████| 12725904/12725904 [00:00&lt;00:00, 16250029.70it/s]

</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="4">
<table class="caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">Experiment</td>
<td data-quarto-table-cell-role="th">Name</td>
<td data-quarto-table-cell-role="th">Type</td>
<td data-quarto-table-cell-role="th">Status</td>
<td data-quarto-table-cell-role="th">Details Page</td>
</tr>
<tr class="even">
<td>data_science</td>
<td>good_helmet_wgcgzlvs99</td>
<td>command</td>
<td>Starting</td>
<td><a href="https://ml.azure.com/runs/good_helmet_wgcgzlvs99?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld&amp;tid=369b25b1-777a-484a-8b5b-52d79bc83015" target="_blank" rel="noopener">Link to Azure Machine Learning studio</a></td>
</tr>
</tbody>
</table>
</div>
</div>
<section id="result-3" class="level3">
<h3 class="anchored" data-anchor-id="result-3">Result</h3>
<p>This experiment failed, since the call to <code>mlflow.start_run (experiment)</code> is incorrect. The correct call is <code>mlflow.start_run(experiment_id=experiment_id)</code></p>
</section>
</section>
<section id="start_run-receives-under-experiment_id-name" class="level2">
<h2 class="anchored" data-anchor-id="start_run-receives-under-experiment_id-name">start_run receives under experiment_id name</h2>
<div id="21d823de-71a3-4884-9aad-dcd4bf225fda" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>writefile hello_world_experiments_id.py</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlflow</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> hello_world_core <span class="im">import</span> hello_world, parse_args</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main():</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Main function of the script."""</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    names <span class="op">=</span> [<span class="st">"John"</span>, <span class="st">"Mary"</span>, <span class="st">"Ana"</span>]</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx, name <span class="kw">in</span> <span class="bu">enumerate</span>(names):</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        experiment_id <span class="op">=</span> mlflow.create_experiment (<span class="bu">str</span>(idx))</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>        mlflow.start_run(experiment_id<span class="op">=</span>experiment_id)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>        mlflow.log_param (<span class="st">"name to log"</span>, name)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>        mlflow.log_metric (<span class="st">"length"</span>, <span class="bu">len</span>(name))</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>        mlflow.end_run ()</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>        hello_world (name)</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    main()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Writing hello_world_experiments_id.py</code></pre>
</div>
</div>
<div id="54eba58e-7e09-437a-8f7e-279bc1c48366" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># configure job</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>job <span class="op">=</span> command(</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    code<span class="op">=</span><span class="ss">f"./"</span>,  <span class="co"># location of source code: in this case, the root folder</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    command<span class="op">=</span><span class="st">"python hello_world_experiments_id.py"</span>,</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    environment<span class="op">=</span><span class="st">"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest"</span>,</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    display_name<span class="op">=</span><span class="st">"Hello World with experiment_id"</span>,</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="co"># submit job</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>ml_client.create_or_update(job)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Uploading data_science (12.74 MBs): 100%|██████████| 12741018/12741018 [00:02&lt;00:00, 5305432.15it/s]

</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="6">
<table class="caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">Experiment</td>
<td data-quarto-table-cell-role="th">Name</td>
<td data-quarto-table-cell-role="th">Type</td>
<td data-quarto-table-cell-role="th">Status</td>
<td data-quarto-table-cell-role="th">Details Page</td>
</tr>
<tr class="even">
<td>data_science</td>
<td>lime_snail_kdddyl016h</td>
<td>command</td>
<td>Starting</td>
<td><a href="https://ml.azure.com/runs/lime_snail_kdddyl016h?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld&amp;tid=369b25b1-777a-484a-8b5b-52d79bc83015" target="_blank" rel="noopener">Link to Azure Machine Learning studio</a></td>
</tr>
</tbody>
</table>
</div>
</div>
<section id="result-4" class="level3">
<h3 class="anchored" data-anchor-id="result-4">Result</h3>
<p>The code is uploaded to an experiment called “data_science”. This experiment makes use of the first parameter value: <code>name="John"</code>. The remaining experiments (<code>1</code>, and <code>2</code>), are created separately for parameter values <code>"Mary"</code> and <code>"Ana"</code>, without code being uploaded to them.</p>
</section>
</section>
<section id="using-separate-runs-instead" class="level2">
<h2 class="anchored" data-anchor-id="using-separate-runs-instead">using separate runs instead</h2>
<div id="515af2e2-bfff-4c26-bcce-fd054e56a3a8" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>writefile hello_world_runs.py</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlflow</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> hello_world_core <span class="im">import</span> hello_world, parse_args</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main():</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Main function of the script."""</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>    names <span class="op">=</span> [<span class="st">"John"</span>, <span class="st">"Mary"</span>, <span class="st">"Ana"</span>]</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    experiment_id <span class="op">=</span> mlflow.create_experiment(<span class="st">"experiment1"</span>)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx, name <span class="kw">in</span> <span class="bu">enumerate</span>(names):</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>        mlflow.start_run(run_name<span class="op">=</span><span class="bu">str</span>(idx), experiment_id<span class="op">=</span>experiment_id)</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>        mlflow.log_param (<span class="st">"name to log"</span>, name)</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>        mlflow.log_metric (<span class="st">"length"</span>, <span class="bu">len</span>(name))</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>        mlflow.end_run ()</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>        hello_world (name)</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>    main()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Writing hello_world_runs.py</code></pre>
</div>
</div>
<div id="4e67f151-5f32-47d4-bc34-a83d1a02e10e" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># configure job</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>job <span class="op">=</span> command(</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    code<span class="op">=</span><span class="ss">f"./"</span>,  <span class="co"># location of source code: in this case, the root folder</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    command<span class="op">=</span><span class="st">"python hello_world_runs.py"</span>,</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    environment<span class="op">=</span><span class="st">"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest"</span>,</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    display_name<span class="op">=</span><span class="st">"Hello World with runs"</span>,</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="co"># submit job</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>ml_client.create_or_update(job)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Uploading data_science (12.75 MBs): 100%|██████████| 12749252/12749252 [00:00&lt;00:00, 18741643.65it/s]

</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="9">
<table class="caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">Experiment</td>
<td data-quarto-table-cell-role="th">Name</td>
<td data-quarto-table-cell-role="th">Type</td>
<td data-quarto-table-cell-role="th">Status</td>
<td data-quarto-table-cell-role="th">Details Page</td>
</tr>
<tr class="even">
<td>data_science</td>
<td>olive_shelf_r4fzsl1f0d</td>
<td>command</td>
<td>Starting</td>
<td><a href="https://ml.azure.com/runs/olive_shelf_r4fzsl1f0d?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld&amp;tid=369b25b1-777a-484a-8b5b-52d79bc83015" target="_blank" rel="noopener">Link to Azure Machine Learning studio</a></td>
</tr>
</tbody>
</table>
</div>
</div>
<section id="result-5" class="level3">
<h3 class="anchored" data-anchor-id="result-5">Result</h3>
<ul>
<li><p>Two runs are created under a job with experiment name “experiment1” and latest_job name <code>1</code> and <code>2</code>: <img src="playing_with_mlflow_files/figure-html/3bf2b9e5-03ab-4b37-8aef-769c543d4a64-2-e920a75d-c54e-4b89-bc5c-9e76994bf47d.png" class="img-fluid"></p></li>
<li><p>The metric values for these can be compared using graphics:</p></li>
</ul>
<p><img src="playing_with_mlflow_files/figure-html/3bf2b9e5-03ab-4b37-8aef-769c543d4a64-1-66642686-e0ec-46e2-a610-c2c548f1b410.png" class="img-fluid"></p>
<ul>
<li>There is still a job created under experiment name <code>data_science</code>. This job contains result of using first parameter value, code and logs. But it cannot be compared.</li>
</ul>
</section>
</section>
<section id="separate-runs-experiment-in-command" class="level2">
<h2 class="anchored" data-anchor-id="separate-runs-experiment-in-command">Separate runs + experiment in <code>command</code></h2>
<div id="75fea28c-b84f-4b6b-8a87-02cc2b1333dd" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>writefile hello_world_runs_command.py</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlflow</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> hello_world_core <span class="im">import</span> hello_world, parse_args</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main():</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Main function of the script."""</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    names <span class="op">=</span> [<span class="st">"John"</span>, <span class="st">"Mary"</span>, <span class="st">"Ana"</span>]</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx, name <span class="kw">in</span> <span class="bu">enumerate</span>(names):</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>        mlflow.start_run(run_name<span class="op">=</span><span class="bu">str</span>(idx))</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>        mlflow.log_param (<span class="st">"name to log"</span>, name)</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>        mlflow.log_metric (<span class="st">"length"</span>, <span class="bu">len</span>(name))</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>        mlflow.end_run ()</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>        hello_world (name)</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>    main()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Writing hello_world_runs_command.py</code></pre>
</div>
</div>
<div id="8a056cd6-2033-4e61-a726-16e6238a6b9b" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># configure job</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>job <span class="op">=</span> command(</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    code<span class="op">=</span><span class="ss">f"./"</span>,  <span class="co"># location of source code: in this case, the root folder</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    command<span class="op">=</span><span class="st">"python hello_world_runs_command.py"</span>,</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    environment<span class="op">=</span><span class="st">"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest"</span>,</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>    experiment_name<span class="op">=</span><span class="st">"hw_runs_command"</span>,</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    display_name<span class="op">=</span><span class="st">"Hello World with runs + command"</span>,</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="co"># submit job</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>ml_client.create_or_update(job)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.
Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.
Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.
Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.
Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.
Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.
Uploading data_science (17.09 MBs): 100%|██████████| 17092597/17092597 [00:00&lt;00:00, 23325862.31it/s]

</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="5">
<table class="caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">Experiment</td>
<td data-quarto-table-cell-role="th">Name</td>
<td data-quarto-table-cell-role="th">Type</td>
<td data-quarto-table-cell-role="th">Status</td>
<td data-quarto-table-cell-role="th">Details Page</td>
</tr>
<tr class="even">
<td>hw_runs_command</td>
<td>jolly_bone_c07ncly80l</td>
<td>command</td>
<td>Starting</td>
<td><a href="https://ml.azure.com/runs/jolly_bone_c07ncly80l?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld&amp;tid=369b25b1-777a-484a-8b5b-52d79bc83015" target="_blank" rel="noopener">Link to Azure Machine Learning studio</a></td>
</tr>
</tbody>
</table>
</div>
</div>
<section id="result-6" class="level3">
<h3 class="anchored" data-anchor-id="result-6">Result</h3>
<ul>
<li>We have the three runs under the same experiment, and we can compare their metrics:</li>
</ul>
<p><img src="playing_with_mlflow_files/figure-html/4e31316d-030a-4c3a-86db-d499d7a00d53-1-d17a7f27-1139-4103-8e9c-4ed4c7b4a8ce.png" class="img-fluid"></p>
</section>
</section>
<section id="logging-images" class="level2">
<h2 class="anchored" data-anchor-id="logging-images">Logging images</h2>
<div id="93b88471-0d5d-4f8b-b589-498984e635a5" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>writefile hello_world_images.py</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlflow</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> hello_world_core <span class="im">import</span> hello_world, parse_args</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main():</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Main function of the script."""</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>    names <span class="op">=</span> [<span class="st">"John"</span>, <span class="st">"Mary"</span>, <span class="st">"Ana"</span>]</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx, name <span class="kw">in</span> <span class="bu">enumerate</span>(names):</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>        mlflow.start_run(run_name<span class="op">=</span><span class="bu">str</span>(idx))</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>        mlflow.log_param (<span class="st">"name to log"</span>, name)</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>        mlflow.log_metric (<span class="st">"length"</span>, <span class="bu">len</span>(name))</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>        fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>        ax.plot([<span class="dv">0</span><span class="op">+</span><span class="dv">10</span><span class="op">*</span><span class="bu">len</span>(name), <span class="dv">1</span><span class="op">+</span><span class="dv">10</span><span class="op">*</span><span class="bu">len</span>(name)], [<span class="dv">2</span><span class="op">+</span><span class="dv">10</span><span class="op">*</span><span class="bu">len</span>(name), <span class="dv">3</span><span class="op">+</span><span class="dv">10</span><span class="op">*</span><span class="bu">len</span>(name)])</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>        mlflow.log_figure(fig, <span class="ss">f"figure_</span><span class="sc">{</span>idx<span class="sc">}</span><span class="ss">.png"</span>)</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>        mlflow.end_run ()</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>        hello_world (name)</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>    main()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Overwriting hello_world_images.py</code></pre>
</div>
</div>
<div id="554d32ae-18bc-4b6c-9beb-ec6abd4a5340" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># configure job</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>job <span class="op">=</span> command(</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    code<span class="op">=</span><span class="ss">f"./"</span>,  <span class="co"># location of source code: in this case, the root folder</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    command<span class="op">=</span><span class="st">"python hello_world_images.py"</span>,</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    environment<span class="op">=</span><span class="st">"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest"</span>,</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    experiment_name<span class="op">=</span><span class="st">"hw_images"</span>,</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>    display_name<span class="op">=</span><span class="st">"Hello World with images"</span>,</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>job.settings.default_compute <span class="op">=</span> <span class="st">"jaumecpu"</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="co"># submit job</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>ml_client.create_or_update(job)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warnings: [settings: Unknown field.]
Warnings: [settings: Unknown field.]
Uploading data_science (16.39 MBs): 100%|██████████| 16392540/16392540 [00:01&lt;00:00, 14278614.03it/s]

</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="11">
<table class="caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">Experiment</td>
<td data-quarto-table-cell-role="th">Name</td>
<td data-quarto-table-cell-role="th">Type</td>
<td data-quarto-table-cell-role="th">Status</td>
<td data-quarto-table-cell-role="th">Details Page</td>
</tr>
<tr class="even">
<td>hw_images</td>
<td>shy_island_fh9jg0r6nf</td>
<td>command</td>
<td>Starting</td>
<td><a href="https://ml.azure.com/runs/shy_island_fh9jg0r6nf?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld&amp;tid=369b25b1-777a-484a-8b5b-52d79bc83015" target="_blank" rel="noopener">Link to Azure Machine Learning studio</a></td>
</tr>
</tbody>
</table>
</div>
</div>
<section id="result-7" class="level3">
<h3 class="anchored" data-anchor-id="result-7">Result</h3>
</section>
</section>
<section id="using-interactive" class="level2">
<h2 class="anchored" data-anchor-id="using-interactive">Using interactive</h2>
<div id="a80b3ca9-c797-4f11-9af5-cdc38d9216be" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the experiment</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>mlflow.set_experiment(<span class="st">"mlflow-interactive-4"</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>names <span class="op">=</span> [<span class="st">"John"</span>, <span class="st">"Mary"</span>, <span class="st">"Ana"</span>]</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, name <span class="kw">in</span> <span class="bu">enumerate</span>(names):</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>    mlflow.start_run(run_name<span class="op">=</span><span class="bu">str</span>(idx)<span class="op">+</span><span class="st">"-bis-2"</span>)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>    mlflow.log_param (<span class="st">"name to log"</span>, name)</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    mlflow.log_metric (<span class="st">"length"</span>, <span class="bu">len</span>(name))</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>    ax.plot([<span class="dv">0</span><span class="op">+</span><span class="dv">10</span><span class="op">*</span><span class="bu">len</span>(name), <span class="dv">1</span><span class="op">+</span><span class="dv">10</span><span class="op">*</span><span class="bu">len</span>(name)], [<span class="dv">2</span><span class="op">+</span><span class="dv">10</span><span class="op">*</span><span class="bu">len</span>(name), <span class="dv">3</span><span class="op">+</span><span class="dv">10</span><span class="op">*</span><span class="bu">len</span>(name)])</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>    mlflow.log_figure(fig, <span class="ss">f"figure_</span><span class="sc">{</span>idx<span class="sc">}</span><span class="ss">.png"</span>)</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>    mlflow.end_run ()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2024/06/04 09:05:04 INFO mlflow.tracking.fluent: Experiment with name 'mlflow-interactive-4' does not exist. Creating a new experiment.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="playing_with_mlflow_files/figure-html/cell-21-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="playing_with_mlflow_files/figure-html/cell-21-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="playing_with_mlflow_files/figure-html/cell-21-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="multiple-metrics" class="level2">
<h2 class="anchored" data-anchor-id="multiple-metrics">Multiple metrics</h2>
<div id="3929638f-b309-4739-9d26-8845af10837b" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>mlflow.set_experiment(<span class="st">"multiple-metrics"</span>)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>client <span class="op">=</span> MlflowClient()</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>names <span class="op">=</span> [<span class="st">"John"</span>, <span class="st">"Mary"</span>, <span class="st">"Ana"</span>]</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, name <span class="kw">in</span> <span class="bu">enumerate</span>(names):</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>    current_run <span class="op">=</span> mlflow.start_run(run_name<span class="op">=</span><span class="bu">str</span>(idx)<span class="op">+</span><span class="st">"-bis-2"</span>)</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>    mlflow.log_param (<span class="st">"name to log"</span>, name)</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>    mlflow.log_metric (<span class="st">"length"</span>, <span class="bu">len</span>(name))</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>    list_to_log <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>])<span class="op">+</span><span class="bu">len</span>(name)<span class="op">*</span><span class="dv">1000</span></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>    client.log_batch(</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>        current_run.info.run_id, </span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>        metrics<span class="op">=</span>[</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>            Metric(key<span class="op">=</span><span class="st">"sample_list"</span>, value<span class="op">=</span>val, timestamp<span class="op">=</span><span class="bu">int</span>(time.time() <span class="op">*</span> <span class="dv">1000</span>), step<span class="op">=</span><span class="dv">0</span>) </span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> val <span class="kw">in</span> list_to_log</span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>    ax.plot(np.array([<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">16</span>,<span class="dv">32</span>,<span class="dv">64</span>,<span class="dv">128</span>]) <span class="op">+</span> <span class="bu">len</span>(name)<span class="op">*</span><span class="dv">1000</span>)</span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>    mlflow.log_figure(fig, <span class="ss">f"figure_</span><span class="sc">{</span>idx<span class="sc">}</span><span class="ss">.png"</span>)</span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a>    mlflow.end_run ()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="playing_with_mlflow_files/figure-html/cell-22-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="playing_with_mlflow_files/figure-html/cell-22-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="playing_with_mlflow_files/figure-html/cell-22-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="using-dataset" class="level2">
<h2 class="anchored" data-anchor-id="using-dataset">Using dataset</h2>
<section id="auto-log-not-using-evaluate" class="level3">
<h3 class="anchored" data-anchor-id="auto-log-not-using-evaluate">auto-log, not using evaluate</h3>
</section>
<section id="without-set_experiment-or-run" class="level3">
<h3 class="anchored" data-anchor-id="without-set_experiment-or-run">without set_experiment or run</h3>
<div id="609194f3-c5d9-492f-b9e3-0c20481dc795" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># enable autologging</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>mlflow.autolog()</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="co"># read data</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>dataset_source_url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv"</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(dataset_source_url, delimiter<span class="op">=</span><span class="st">";"</span>)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a><span class="co"># split data</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">"quality"</span>]</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop(<span class="st">"quality"</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>    X, y, test_size<span class="op">=</span><span class="fl">0.33</span>, random_state<span class="op">=</span><span class="dv">17</span></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a><span class="co"># train model</span></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> RandomForestClassifier ()</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>model.fit (X_train, y_train)</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate</span></span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>y_hat <span class="op">=</span> model.predict(X_test)</span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>acc <span class="op">=</span> np.average(y_hat <span class="op">==</span> y_test)</span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Accuracy:'</span>, acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2024/06/04 13:32:06 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.
2024/06/04 13:32:06 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '79f7182e-22da-49bc-9ffb-5d4d3ab19172', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow
2024/06/04 13:32:14 WARNING mlflow.sklearn: Failed to log evaluation dataset information to MLflow Tracking. Reason: BAD_REQUEST: Response: {'Error': {'Code': 'UserError', 'Severity': None, 'Message': 'Cannot log the same dataset with different context', 'MessageFormat': None, 'MessageParameters': None, 'ReferenceCode': None, 'DetailsUri': None, 'Target': None, 'Details': [], 'InnerError': None, 'DebugInfo': None, 'AdditionalInfo': None}, 'Correlation': {'operation': '8bf21ee6ca6b0a22621d463962cdb6c7', 'request': 'bb3b9eee5cc179d2'}, 'Environment': 'eastus2', 'Location': 'eastus2', 'Time': '2024-06-04T13:32:14.3398324+00:00', 'ComponentName': 'mlflow', 'statusCode': 400, 'error_code': 'BAD_REQUEST'}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.6611008039579468</code></pre>
</div>
</div>
</section>
<section id="with-set_experiment-or-run" class="level3">
<h3 class="anchored" data-anchor-id="with-set_experiment-or-run">with set_experiment or run</h3>
<div id="1b0b1384-09be-4f96-a843-274727ad65f0" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>mlflow.set_experiment(<span class="st">"auto-log-diabetes"</span>)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>current_run <span class="op">=</span> mlflow.start_run(run_name<span class="op">=</span><span class="st">"single-run"</span>)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="co"># enable autologging</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>mlflow.autolog()</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="co"># read data</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>dataset_source_url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv"</span></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(dataset_source_url, delimiter<span class="op">=</span><span class="st">";"</span>)</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a><span class="co"># split data</span></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">"quality"</span>]</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop(<span class="st">"quality"</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>    X, y, test_size<span class="op">=</span><span class="fl">0.33</span>, random_state<span class="op">=</span><span class="dv">17</span></span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a><span class="co"># train model</span></span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> RandomForestClassifier ()</span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a>model.fit (X_train, y_train)</span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate</span></span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a>y_hat <span class="op">=</span> model.predict(X_test)</span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a>acc <span class="op">=</span> np.average(y_hat <span class="op">==</span> y_test)</span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Accuracy:'</span>, acc)</span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a>mlflow.end_run()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2024/06/04 13:42:27 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.
2024/06/04 13:42:34 WARNING mlflow.sklearn: Failed to log evaluation dataset information to MLflow Tracking. Reason: BAD_REQUEST: Response: {'Error': {'Code': 'UserError', 'Severity': None, 'Message': 'Cannot log the same dataset with different context', 'MessageFormat': None, 'MessageParameters': None, 'ReferenceCode': None, 'DetailsUri': None, 'Target': None, 'Details': [], 'InnerError': None, 'DebugInfo': None, 'AdditionalInfo': None}, 'Correlation': {'operation': '2ca9d0147eefa2c27982cd511b017688', 'request': '0c999aaf8b0221f2'}, 'Environment': 'eastus2', 'Location': 'eastus2', 'Time': '2024-06-04T13:42:34.8823005+00:00', 'ComponentName': 'mlflow', 'statusCode': 400, 'error_code': 'BAD_REQUEST'}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.6635745207173779</code></pre>
</div>
</div>
</section>
<section id="with-evaluate" class="level3">
<h3 class="anchored" data-anchor-id="with-evaluate">with evaluate</h3>
<div id="5c13c550-ac4e-425b-a5b8-aa0b9294e72e" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>mlflow.set_experiment(<span class="st">"auto-log-evaluate"</span>)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>current_run <span class="op">=</span> mlflow.start_run(run_name<span class="op">=</span><span class="st">"single-run"</span>)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="co"># enable autologging</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>mlflow.autolog()</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a><span class="co"># read data</span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>dataset_source_url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv"</span></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(dataset_source_url, delimiter<span class="op">=</span><span class="st">";"</span>)</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a><span class="co"># split data</span></span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">"quality"</span>]</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop(<span class="st">"quality"</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>    X, y, test_size<span class="op">=</span><span class="fl">0.33</span>, random_state<span class="op">=</span><span class="dv">17</span></span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a><span class="co"># train model</span></span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> RandomForestClassifier ()</span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a>model.fit (X_train, y_train)</span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate</span></span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a>y_hat <span class="op">=</span> model.predict(X_test)</span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-26"><a href="#cb45-26" aria-hidden="true" tabindex="-1"></a><span class="co"># --------------------------------------</span></span>
<span id="cb45-27"><a href="#cb45-27" aria-hidden="true" tabindex="-1"></a>eval_data <span class="op">=</span> X_test</span>
<span id="cb45-28"><a href="#cb45-28" aria-hidden="true" tabindex="-1"></a>eval_data[<span class="st">"label"</span>] <span class="op">=</span> y_test</span>
<span id="cb45-29"><a href="#cb45-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-30"><a href="#cb45-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Assign the decoded predictions to the Evaluation Dataset</span></span>
<span id="cb45-31"><a href="#cb45-31" aria-hidden="true" tabindex="-1"></a>eval_data[<span class="st">"predictions"</span>] <span class="op">=</span> y_hat</span>
<span id="cb45-32"><a href="#cb45-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-33"><a href="#cb45-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the PandasDataset for use in mlflow evaluate</span></span>
<span id="cb45-34"><a href="#cb45-34" aria-hidden="true" tabindex="-1"></a>pd_dataset <span class="op">=</span> mlflow.data.from_pandas(</span>
<span id="cb45-35"><a href="#cb45-35" aria-hidden="true" tabindex="-1"></a>    eval_data, predictions<span class="op">=</span><span class="st">"predictions"</span>, targets<span class="op">=</span><span class="st">"label"</span></span>
<span id="cb45-36"><a href="#cb45-36" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb45-37"><a href="#cb45-37" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> mlflow.evaluate(data<span class="op">=</span>pd_dataset, predictions<span class="op">=</span><span class="va">None</span>, model_type<span class="op">=</span><span class="st">"classifier"</span>)</span>
<span id="cb45-38"><a href="#cb45-38" aria-hidden="true" tabindex="-1"></a><span class="co">#result = mlflow.evaluate(model=model, predictions=None, model_type="classifier")</span></span>
<span id="cb45-39"><a href="#cb45-39" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="with-shap-extra-metrics-and-multiple-runs" class="level3">
<h3 class="anchored" data-anchor-id="with-shap-extra-metrics-and-multiple-runs">with shap, extra metrics, and multiple runs</h3>
<div id="fea549ea-3c88-496c-955b-87c551997fdd" class="cell" data-tags="[]" data-execution_count="8">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>mlflow.end_run()</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f1_at_70 (eval_df, _builtin_metrics):</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>    precision, recall, thresholds <span class="op">=</span> precision_recall_curve (eval_df[<span class="st">"target"</span>], eval_df[<span class="st">"prediction"</span>])</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>    thresholds <span class="op">=</span> np.append (thresholds, values<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>    threshold<span class="op">=</span>thresholds[precision<span class="op">&gt;</span><span class="fl">0.7</span>]</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>    metrics <span class="op">=</span> classification_report (y_test, y_hat<span class="op">&gt;</span>threshold, output_dict<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> metrics[<span class="st">'1'</span>][<span class="st">'f1-score'</span>]</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>f1_at_70_metric <span class="op">=</span> mlflow.models.make_metric(</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>    eval_fn<span class="op">=</span>f1_at_70,</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>    greater_is_better<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>mlflow.set_experiment(<span class="st">"shap-extra-many-runs-17"</span>)</span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a><span class="co"># enable autologging</span></span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>mlflow.autolog()</span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a><span class="co"># read data</span></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a>dataset_source_url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv"</span></span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(dataset_source_url, delimiter<span class="op">=</span><span class="st">";"</span>)</span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a><span class="co"># split data</span></span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">"quality"</span>]</span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop(<span class="st">"quality"</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X[(y<span class="op">==</span><span class="dv">6</span>) <span class="op">|</span> (y<span class="op">==</span><span class="dv">5</span>)]</span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y[(y<span class="op">==</span><span class="dv">6</span>) <span class="op">|</span> (y<span class="op">==</span><span class="dv">5</span>)]</span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a>y[y<span class="op">==</span><span class="dv">6</span>]<span class="op">=</span><span class="dv">1</span></span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a>y[y<span class="op">==</span><span class="dv">5</span>]<span class="op">=</span><span class="dv">0</span></span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-31"><a href="#cb46-31" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb46-32"><a href="#cb46-32" aria-hidden="true" tabindex="-1"></a>    X, y, test_size<span class="op">=</span><span class="fl">0.33</span>, random_state<span class="op">=</span><span class="dv">17</span></span>
<span id="cb46-33"><a href="#cb46-33" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb46-34"><a href="#cb46-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-35"><a href="#cb46-35" aria-hidden="true" tabindex="-1"></a><span class="co"># hp</span></span>
<span id="cb46-36"><a href="#cb46-36" aria-hidden="true" tabindex="-1"></a>n_estimators_values <span class="op">=</span> [<span class="dv">25</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">200</span>]</span>
<span id="cb46-37"><a href="#cb46-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-38"><a href="#cb46-38" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, n_estimators <span class="kw">in</span> <span class="bu">enumerate</span> (n_estimators_values):</span>
<span id="cb46-39"><a href="#cb46-39" aria-hidden="true" tabindex="-1"></a>    current_run <span class="op">=</span> mlflow.start_run(run_name<span class="op">=</span><span class="ss">f"run_</span><span class="sc">{</span>idx<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb46-40"><a href="#cb46-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-41"><a href="#cb46-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># train model</span></span>
<span id="cb46-42"><a href="#cb46-42" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> RandomForestClassifier (n_estimators<span class="op">=</span>n_estimators)</span>
<span id="cb46-43"><a href="#cb46-43" aria-hidden="true" tabindex="-1"></a>    model.fit (X_train, y_train)</span>
<span id="cb46-44"><a href="#cb46-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-45"><a href="#cb46-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># evaluate</span></span>
<span id="cb46-46"><a href="#cb46-46" aria-hidden="true" tabindex="-1"></a>    <span class="co">#y_hat = model.predict_proba(X_test)[:, 1]</span></span>
<span id="cb46-47"><a href="#cb46-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-48"><a href="#cb46-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># --------------------------------------</span></span>
<span id="cb46-49"><a href="#cb46-49" aria-hidden="true" tabindex="-1"></a>    eval_data <span class="op">=</span> X_test.copy()</span>
<span id="cb46-50"><a href="#cb46-50" aria-hidden="true" tabindex="-1"></a>    eval_data[<span class="st">"label"</span>] <span class="op">=</span> y_test</span>
<span id="cb46-51"><a href="#cb46-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-52"><a href="#cb46-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Assign the decoded predictions to the Evaluation Dataset</span></span>
<span id="cb46-53"><a href="#cb46-53" aria-hidden="true" tabindex="-1"></a>    <span class="co">#eval_data["predictions"] = y_hat</span></span>
<span id="cb46-54"><a href="#cb46-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-55"><a href="#cb46-55" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create the PandasDataset for use in mlflow evaluate</span></span>
<span id="cb46-56"><a href="#cb46-56" aria-hidden="true" tabindex="-1"></a>    pd_dataset <span class="op">=</span> mlflow.data.from_pandas(</span>
<span id="cb46-57"><a href="#cb46-57" aria-hidden="true" tabindex="-1"></a>        eval_data, </span>
<span id="cb46-58"><a href="#cb46-58" aria-hidden="true" tabindex="-1"></a>        <span class="co">#predictions="predictions", </span></span>
<span id="cb46-59"><a href="#cb46-59" aria-hidden="true" tabindex="-1"></a>        targets<span class="op">=</span><span class="st">"label"</span>, </span>
<span id="cb46-60"><a href="#cb46-60" aria-hidden="true" tabindex="-1"></a>        source<span class="op">=</span>dataset_source_url, </span>
<span id="cb46-61"><a href="#cb46-61" aria-hidden="true" tabindex="-1"></a>        name<span class="op">=</span><span class="st">"wine-quality-white-bis"</span>, </span>
<span id="cb46-62"><a href="#cb46-62" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb46-63"><a href="#cb46-63" aria-hidden="true" tabindex="-1"></a>    <span class="co">#result = mlflow.evaluate(data=pd_dataset, predictions=None, model_type="classifier", extra_metrics=[f1_at_70_metric])</span></span>
<span id="cb46-64"><a href="#cb46-64" aria-hidden="true" tabindex="-1"></a>    signature <span class="op">=</span> infer_signature(X_test, model.predict(X_test))</span>
<span id="cb46-65"><a href="#cb46-65" aria-hidden="true" tabindex="-1"></a>    model_info <span class="op">=</span> mlflow.sklearn.log_model(sk_model<span class="op">=</span>model, artifact_path<span class="op">=</span><span class="st">"model"</span>, signature<span class="op">=</span>signature)</span>
<span id="cb46-66"><a href="#cb46-66" aria-hidden="true" tabindex="-1"></a>    sklearn_pyfunc <span class="op">=</span> mlflow.pyfunc.load_model(model_uri<span class="op">=</span>model_info.model_uri)</span>
<span id="cb46-67"><a href="#cb46-67" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> mlflow.evaluate(model<span class="op">=</span>sklearn_pyfunc, data<span class="op">=</span>pd_dataset, predictions<span class="op">=</span><span class="va">None</span>, model_type<span class="op">=</span><span class="st">"classifier"</span>, extra_metrics<span class="op">=</span>[f1_at_70_metric])</span>
<span id="cb46-68"><a href="#cb46-68" aria-hidden="true" tabindex="-1"></a>    <span class="co"># -------------------------------------</span></span>
<span id="cb46-69"><a href="#cb46-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-70"><a href="#cb46-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-71"><a href="#cb46-71" aria-hidden="true" tabindex="-1"></a>    mlflow.end_run()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2024/06/04 17:22:15 INFO mlflow.tracking.fluent: Experiment with name 'shap-extra-many-runs-17' does not exist. Creating a new experiment.
2024/06/04 17:22:15 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.
/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/mlflow/data/dataset_source_registry.py:150: UserWarning: Failed to determine whether UCVolumeDatasetSource can resolve source information for 'https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv'. Exception: 
  return _dataset_source_registry.resolve(
2024/06/04 17:22:21 WARNING mlflow.sklearn: Failed to log evaluation dataset information to MLflow Tracking. Reason: BAD_REQUEST: Response: {'Error': {'Code': 'UserError', 'Severity': None, 'Message': 'Cannot log the same dataset with different context', 'MessageFormat': None, 'MessageParameters': None, 'ReferenceCode': None, 'DetailsUri': None, 'Target': None, 'Details': [], 'InnerError': None, 'DebugInfo': None, 'AdditionalInfo': None}, 'Correlation': {'operation': '0ed9f77ef26119702d55b364f4936aba', 'request': '21f226dd00f8e20d'}, 'Environment': 'eastus2', 'Location': 'eastus2', 'Time': '2024-06-04T17:22:21.5642691+00:00', 'ComponentName': 'mlflow', 'statusCode': 400, 'error_code': 'BAD_REQUEST'}</code></pre>
</div>
<div class="cell-output cell-output-error">
<pre><code>Exception: UserError: Resource Conflict: ArtifactId ExperimentRun/dcid.4631baa7-0c38-4f37-ae4c-c02d11d7a8db/model/requirements.txt already exists.
UserError: Resource Conflict: ArtifactId ExperimentRun/dcid.4631baa7-0c38-4f37-ae4c-c02d11d7a8db/model/python_env.yaml already exists.
UserError: Resource Conflict: ArtifactId ExperimentRun/dcid.4631baa7-0c38-4f37-ae4c-c02d11d7a8db/model/model.pkl already exists.
UserError: Resource Conflict: ArtifactId ExperimentRun/dcid.4631baa7-0c38-4f37-ae4c-c02d11d7a8db/model/conda.yaml already exists.
UserError: Resource Conflict: ArtifactId ExperimentRun/dcid.4631baa7-0c38-4f37-ae4c-c02d11d7a8db/model/MLmodel already exists.
UserError: Resource Conflict: ArtifactId ExperimentRun/dcid.4631baa7-0c38-4f37-ae4c-c02d11d7a8db/model/metadata/requirements.txt already exists.
UserError: Resource Conflict: ArtifactId ExperimentRun/dcid.4631baa7-0c38-4f37-ae4c-c02d11d7a8db/model/metadata/python_env.yaml already exists.
UserError: Resource Conflict: ArtifactId ExperimentRun/dcid.4631baa7-0c38-4f37-ae4c-c02d11d7a8db/model/metadata/conda.yaml already exists.
UserError: Resource Conflict: ArtifactId ExperimentRun/dcid.4631baa7-0c38-4f37-ae4c-c02d11d7a8db/model/metadata/MLmodel already exists.</code></pre>
</div>
</div>
<div id="666429b4-f138-459a-b4dd-7706c3921077" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a> mlflow.evaluate?</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">Signature:</span>
mlflow<span class="ansi-blue-fg">.</span>evaluate<span class="ansi-blue-fg">(</span>
    model<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    data<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    <span class="ansi-blue-fg">*</span><span class="ansi-blue-fg">,</span>
    model_type<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    targets<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    predictions<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    dataset_path<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    feature_names<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    evaluators<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    evaluator_config<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    custom_metrics<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    extra_metrics<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    custom_artifacts<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    validation_thresholds<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    baseline_model<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    env_manager<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">'local'</span><span class="ansi-blue-fg">,</span>
    model_config<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    baseline_config<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    inference_params<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
<span class="ansi-blue-fg">)</span>
<span class="ansi-red-fg">Docstring:</span>
Evaluate the model performance on given data and selected metrics.
This function evaluates a PyFunc model or custom callable on the specified dataset using
specified ``evaluators``, and logs resulting metrics &amp; artifacts to MLflow tracking server.
Users can also skip setting ``model`` and put the model outputs in ``data`` directly for
evaluation. For detailed information, please read
:ref:`the Model Evaluation documentation &lt;model-evaluation&gt;`.
Default Evaluator behavior:
 - The default evaluator, which can be invoked with ``evaluators="default"`` or
   ``evaluators=None``, supports model types listed below. For each pre-defined model type, the
   default evaluator evaluates your model on a selected set of metrics and generate artifacts
   like plots. Please find more details below.
 - For both the ``"regressor"`` and ``"classifier"`` model types, the default evaluator
   generates model summary plots and feature importance plots using
   `SHAP &lt;https://shap.readthedocs.io/en/latest/index.html&gt;`_.
 - For regressor models, the default evaluator additionally logs:
    - **metrics**: example_count, mean_absolute_error, mean_squared_error,
      root_mean_squared_error, sum_on_target, mean_on_target, r2_score, max_error,
      mean_absolute_percentage_error.
 - For binary classifiers, the default evaluator additionally logs:
    - **metrics**: true_negatives, false_positives, false_negatives, true_positives, recall,
      precision, f1_score, accuracy_score, example_count, log_loss, roc_auc,
      precision_recall_auc.
    - **artifacts**: lift curve plot, precision-recall plot, ROC plot.
 - For multiclass classifiers, the default evaluator additionally logs:
    - **metrics**: accuracy_score, example_count, f1_score_micro, f1_score_macro, log_loss
    - **artifacts**: A CSV file for "per_class_metrics" (per-class metrics includes
      true_negatives/false_positives/false_negatives/true_positives/recall/precision/roc_auc,
      precision_recall_auc), precision-recall merged curves plot, ROC merged curves plot.
 - For question-answering models, the default evaluator logs:
    - **metrics**: ``exact_match``, ``token_count``, `toxicity`_ (requires `evaluate`_,
      `torch`_, `flesch_kincaid_grade_level`_ (requires `textstat`_) and `ari_grade_level`_.
    - **artifacts**: A JSON file containing the inputs, outputs, targets (if the ``targets``
      argument is supplied), and per-row metrics of the model in tabular format.
    .. _toxicity:
        https://huggingface.co/spaces/evaluate-measurement/toxicity
    .. _torch:
        https://pytorch.org/get-started/locally/
    .. _transformers:
        https://huggingface.co/docs/transformers/installation
    .. _ari_grade_level:
        https://en.wikipedia.org/wiki/Automated_readability_index
    .. _flesch_kincaid_grade_level:
        https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#Flesch%E2%80%93Kincaid_grade_level
    .. _evaluate:
        https://pypi.org/project/evaluate
    .. _textstat:
        https://pypi.org/project/textstat
 - For text-summarization models, the default evaluator logs:
    - **metrics**: ``token_count``, `ROUGE`_ (requires `evaluate`_, `nltk`_, and
      `rouge_score`_ to be installed), `toxicity`_ (requires `evaluate`_, `torch`_,
      `transformers`_), `ari_grade_level`_ (requires `textstat`_),
      `flesch_kincaid_grade_level`_ (requires `textstat`_).
    - **artifacts**: A JSON file containing the inputs, outputs, targets (if the ``targets``
      argument is supplied), and per-row metrics of the model in the tabular format.
    .. _ROUGE:
        https://huggingface.co/spaces/evaluate-metric/rouge
    .. _toxicity:
        https://huggingface.co/spaces/evaluate-measurement/toxicity
    .. _torch:
        https://pytorch.org/get-started/locally/
    .. _transformers:
        https://huggingface.co/docs/transformers/installation
    .. _ari_grade_level:
        https://en.wikipedia.org/wiki/Automated_readability_index
    .. _flesch_kincaid_grade_level:
        https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#Flesch%E2%80%93Kincaid_grade_level
    .. _evaluate:
        https://pypi.org/project/evaluate
    .. _nltk:
        https://pypi.org/project/nltk
    .. _rouge_score:
        https://pypi.org/project/rouge-score
    .. _textstat:
        https://pypi.org/project/textstat
 - For text models, the default evaluator logs:
    - **metrics**: ``token_count``, `toxicity`_ (requires `evaluate`_, `torch`_,
      `transformers`_), `ari_grade_level`_ (requires `textstat`_),
      `flesch_kincaid_grade_level`_ (requires `textstat`_).
    - **artifacts**: A JSON file containing the inputs, outputs, targets (if the ``targets``
      argument is supplied), and per-row metrics of the model in tabular format.
    .. _evaluate:
        https://pypi.org/project/evaluate
    .. _toxicity:
        https://huggingface.co/spaces/evaluate-measurement/toxicity
    .. _torch:
        https://pytorch.org/get-started/locally/
    .. _transformers:
        https://huggingface.co/docs/transformers/installation
    .. _ari_grade_level:
        https://en.wikipedia.org/wiki/Automated_readability_index
    .. _flesch_kincaid_grade_level:
        https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#Flesch%E2%80%93Kincaid_grade_level
    .. _textstat:
        https://pypi.org/project/textstat
 - For retriever models, the default evaluator logs:
    - **metrics**: :mod:`precision_at_k(k) &lt;mlflow.metrics.precision_at_k&gt;`,
      :mod:`recall_at_k(k) &lt;mlflow.metrics.recall_at_k&gt;` and
      :mod:`ndcg_at_k(k) &lt;mlflow.metrics.ndcg_at_k&gt;` - all have a default value of
      ``retriever_k`` = 3.
    - **artifacts**: A JSON file containing the inputs, outputs, targets, and per-row metrics
      of the model in tabular format.
 - For sklearn models, the default evaluator additionally logs the model's evaluation criterion
   (e.g. mean accuracy for a classifier) computed by `model.score` method.
 - The metrics/artifacts listed above are logged to the active MLflow run.
   If no active run exists, a new MLflow run is created for logging these metrics and
   artifacts. Note that no metrics/artifacts are logged for the ``baseline_model``.
 - Additionally, information about the specified dataset - hash, name (if specified), path
   (if specified), and the UUID of the model that evaluated it - is logged to the
   ``mlflow.datasets`` tag.
 - The available ``evaluator_config`` options for the default evaluator include:
    - **log_model_explainability**: A boolean value specifying whether or not to log model
      explainability insights, default value is True.
    - **explainability_algorithm**: A string to specify the SHAP Explainer algorithm for model
      explainability. Supported algorithm includes: 'exact', 'permutation', 'partition',
      'kernel'.
      If not set, ``shap.Explainer`` is used with the "auto" algorithm, which chooses the best
      Explainer based on the model.
    - **explainability_nsamples**: The number of sample rows to use for computing model
      explainability insights. Default value is 2000.
    - **explainability_kernel_link**: The kernel link function used by shap kernal explainer.
      Available values are "identity" and "logit". Default value is "identity".
    - **max_classes_for_multiclass_roc_pr**:
      For multiclass classification tasks, the maximum number of classes for which to log
      the per-class ROC curve and Precision-Recall curve. If the number of classes is
      larger than the configured maximum, these curves are not logged.
    - **metric_prefix**: An optional prefix to prepend to the name of each metric and artifact
      produced during evaluation.
    - **log_metrics_with_dataset_info**: A boolean value specifying whether or not to include
      information about the evaluation dataset in the name of each metric logged to MLflow
      Tracking during evaluation, default value is True.
    - **pos_label**: If specified, the positive label to use when computing classification
      metrics such as precision, recall, f1, etc. for binary classification models. For
      multiclass classification and regression models, this parameter will be ignored.
    - **average**: The averaging method to use when computing classification metrics such as
      precision, recall, f1, etc. for multiclass classification models
      (default: ``'weighted'``). For binary classification and regression models, this
      parameter will be ignored.
    - **sample_weights**: Weights for each sample to apply when computing model performance
      metrics.
    - **col_mapping**: A dictionary mapping column names in the input dataset or output
      predictions to column names used when invoking the evaluation functions.
    - **retriever_k**: A parameter used when ``model_type="retriever"`` as the number of
      top-ranked retrieved documents to use when computing the built-in metric
      :mod:`precision_at_k(k) &lt;mlflow.metrics.precision_at_k&gt;`,
      :mod:`recall_at_k(k) &lt;mlflow.metrics.recall_at_k&gt;` and
      :mod:`ndcg_at_k(k) &lt;mlflow.metrics.ndcg_at_k&gt;`. Default value is 3. For all other
      model types, this parameter will be ignored.
 - Limitations of evaluation dataset:
    - For classification tasks, dataset labels are used to infer the total number of classes.
    - For binary classification tasks, the negative label value must be 0 or -1 or False, and
      the positive label value must be 1 or True.
 - Limitations of metrics/artifacts computation:
    - For classification tasks, some metric and artifact computations require the model to
      output class probabilities. Currently, for scikit-learn models, the default evaluator
      calls the ``predict_proba`` method on the underlying model to obtain probabilities. For
      other model types, the default evaluator does not compute metrics/artifacts that require
      probability outputs.
 - Limitations of default evaluator logging model explainability insights:
    - The ``shap.Explainer`` ``auto`` algorithm uses the ``Linear`` explainer for linear models
      and the ``Tree`` explainer for tree models. Because SHAP's ``Linear`` and ``Tree``
      explainers do not support multi-class classification, the default evaluator falls back to
      using the ``Exact`` or ``Permutation`` explainers for multi-class classification tasks.
    - Logging model explainability insights is not currently supported for PySpark models.
    - The evaluation dataset label values must be numeric or boolean, all feature values
      must be numeric, and each feature column must only contain scalar values.
 - Limitations when environment restoration is enabled:
    - When environment restoration is enabled for the evaluated model (i.e. a non-local
      ``env_manager`` is specified), the model is loaded as a client that invokes a MLflow
      Model Scoring Server process in an independent Python environment with the model's
      training time dependencies installed. As such, methods like ``predict_proba`` (for
      probability outputs) or ``score`` (computes the evaluation criterian for sklearn models)
      of the model become inaccessible and the default evaluator does not compute metrics or
      artifacts that require those methods.
    - Because the model is an MLflow Model Server process, SHAP explanations are slower to
      compute. As such, model explainaibility is disabled when a non-local ``env_manager``
      specified, unless the ``evaluator_config`` option **log_model_explainability** is
      explicitly set to ``True``.
Args:
    model: Optional. If specified, it should be one of the following:
        - A pyfunc model instance
        - A URI referring to a pyfunc model
        - A URI referring to an MLflow Deployments endpoint e.g. ``"endpoints:/my-chat"``
        - A callable function: This function should be able to take in model input and
          return predictions. It should follow the signature of the
          :py:func:`predict &lt;mlflow.pyfunc.PyFuncModel.predict&gt;` method. Here's an example
          of a valid function:
          ..code-block:: python
              model = mlflow.pyfunc.load_model(model_uri)
              def fn(model_input):
                  return model.predict(model_input)
        If omitted, it indicates a static dataset will be used for evaluation instead of a
        model.  In this case, the ``data`` argument must be a Pandas DataFrame or an mlflow
        PandasDataset that contains model outputs, and the ``predictions`` argument must be the
        name of the column in ``data`` that contains model outputs.
    data: One of the
        following:
        - A numpy array or list of evaluation features, excluding labels.
        - A Pandas DataFrame containing evaluation features, labels, and optionally model
            outputs. Model outputs are required to be provided when model is unspecified.
            If ``feature_names`` argument not specified, all columns except for the label
            column and model_output column are regarded as feature columns. Otherwise,
            only column names present in ``feature_names`` are regarded as feature columns.
        -  A Spark DataFrame containing evaluation features and labels. If
            ``feature_names`` argument not specified, all columns except for the label
            column are regarded as feature columns. Otherwise, only column names present in
            ``feature_names`` are regarded as feature columns. Only the first 10000 rows in
            the Spark DataFrame will be used as evaluation data.
        - A :py:class:`mlflow.data.dataset.Dataset` instance containing evaluation
            features, labels, and optionally model outputs. Model outputs are only supported
            with a PandasDataset. Model outputs are required when model is unspecified, and
            should be specified via the ``predictions`` prerty of the PandasDataset.
    targets: If ``data`` is a numpy array or list, a numpy array or list of evaluation
        labels. If ``data`` is a DataFrame, the string name of a column from ``data``
        that contains evaluation labels. Required for classifier and regressor models,
        but optional for question-answering, text-summarization, and text models. If
        ``data`` is a :py:class:`mlflow.data.dataset.Dataset` that defines targets,
        then ``targets`` is optional.
    predictions: Optional. The name of the column that contains model outputs.
        - When ``model`` is specified and outputs multiple columns, ``predictions`` can be used
          to specify the name of the column that will be used to store model outputs for
          evaluation.
        - When ``model`` is not specified and ``data`` is a pandas dataframe,
          ``predictions`` can be used to specify the name of the column in ``data`` that
          contains model outputs.
        .. code-block:: python
            :caption: Example usage of predictions
            # Evaluate a model that outputs multiple columns
            data = pd.DataFrame({"question": ["foo"]})
            def model(inputs):
                return pd.DataFrame({"answer": ["bar"], "source": ["baz"]})
            results = evaluate(model=model, data=data, predictions="answer", ...)
            # Evaluate a static dataset
            data = pd.DataFrame({"question": ["foo"], "answer": ["bar"], "source": ["baz"]})
            results = evaluate(data=data, predictions="answer", ...)
    model_type: (Optional) A string describing the model type. The default evaluator
        supports the following model types:
        - ``'classifier'``
        - ``'regressor'``
        - ``'question-answering'``
        - ``'text-summarization'``
        - ``'text'``
        - ``'retriever'``
        If no ``model_type`` is specified, then you must provide a a list of
        metrics to compute via the ``extra_metrics`` param.
        .. note::
            ``'question-answering'``, ``'text-summarization'``, ``'text'``, and
            ``'retriever'`` are experimental and may be changed or removed in a
            future release.
    inference_params: (Optional) A dictionary of inference parameters to be passed to the model
        when making predictions, such as ``{"max_tokens": 100}``. This is only used when
        the ``model`` is an MLflow Deployments endpoint URI e.g. ``"endpoints:/my-chat"``
    dataset_path: (Optional) The path where the data is stored. Must not contain double
        quotes (``“``). If specified, the path is logged to the ``mlflow.datasets``
        tag for lineage tracking purposes.
    feature_names: (Optional) A list. If the ``data`` argument is a numpy array or list,
        ``feature_names`` is a list of the feature names for each feature. If
        ``feature_names=None``, then the ``feature_names`` are generated using the
        format ``feature_{feature_index}``. If the ``data`` argument is a Pandas
        DataFrame or a Spark DataFrame, ``feature_names`` is a list of the names
        of the feature columns in the DataFrame. If ``feature_names=None``, then
        all columns except the label column and the predictions column are
        regarded as feature columns.
    evaluators: The name of the evaluator to use for model evaluation, or a list of
        evaluator names. If unspecified, all evaluators capable of evaluating the
        specified model on the specified dataset are used. The default evaluator
        can be referred to by the name ``"default"``. To see all available
        evaluators, call :py:func:`mlflow.models.list_evaluators`.
    evaluator_config: A dictionary of additional configurations to supply to the evaluator.
        If multiple evaluators are specified, each configuration should be
        supplied as a nested dictionary whose key is the evaluator name.
    extra_metrics:
        (Optional) A list of :py:class:`EvaluationMetric &lt;mlflow.models.EvaluationMetric&gt;`
        objects.  These metrics are computed in addition to the default metrics associated with
        pre-defined `model_type`, and setting `model_type=None` will only compute the metrics
        specified in `extra_metrics`. See the `mlflow.metrics` module for more information about
        the builtin metrics and how to define extra metrics.
        .. code-block:: python
            :caption: Example usage of extra metrics
            import mlflow
            import numpy as np
            def root_mean_squared_error(eval_df, _builtin_metrics):
                return np.sqrt((np.abs(eval_df["prediction"] - eval_df["target"]) ** 2).mean)
            rmse_metric = mlflow.models.make_metric(
                eval_fn=root_mean_squared_error,
                greater_is_better=False,
            )
            mlflow.evaluate(..., extra_metrics=[rmse_metric])
    custom_artifacts:
        (Optional) A list of custom artifact functions with the following signature:
        .. code-block:: python
            def custom_artifact(
                eval_df: Union[pandas.Dataframe, pyspark.sql.DataFrame],
                builtin_metrics: Dict[str, float],
                artifacts_dir: str,
            ) -&gt; Dict[str, Any]:
                """
                Args:
                    eval_df:
                        A Pandas or Spark DataFrame containing ``prediction`` and ``target``
                        column.  The ``prediction`` column contains the predictions made by the
                        model.  The ``target`` column contains the corresponding labels to the
                        predictions made on that row.
                    builtin_metrics:
                        A dictionary containing the metrics calculated by the default evaluator.
                        The keys are the names of the metrics and the values are the scalar
                        values of the metrics. Refer to the DefaultEvaluator behavior section
                        for what metrics will be returned based on the type of model (i.e.
                        classifier or regressor).
                    artifacts_dir:
                        A temporary directory path that can be used by the custom artifacts
                        function to temporarily store produced artifacts. The directory will be
                        deleted after the artifacts are logged.
                Returns:
                    A dictionary that maps artifact names to artifact objects
                    (e.g. a Matplotlib Figure) or to artifact paths within ``artifacts_dir``.
                """
                ...
        Object types that artifacts can be represented as:
            - A string uri representing the file path to the artifact. MLflow will infer the
              type of the artifact based on the file extension.
            - A string representation of a JSON object. This will be saved as a .json artifact.
            - Pandas DataFrame. This will be resolved as a CSV artifact.
            - Numpy array. This will be saved as a .npy artifact.
            - Matplotlib Figure. This will be saved as an image artifact. Note that
              ``matplotlib.pyplot.savefig`` is called behind the scene with default
              configurations.
              To customize, either save the figure with the desired configurations and return
              its file path or define customizations through environment variables in
              ``matplotlib.rcParams``.
            - Other objects will be attempted to be pickled with the default protocol.
        .. code-block:: python
            :caption: Example usage of custom artifacts
            import mlflow
            import matplotlib.pyplot as plt
            def scatter_plot(eval_df, builtin_metrics, artifacts_dir):
                plt.scatter(eval_df["prediction"], eval_df["target"])
                plt.xlabel("Targets")
                plt.ylabel("Predictions")
                plt.title("Targets vs. Predictions")
                plt.savefig(os.path.join(artifacts_dir, "example.png"))
                plt.close()
                return {"pred_target_scatter": os.path.join(artifacts_dir, "example.png")}
            def pred_sample(eval_df, _builtin_metrics, _artifacts_dir):
                return {"pred_sample": pred_sample.head(10)}
            mlflow.evaluate(..., custom_artifacts=[scatter_plot, pred_sample])
    validation_thresholds: (Optional) A dictionary of metric name to
        :py:class:`mlflow.models.MetricThreshold` used for model validation. Each metric name
        must either be the name of a builtin metric or the name of a metric defined in the
        ``extra_metrics`` parameter.
        .. code-block:: python
            :caption: Example of Model Validation
            from mlflow.models import MetricThreshold
            thresholds = {
                "accuracy_score": MetricThreshold(
                    # accuracy should be &gt;=0.8
                    threshold=0.8,
                    # accuracy should be at least 5 percent greater than baseline model accuracy
                    min_absolute_change=0.05,
                    # accuracy should be at least 0.05 greater than baseline model accuracy
                    min_relative_change=0.05,
                    greater_is_better=True,
                ),
            }
            with mlflow.start_run():
                mlflow.evaluate(
                    model=your_candidate_model,
                    data,
                    targets,
                    model_type,
                    dataset_name,
                    evaluators,
                    validation_thresholds=thresholds,
                    baseline_model=your_baseline_model,
                )
        See :ref:`the Model Validation documentation &lt;model-validation&gt;`
        for more details.
    baseline_model: (Optional) A string URI referring to an MLflow model with the pyfunc
        flavor. If specified, the candidate ``model`` is compared to this
        baseline for model validation purposes.
    env_manager: Specify an environment manager to load the candidate ``model`` and
        ``baseline_model`` in isolated Python environments and restore their
        dependencies. Default value is ``local``, and the following values are
        supported:
        - ``virtualenv``: (Recommended) Use virtualenv to restore the python
          environment that was used to train the model.
        - ``conda``:  Use Conda to restore the software environment that was used
          to train the model.
        - ``local``: Use the current Python environment for model inference, which
          may differ from the environment used to train the model and may lead to
          errors or invalid predictions.
    model_config: the model configuration to use for loading the model with pyfunc. Inspect
        the model's pyfunc flavor to know which keys are supported for your
        specific model. If not indicated, the default model configuration
        from the model is used (if any).
    baseline_config: the model configuration to use for loading the baseline
        model. If not indicated, the default model configuration
        from the baseline model is used (if any).
Returns:
    An :py:class:`mlflow.models.EvaluationResult` instance containing
    metrics of candidate model and baseline model, and artifacts of candidate model.
<span class="ansi-red-fg">File:</span>      /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/mlflow/models/evaluation/base.py
<span class="ansi-red-fg">Type:</span>      function</pre>
</div>
</div>
</div>
<div id="7d0a3450-164c-4e50-be3f-d677db2daddb" class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>debug</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/sklearn/metrics/_classification.py(95)_check_targets()
     93 
     94     if len(y_type) &gt; 1:
---&gt; 95         raise ValueError(
     96             "Classification metrics can't handle a mix of {0} and {1} targets".format(
     97                 type_true, type_pred

{'binary', 'continuous'}
     59 def _check_targets(y_true, y_pred):
     60     """Check that y_true and y_pred belong to the same classification task.
     61 
     62     This converts multiclass or binary types to a common shape, and raises a
     63     ValueError for a mix of multilabel and multiclass targets, a mix of
     64     multilabel formats, for the presence of continuous-valued or multioutput
     65     targets, or for targets of different lengths.
     66 
     67     Column vectors are squeezed to 1d, while multilabel formats are returned
     68     as CSR sparse label indicators.
     69 
     70     Parameters
     71     ----------
     72     y_true : array-like
     73 
     74     y_pred : array-like
     75 
     76     Returns
     77     -------
     78     type_true : one of {'multilabel-indicator', 'multiclass', 'binary'}
     79         The type of the true target data, as output by
     80         ``utils.multiclass.type_of_target``.
     81 
     82     y_true : array or indicator matrix
     83 
     84     y_pred : array or indicator matrix
     85     """
     86     check_consistent_length(y_true, y_pred)
     87     type_true = type_of_target(y_true, input_name="y_true")
     88     type_pred = type_of_target(y_pred, input_name="y_pred")
     89 
     90     y_type = {type_true, type_pred}
     91     if y_type == {"binary", "multiclass"}:
     92         y_type = {"multiclass"}
     93 
     94     if len(y_type) &gt; 1:
---&gt; 95         raise ValueError(
     96             "Classification metrics can't handle a mix of {0} and {1} targets".format(
     97                 type_true, type_pred
     98             )
     99         )
    100 
    101     # We can't have more than one value on y_type =&gt; The set is no more needed
    102     y_type = y_type.pop()
    103 
    104     # No metrics support "multiclass-multioutput" format
    105     if y_type not in ["binary", "multiclass", "multilabel-indicator"]:
    106         raise ValueError("{0} is not supported".format(y_type))
    107 
    108     if y_type in ["binary", "multiclass"]:
    109         y_true = column_or_1d(y_true)
    110         y_pred = column_or_1d(y_pred)
    111         if y_type == "binary":
    112             try:
    113                 unique_values = np.union1d(y_true, y_pred)
    114             except TypeError as e:
    115                 # We expect y_true and y_pred to be of the same data type.
    116                 # If `y_true` was provided to the classifier as strings,
    117                 # `y_pred` given by the classifier will also be encoded with
    118                 # strings. So we raise a meaningful error
    119                 raise TypeError(
    120                     "Labels in y_true and y_pred should be of the same type. "
    121                     f"Got y_true={np.unique(y_true)} and "
    122                     f"y_pred={np.unique(y_pred)}. Make sure that the "
    123                     "predictions provided by the classifier coincides with "
    124                     "the true labels."
    125                 ) from e
    126             if len(unique_values) &gt; 2:
    127                 y_type = "multiclass"
    128 
    129     if y_type.startswith("multilabel"):
    130         y_true = csr_matrix(y_true)
    131         y_pred = csr_matrix(y_pred)
    132         y_type = "multilabel-indicator"
    133 
    134     return y_type, y_true, y_pred
    135 

{'binary', 'continuous'}
*** SyntaxError: '[' was never closed
False
'binary'
array([0.08, 0.84, 1.  , ..., 0.32, 0.88, 0.04])
array([0.  , 0.04, 0.08, 0.12, 0.16, 0.2 , 0.24, 0.28, 0.32, 0.36, 0.4 ,
       0.44, 0.48, 0.52, 0.56, 0.6 , 0.64, 0.68, 0.72, 0.76, 0.8 , 0.84,
       0.88, 0.92, 0.96, 1.  ])
&gt; /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/sklearn/metrics/_classification.py(317)confusion_matrix()
    315     (0, 2, 1, 1)
    316     """
--&gt; 317     y_type, y_true, y_pred = _check_targets(y_true, y_pred)
    318     if y_type not in ("binary", "multiclass"):
    319         raise ValueError("%s is not supported" % y_type)

&gt; /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py(456)safe_patch_function()
    454                     reroute_warnings=False,
    455                 ):
--&gt; 456                     return original(*args, **kwargs)
    457 
    458             # Whether or not the original / underlying function has been called during the

&lt;function confusion_matrix at 0x7f0fd89da3b0&gt;
    382     def safe_patch_function(*args, **kwargs):
    383         """
    384         A safe wrapper around the specified `patch_function` implementation designed to
    385         handle exceptions thrown during the execution of `patch_function`. This wrapper
    386         distinguishes exceptions thrown from the underlying / original function
    387         (`&lt;destination&gt;.&lt;function_name&gt;`) from exceptions thrown from other parts of
    388         `patch_function`. This distinction is made by passing an augmented version of the
    389         underlying / original function to `patch_function` that uses nonlocal state to track
    390         whether or not it has been executed and whether or not it threw an exception.
    391         Exceptions thrown from the underlying / original function are propagated to the caller,
    392         while exceptions thrown from other parts of `patch_function` are caught and logged as
    393         warnings.
    394         """
    395         # Reroute warnings encountered during the patch function implementation to an MLflow event
    396         # logger, and enforce silent mode if applicable (i.e. if the corresponding autologging
    397         # integration was called with `silent=True`), hiding MLflow event logging statements and
    398         # hiding all warnings in the autologging preamble and postamble (i.e. the code surrounding
    399         # the user's original / underlying ML function). Non-MLflow warnings are enabled during the
    400         # execution of the original / underlying ML function
    401         #
    402         # Note that we've opted *not* to apply this context manager as a decorator on
    403         # `safe_patch_function` because the context-manager-as-decorator pattern uses
    404         # `contextlib.ContextDecorator`, which creates generator expressions that cannot be pickled
    405         # during model serialization by ML frameworks such as scikit-learn
    406         is_silent_mode = get_autologging_config(autologging_integration, "silent", False)
    407         with set_mlflow_events_and_warnings_behavior_globally(
    408             # MLflow warnings emitted during autologging training sessions are likely not
    409             # actionable and result from the autologging implementation invoking another MLflow
    410             # API. Accordingly, we reroute these warnings to the MLflow event logger with level
    411             # WARNING For reference, see recommended warning and event logging behaviors from
    412             # https://docs.python.org/3/howto/logging.html#when-to-use-logging
    413             reroute_warnings=True,
    414             disable_event_logs=is_silent_mode,
    415             disable_warnings=is_silent_mode,
    416         ), set_non_mlflow_warnings_behavior_for_current_thread(
    417             # non-MLflow Warnings emitted during the autologging preamble (before the original /
    418             # underlying ML function is called) and postamble (after the original / underlying ML
    419             # function is called) are likely not actionable and result from the autologging
    420             # implementation invoking an API from a dependent library. Accordingly, we reroute
    421             # these warnings to the MLflow event logger with level WARNING. For reference, see
    422             # recommended warning and event logging behaviors from
    423             # https://docs.python.org/3/howto/logging.html#when-to-use-logging
    424             reroute_warnings=True,
    425             disable_warnings=is_silent_mode,
    426         ):
    427             if is_testing():
    428                 preexisting_run_for_testing = mlflow.active_run()
    429 
    430             # Whether or not to exclude autologged content from user-created fluent runs
    431             # (i.e. runs created manually via `mlflow.start_run()`)
    432             exclusive = get_autologging_config(autologging_integration, "exclusive", False)
    433             user_created_fluent_run_is_active = (
    434                 mlflow.active_run() and not _AutologgingSessionManager.active_session()
    435             )
    436             active_session_failed = (
    437                 _AutologgingSessionManager.active_session() is not None
    438                 and _AutologgingSessionManager.active_session().state == "failed"
    439             )
    440 
    441             if (
    442                 active_session_failed
    443                 or autologging_is_disabled(autologging_integration)
    444                 or (user_created_fluent_run_is_active and exclusive)
    445                 or mlflow.utils.autologging_utils._AUTOLOGGING_GLOBALLY_DISABLED
    446             ):
    447                 # If the autologging integration associated with this patch is disabled,
    448                 # or if the current autologging integration is in exclusive mode and a user-created
    449                 # fluent run is active, call the original function and return. Restore the original
    450                 # warning behavior during original function execution, since autologging is being
    451                 # skipped
    452                 with set_non_mlflow_warnings_behavior_for_current_thread(
    453                     disable_warnings=False,
    454                     reroute_warnings=False,
    455                 ):
--&gt; 456                     return original(*args, **kwargs)
    457 
    458             # Whether or not the original / underlying function has been called during the
    459             # execution of patched code
    460             original_has_been_called = False
    461             # The value returned by the call to the original / underlying function during
    462             # the execution of patched code
    463             original_result = None
    464             # Whether or not an exception was raised from within the original / underlying function
    465             # during the execution of patched code
    466             failed_during_original = False
    467             # The active MLflow run (if any) associated with patch code execution
    468             patch_function_run_for_testing = None
    469             # The exception raised during executing patching function
    470             patch_function_exception = None
    471 
    472             def try_log_autologging_event(log_fn, *args):
    473                 try:
    474                     log_fn(*args)
    475                 except Exception as e:
    476                     _logger.debug(
    477                         "Failed to log autologging event via '%s'. Exception: %s",
    478                         log_fn,
    479                         e,
    480                     )
    481 
    482             def call_original_fn_with_event_logging(original_fn, og_args, og_kwargs):
    483                 try:
    484                     try_log_autologging_event(
    485                         AutologgingEventLogger.get_logger().log_original_function_start,
    486                         session,
    487                         destination,
    488                         function_name,
    489                         og_args,
    490                         og_kwargs,
    491                     )
    492                     original_fn_result = original_fn(*og_args, **og_kwargs)
    493 
    494                     try_log_autologging_event(
    495                         AutologgingEventLogger.get_logger().log_original_function_success,
    496                         session,
    497                         destination,
    498                         function_name,
    499                         og_args,
    500                         og_kwargs,
    501                     )
    502                     return original_fn_result
    503                 except Exception as original_fn_e:
    504                     try_log_autologging_event(
    505                         AutologgingEventLogger.get_logger().log_original_function_error,
    506                         session,
    507                         destination,
    508                         function_name,
    509                         og_args,
    510                         og_kwargs,
    511                         original_fn_e,
    512                     )
    513 
    514                     nonlocal failed_during_original
    515                     failed_during_original = True
    516                     raise
    517 
    518             with _AutologgingSessionManager.start_session(autologging_integration) as session:
    519                 try:
    520 
    521                     def call_original(*og_args, **og_kwargs):
    522                         def _original_fn(*_og_args, **_og_kwargs):
    523                             if is_testing():
    524                                 _validate_args(
    525                                     autologging_integration,
    526                                     function_name,
    527                                     args,
    528                                     kwargs,
    529                                     og_args,
    530                                     og_kwargs,
    531                                 )
    532                                 # By the time `original` is called by the patch implementation, we
    533                                 # assume that either: 1. the patch implementation has already
    534                                 # created an MLflow run or 2. the patch code will not create an
    535                                 # MLflow run during the current execution. Here, we capture a
    536                                 # reference to the active run, which we will use later on to
    537                                 # determine whether or not the patch implementation created
    538                                 # a run and perform validation if necessary
    539                                 nonlocal patch_function_run_for_testing
    540                                 patch_function_run_for_testing = mlflow.active_run()
    541 
    542                             nonlocal original_has_been_called
    543                             original_has_been_called = True
    544 
    545                             nonlocal original_result
    546                             # Show all non-MLflow warnings as normal (i.e. not as event logs)
    547                             # during original function execution, even if silent mode is enabled
    548                             # (`silent=True`), since these warnings originate from the ML framework
    549                             # or one of its dependencies and are likely relevant to the caller
    550                             with set_non_mlflow_warnings_behavior_for_current_thread(
    551                                 disable_warnings=False,
    552                                 reroute_warnings=False,
    553                             ):
    554                                 original_result = original(*_og_args, **_og_kwargs)
    555                                 return original_result
    556 
    557                         return call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)
    558 
    559                     # Apply the name, docstring, and signature of `original` to `call_original`.
    560                     # This is important because several autologging patch implementations inspect
    561                     # the signature of the `original` argument during execution
    562                     call_original = update_wrapper_extended(call_original, original)
    563 
    564                     try_log_autologging_event(
    565                         AutologgingEventLogger.get_logger().log_patch_function_start,
    566                         session,
    567                         destination,
    568                         function_name,
    569                         args,
    570                         kwargs,
    571                     )
    572 
    573                     if patch_is_class:
    574                         patch_function.call(call_original, *args, **kwargs)
    575                     else:
    576                         patch_function(call_original, *args, **kwargs)
    577 
    578                     session.state = "succeeded"
    579 
    580                     try_log_autologging_event(
    581                         AutologgingEventLogger.get_logger().log_patch_function_success,
    582                         session,
    583                         destination,
    584                         function_name,
    585                         args,
    586                         kwargs,
    587                     )
    588                 except Exception as e:
    589                     session.state = "failed"
    590                     patch_function_exception = e
    591                     # Exceptions thrown during execution of the original function should be
    592                     # propagated to the caller. Additionally, exceptions encountered during test
    593                     # mode should be reraised to detect bugs in autologging implementations
    594                     if failed_during_original or is_testing():
    595                         raise
    596 
    597                 if is_testing() and not preexisting_run_for_testing:
    598                     # If an MLflow run was created during the execution of patch code, verify that
    599                     # it is no longer active and that it contains expected autologging tags
    600                     assert (
    601                         not mlflow.active_run()
    602                     ), f"Autologging integration {autologging_integration} leaked an active run"
    603                     if patch_function_run_for_testing:
    604                         _validate_autologging_run(
    605                             autologging_integration, patch_function_run_for_testing.info.run_id
    606                         )
    607                 try:
    608                     if original_has_been_called:
    609                         return original_result
    610                     else:
    611                         return call_original_fn_with_event_logging(original, args, kwargs)
    612                 finally:
    613                     # If original function succeeds, but `patch_function_exception` exists,
    614                     # it represent patching code unexpected failure, so we call
    615                     # `log_patch_function_error` in this case.
    616                     # If original function failed, we don't call `log_patch_function_error`
    617                     # even if `patch_function_exception` exists, because original function failure
    618                     # means there's some error in user code (e.g. user provide wrong arguments)
    619                     if patch_function_exception is not None and not failed_during_original:
    620                         try_log_autologging_event(
    621                             AutologgingEventLogger.get_logger().log_patch_function_error,
    622                             session,
    623                             destination,
    624                             function_name,
    625                             args,
    626                             kwargs,
    627                             patch_function_exception,
    628                         )
    629 
    630                         _logger.warning(
    631                             "Encountered unexpected error during %s autologging: %s",
    632                             autologging_integration,
    633                             patch_function_exception,
    634                         )
    635 

&gt; /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/mlflow/models/evaluation/default_evaluator.py(247)_get_binary_classifier_metrics()
    245 ):
    246     with _suppress_class_imbalance_errors(ValueError):
--&gt; 247         tn, fp, fn, tp = sk_metrics.confusion_matrix(y_true, y_pred).ravel()
    248         return {
    249             "true_negatives": tn,

array([0.08, 0.84, 1.  , ..., 0.32, 0.88, 0.04])
&gt; /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/mlflow/models/evaluation/default_evaluator.py(1521)_compute_builtin_metrics()
   1519         if self.model_type == _ModelType.CLASSIFIER:
   1520             if self.is_binomial:
-&gt; 1521                 metrics = _get_binary_classifier_metrics(
   1522                     y_true=self.y,
   1523                     y_pred=self.y_pred,

&lt;mlflow.models.evaluation.default_evaluator.DefaultEvaluator object at 0x7f0fcb072800&gt;
True
   1514     def _compute_builtin_metrics(self):
   1515         """
   1516         Helper method for computing builtin metrics
   1517         """
   1518         self._evaluate_sklearn_model_score_if_scorable()
   1519         if self.model_type == _ModelType.CLASSIFIER:
   1520             if self.is_binomial:
-&gt; 1521                 metrics = _get_binary_classifier_metrics(
   1522                     y_true=self.y,
   1523                     y_pred=self.y_pred,
   1524                     y_proba=self.y_probs,
   1525                     labels=self.label_list,
   1526                     pos_label=self.pos_label,
   1527                     sample_weights=self.sample_weights,
   1528                 )
   1529                 if metrics:
   1530                     self.metrics_values.update(_get_aggregate_metrics_values(metrics))
   1531                     self._compute_roc_and_pr_curve()
   1532             else:
   1533                 average = self.evaluator_config.get("average", "weighted")
   1534                 metrics = _get_multiclass_classifier_metrics(
   1535                     y_true=self.y,
   1536                     y_pred=self.y_pred,
   1537                     y_proba=self.y_probs,
   1538                     labels=self.label_list,
   1539                     average=average,
   1540                     sample_weights=self.sample_weights,
   1541                 )
   1542                 if metrics:
   1543                     self.metrics_values.update(_get_aggregate_metrics_values(metrics))
   1544         elif self.model_type == _ModelType.REGRESSOR:
   1545             self.metrics_values.update(
   1546                 _get_aggregate_metrics_values(
   1547                     _get_regressor_metrics(self.y, self.y_pred, self.sample_weights)
   1548                 )
   1549             )
   1550 
</code></pre>
</div>
<div class="cell-output cell-output-stdin">
<pre><code>ipdb&gt;  y_type
ipdb&gt;  ll
ipdb&gt;  y_type 
ipdb&gt;  y_type in ["binary", "multiclass"
ipdb&gt;  y_type in ["binary", "multiclass"]
ipdb&gt;  type_true 
ipdb&gt;  y_pred
ipdb&gt;  np.unique(y_pred)
ipdb&gt;  up
ipdb&gt;  up
ipdb&gt;  original
ipdb&gt;  ll
ipdb&gt;  up
ipdb&gt;  y_pred
ipdb&gt;  up
ipdb&gt;  self
ipdb&gt;  self.is_binomial
ipdb&gt;  ll
ipdb&gt;  q</code></pre>
</div>
</div>
<div id="2b0a4caf-42b9-4d7f-9267-81a251b828a4" class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> root_mean_squared_error(eval_df, _builtin_metrics):</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>    precision, recall, thresholds <span class="op">=</span> precision_recall_curve (eval_df[<span class="st">"label"</span>], eval_df[<span class="st">"predictions"</span>])</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>    thresholds <span class="op">=</span> np.append ([thresholds, <span class="fl">1.0</span>])</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.sqrt((np.<span class="bu">abs</span>( <span class="op">-</span> ) <span class="op">**</span> <span class="dv">2</span>).mean)</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>rmse_metric <span class="op">=</span> mlflow.models.make_metric(</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>    eval_fn<span class="op">=</span>root_mean_squared_error,</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>    greater_is_better<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>mlflow.evaluate(..., extra_metrics<span class="op">=</span>[rmse_metric])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="26153e9f-d97c-4cc3-b937-e05bc82ee054" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlflow</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>mlflow.__version__</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>'2.13.1'</code></pre>
</div>
</div>
<div id="0176deb8-3abd-49eb-b6b7-2f667c319154" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>mlflow.evaluate?</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">Signature:</span>
mlflow<span class="ansi-blue-fg">.</span>evaluate<span class="ansi-blue-fg">(</span>
    model<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    data<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    <span class="ansi-blue-fg">*</span><span class="ansi-blue-fg">,</span>
    model_type<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    targets<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    predictions<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    dataset_path<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    feature_names<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    evaluators<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    evaluator_config<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    custom_metrics<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    extra_metrics<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    custom_artifacts<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    validation_thresholds<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    baseline_model<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    env_manager<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">'local'</span><span class="ansi-blue-fg">,</span>
    model_config<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    baseline_config<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    inference_params<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
<span class="ansi-blue-fg">)</span>
<span class="ansi-red-fg">Docstring:</span>
Evaluate the model performance on given data and selected metrics.
This function evaluates a PyFunc model or custom callable on the specified dataset using
specified ``evaluators``, and logs resulting metrics &amp; artifacts to MLflow tracking server.
Users can also skip setting ``model`` and put the model outputs in ``data`` directly for
evaluation. For detailed information, please read
:ref:`the Model Evaluation documentation &lt;model-evaluation&gt;`.
Default Evaluator behavior:
 - The default evaluator, which can be invoked with ``evaluators="default"`` or
   ``evaluators=None``, supports model types listed below. For each pre-defined model type, the
   default evaluator evaluates your model on a selected set of metrics and generate artifacts
   like plots. Please find more details below.
 - For both the ``"regressor"`` and ``"classifier"`` model types, the default evaluator
   generates model summary plots and feature importance plots using
   `SHAP &lt;https://shap.readthedocs.io/en/latest/index.html&gt;`_.
 - For regressor models, the default evaluator additionally logs:
    - **metrics**: example_count, mean_absolute_error, mean_squared_error,
      root_mean_squared_error, sum_on_target, mean_on_target, r2_score, max_error,
      mean_absolute_percentage_error.
 - For binary classifiers, the default evaluator additionally logs:
    - **metrics**: true_negatives, false_positives, false_negatives, true_positives, recall,
      precision, f1_score, accuracy_score, example_count, log_loss, roc_auc,
      precision_recall_auc.
    - **artifacts**: lift curve plot, precision-recall plot, ROC plot.
 - For multiclass classifiers, the default evaluator additionally logs:
    - **metrics**: accuracy_score, example_count, f1_score_micro, f1_score_macro, log_loss
    - **artifacts**: A CSV file for "per_class_metrics" (per-class metrics includes
      true_negatives/false_positives/false_negatives/true_positives/recall/precision/roc_auc,
      precision_recall_auc), precision-recall merged curves plot, ROC merged curves plot.
 - For question-answering models, the default evaluator logs:
    - **metrics**: ``exact_match``, ``token_count``, `toxicity`_ (requires `evaluate`_,
      `torch`_, `flesch_kincaid_grade_level`_ (requires `textstat`_) and `ari_grade_level`_.
    - **artifacts**: A JSON file containing the inputs, outputs, targets (if the ``targets``
      argument is supplied), and per-row metrics of the model in tabular format.
    .. _toxicity:
        https://huggingface.co/spaces/evaluate-measurement/toxicity
    .. _torch:
        https://pytorch.org/get-started/locally/
    .. _transformers:
        https://huggingface.co/docs/transformers/installation
    .. _ari_grade_level:
        https://en.wikipedia.org/wiki/Automated_readability_index
    .. _flesch_kincaid_grade_level:
        https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#Flesch%E2%80%93Kincaid_grade_level
    .. _evaluate:
        https://pypi.org/project/evaluate
    .. _textstat:
        https://pypi.org/project/textstat
 - For text-summarization models, the default evaluator logs:
    - **metrics**: ``token_count``, `ROUGE`_ (requires `evaluate`_, `nltk`_, and
      `rouge_score`_ to be installed), `toxicity`_ (requires `evaluate`_, `torch`_,
      `transformers`_), `ari_grade_level`_ (requires `textstat`_),
      `flesch_kincaid_grade_level`_ (requires `textstat`_).
    - **artifacts**: A JSON file containing the inputs, outputs, targets (if the ``targets``
      argument is supplied), and per-row metrics of the model in the tabular format.
    .. _ROUGE:
        https://huggingface.co/spaces/evaluate-metric/rouge
    .. _toxicity:
        https://huggingface.co/spaces/evaluate-measurement/toxicity
    .. _torch:
        https://pytorch.org/get-started/locally/
    .. _transformers:
        https://huggingface.co/docs/transformers/installation
    .. _ari_grade_level:
        https://en.wikipedia.org/wiki/Automated_readability_index
    .. _flesch_kincaid_grade_level:
        https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#Flesch%E2%80%93Kincaid_grade_level
    .. _evaluate:
        https://pypi.org/project/evaluate
    .. _nltk:
        https://pypi.org/project/nltk
    .. _rouge_score:
        https://pypi.org/project/rouge-score
    .. _textstat:
        https://pypi.org/project/textstat
 - For text models, the default evaluator logs:
    - **metrics**: ``token_count``, `toxicity`_ (requires `evaluate`_, `torch`_,
      `transformers`_), `ari_grade_level`_ (requires `textstat`_),
      `flesch_kincaid_grade_level`_ (requires `textstat`_).
    - **artifacts**: A JSON file containing the inputs, outputs, targets (if the ``targets``
      argument is supplied), and per-row metrics of the model in tabular format.
    .. _evaluate:
        https://pypi.org/project/evaluate
    .. _toxicity:
        https://huggingface.co/spaces/evaluate-measurement/toxicity
    .. _torch:
        https://pytorch.org/get-started/locally/
    .. _transformers:
        https://huggingface.co/docs/transformers/installation
    .. _ari_grade_level:
        https://en.wikipedia.org/wiki/Automated_readability_index
    .. _flesch_kincaid_grade_level:
        https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#Flesch%E2%80%93Kincaid_grade_level
    .. _textstat:
        https://pypi.org/project/textstat
 - For retriever models, the default evaluator logs:
    - **metrics**: :mod:`precision_at_k(k) &lt;mlflow.metrics.precision_at_k&gt;`,
      :mod:`recall_at_k(k) &lt;mlflow.metrics.recall_at_k&gt;` and
      :mod:`ndcg_at_k(k) &lt;mlflow.metrics.ndcg_at_k&gt;` - all have a default value of
      ``retriever_k`` = 3.
    - **artifacts**: A JSON file containing the inputs, outputs, targets, and per-row metrics
      of the model in tabular format.
 - For sklearn models, the default evaluator additionally logs the model's evaluation criterion
   (e.g. mean accuracy for a classifier) computed by `model.score` method.
 - The metrics/artifacts listed above are logged to the active MLflow run.
   If no active run exists, a new MLflow run is created for logging these metrics and
   artifacts. Note that no metrics/artifacts are logged for the ``baseline_model``.
 - Additionally, information about the specified dataset - hash, name (if specified), path
   (if specified), and the UUID of the model that evaluated it - is logged to the
   ``mlflow.datasets`` tag.
 - The available ``evaluator_config`` options for the default evaluator include:
    - **log_model_explainability**: A boolean value specifying whether or not to log model
      explainability insights, default value is True.
    - **explainability_algorithm**: A string to specify the SHAP Explainer algorithm for model
      explainability. Supported algorithm includes: 'exact', 'permutation', 'partition',
      'kernel'.
      If not set, ``shap.Explainer`` is used with the "auto" algorithm, which chooses the best
      Explainer based on the model.
    - **explainability_nsamples**: The number of sample rows to use for computing model
      explainability insights. Default value is 2000.
    - **explainability_kernel_link**: The kernel link function used by shap kernal explainer.
      Available values are "identity" and "logit". Default value is "identity".
    - **max_classes_for_multiclass_roc_pr**:
      For multiclass classification tasks, the maximum number of classes for which to log
      the per-class ROC curve and Precision-Recall curve. If the number of classes is
      larger than the configured maximum, these curves are not logged.
    - **metric_prefix**: An optional prefix to prepend to the name of each metric and artifact
      produced during evaluation.
    - **log_metrics_with_dataset_info**: A boolean value specifying whether or not to include
      information about the evaluation dataset in the name of each metric logged to MLflow
      Tracking during evaluation, default value is True.
    - **pos_label**: If specified, the positive label to use when computing classification
      metrics such as precision, recall, f1, etc. for binary classification models. For
      multiclass classification and regression models, this parameter will be ignored.
    - **average**: The averaging method to use when computing classification metrics such as
      precision, recall, f1, etc. for multiclass classification models
      (default: ``'weighted'``). For binary classification and regression models, this
      parameter will be ignored.
    - **sample_weights**: Weights for each sample to apply when computing model performance
      metrics.
    - **col_mapping**: A dictionary mapping column names in the input dataset or output
      predictions to column names used when invoking the evaluation functions.
    - **retriever_k**: A parameter used when ``model_type="retriever"`` as the number of
      top-ranked retrieved documents to use when computing the built-in metric
      :mod:`precision_at_k(k) &lt;mlflow.metrics.precision_at_k&gt;`,
      :mod:`recall_at_k(k) &lt;mlflow.metrics.recall_at_k&gt;` and
      :mod:`ndcg_at_k(k) &lt;mlflow.metrics.ndcg_at_k&gt;`. Default value is 3. For all other
      model types, this parameter will be ignored.
 - Limitations of evaluation dataset:
    - For classification tasks, dataset labels are used to infer the total number of classes.
    - For binary classification tasks, the negative label value must be 0 or -1 or False, and
      the positive label value must be 1 or True.
 - Limitations of metrics/artifacts computation:
    - For classification tasks, some metric and artifact computations require the model to
      output class probabilities. Currently, for scikit-learn models, the default evaluator
      calls the ``predict_proba`` method on the underlying model to obtain probabilities. For
      other model types, the default evaluator does not compute metrics/artifacts that require
      probability outputs.
 - Limitations of default evaluator logging model explainability insights:
    - The ``shap.Explainer`` ``auto`` algorithm uses the ``Linear`` explainer for linear models
      and the ``Tree`` explainer for tree models. Because SHAP's ``Linear`` and ``Tree``
      explainers do not support multi-class classification, the default evaluator falls back to
      using the ``Exact`` or ``Permutation`` explainers for multi-class classification tasks.
    - Logging model explainability insights is not currently supported for PySpark models.
    - The evaluation dataset label values must be numeric or boolean, all feature values
      must be numeric, and each feature column must only contain scalar values.
 - Limitations when environment restoration is enabled:
    - When environment restoration is enabled for the evaluated model (i.e. a non-local
      ``env_manager`` is specified), the model is loaded as a client that invokes a MLflow
      Model Scoring Server process in an independent Python environment with the model's
      training time dependencies installed. As such, methods like ``predict_proba`` (for
      probability outputs) or ``score`` (computes the evaluation criterian for sklearn models)
      of the model become inaccessible and the default evaluator does not compute metrics or
      artifacts that require those methods.
    - Because the model is an MLflow Model Server process, SHAP explanations are slower to
      compute. As such, model explainaibility is disabled when a non-local ``env_manager``
      specified, unless the ``evaluator_config`` option **log_model_explainability** is
      explicitly set to ``True``.
Args:
    model: Optional. If specified, it should be one of the following:
        - A pyfunc model instance
        - A URI referring to a pyfunc model
        - A URI referring to an MLflow Deployments endpoint e.g. ``"endpoints:/my-chat"``
        - A callable function: This function should be able to take in model input and
          return predictions. It should follow the signature of the
          :py:func:`predict &lt;mlflow.pyfunc.PyFuncModel.predict&gt;` method. Here's an example
          of a valid function:
          ..code-block:: python
              model = mlflow.pyfunc.load_model(model_uri)
              def fn(model_input):
                  return model.predict(model_input)
        If omitted, it indicates a static dataset will be used for evaluation instead of a
        model.  In this case, the ``data`` argument must be a Pandas DataFrame or an mlflow
        PandasDataset that contains model outputs, and the ``predictions`` argument must be the
        name of the column in ``data`` that contains model outputs.
    data: One of the
        following:
        - A numpy array or list of evaluation features, excluding labels.
        - A Pandas DataFrame containing evaluation features, labels, and optionally model
            outputs. Model outputs are required to be provided when model is unspecified.
            If ``feature_names`` argument not specified, all columns except for the label
            column and model_output column are regarded as feature columns. Otherwise,
            only column names present in ``feature_names`` are regarded as feature columns.
        -  A Spark DataFrame containing evaluation features and labels. If
            ``feature_names`` argument not specified, all columns except for the label
            column are regarded as feature columns. Otherwise, only column names present in
            ``feature_names`` are regarded as feature columns. Only the first 10000 rows in
            the Spark DataFrame will be used as evaluation data.
        - A :py:class:`mlflow.data.dataset.Dataset` instance containing evaluation
            features, labels, and optionally model outputs. Model outputs are only supported
            with a PandasDataset. Model outputs are required when model is unspecified, and
            should be specified via the ``predictions`` prerty of the PandasDataset.
    targets: If ``data`` is a numpy array or list, a numpy array or list of evaluation
        labels. If ``data`` is a DataFrame, the string name of a column from ``data``
        that contains evaluation labels. Required for classifier and regressor models,
        but optional for question-answering, text-summarization, and text models. If
        ``data`` is a :py:class:`mlflow.data.dataset.Dataset` that defines targets,
        then ``targets`` is optional.
    predictions: Optional. The name of the column that contains model outputs.
        - When ``model`` is specified and outputs multiple columns, ``predictions`` can be used
          to specify the name of the column that will be used to store model outputs for
          evaluation.
        - When ``model`` is not specified and ``data`` is a pandas dataframe,
          ``predictions`` can be used to specify the name of the column in ``data`` that
          contains model outputs.
        .. code-block:: python
            :caption: Example usage of predictions
            # Evaluate a model that outputs multiple columns
            data = pd.DataFrame({"question": ["foo"]})
            def model(inputs):
                return pd.DataFrame({"answer": ["bar"], "source": ["baz"]})
            results = evaluate(model=model, data=data, predictions="answer", ...)
            # Evaluate a static dataset
            data = pd.DataFrame({"question": ["foo"], "answer": ["bar"], "source": ["baz"]})
            results = evaluate(data=data, predictions="answer", ...)
    model_type: (Optional) A string describing the model type. The default evaluator
        supports the following model types:
        - ``'classifier'``
        - ``'regressor'``
        - ``'question-answering'``
        - ``'text-summarization'``
        - ``'text'``
        - ``'retriever'``
        If no ``model_type`` is specified, then you must provide a a list of
        metrics to compute via the ``extra_metrics`` param.
        .. note::
            ``'question-answering'``, ``'text-summarization'``, ``'text'``, and
            ``'retriever'`` are experimental and may be changed or removed in a
            future release.
    inference_params: (Optional) A dictionary of inference parameters to be passed to the model
        when making predictions, such as ``{"max_tokens": 100}``. This is only used when
        the ``model`` is an MLflow Deployments endpoint URI e.g. ``"endpoints:/my-chat"``
    dataset_path: (Optional) The path where the data is stored. Must not contain double
        quotes (``“``). If specified, the path is logged to the ``mlflow.datasets``
        tag for lineage tracking purposes.
    feature_names: (Optional) A list. If the ``data`` argument is a numpy array or list,
        ``feature_names`` is a list of the feature names for each feature. If
        ``feature_names=None``, then the ``feature_names`` are generated using the
        format ``feature_{feature_index}``. If the ``data`` argument is a Pandas
        DataFrame or a Spark DataFrame, ``feature_names`` is a list of the names
        of the feature columns in the DataFrame. If ``feature_names=None``, then
        all columns except the label column and the predictions column are
        regarded as feature columns.
    evaluators: The name of the evaluator to use for model evaluation, or a list of
        evaluator names. If unspecified, all evaluators capable of evaluating the
        specified model on the specified dataset are used. The default evaluator
        can be referred to by the name ``"default"``. To see all available
        evaluators, call :py:func:`mlflow.models.list_evaluators`.
    evaluator_config: A dictionary of additional configurations to supply to the evaluator.
        If multiple evaluators are specified, each configuration should be
        supplied as a nested dictionary whose key is the evaluator name.
    extra_metrics:
        (Optional) A list of :py:class:`EvaluationMetric &lt;mlflow.models.EvaluationMetric&gt;`
        objects.  These metrics are computed in addition to the default metrics associated with
        pre-defined `model_type`, and setting `model_type=None` will only compute the metrics
        specified in `extra_metrics`. See the `mlflow.metrics` module for more information about
        the builtin metrics and how to define extra metrics.
        .. code-block:: python
            :caption: Example usage of extra metrics
            import mlflow
            import numpy as np
            def root_mean_squared_error(eval_df, _builtin_metrics):
                return np.sqrt((np.abs(eval_df["prediction"] - eval_df["target"]) ** 2).mean)
            rmse_metric = mlflow.models.make_metric(
                eval_fn=root_mean_squared_error,
                greater_is_better=False,
            )
            mlflow.evaluate(..., extra_metrics=[rmse_metric])
    custom_artifacts:
        (Optional) A list of custom artifact functions with the following signature:
        .. code-block:: python
            def custom_artifact(
                eval_df: Union[pandas.Dataframe, pyspark.sql.DataFrame],
                builtin_metrics: Dict[str, float],
                artifacts_dir: str,
            ) -&gt; Dict[str, Any]:
                """
                Args:
                    eval_df:
                        A Pandas or Spark DataFrame containing ``prediction`` and ``target``
                        column.  The ``prediction`` column contains the predictions made by the
                        model.  The ``target`` column contains the corresponding labels to the
                        predictions made on that row.
                    builtin_metrics:
                        A dictionary containing the metrics calculated by the default evaluator.
                        The keys are the names of the metrics and the values are the scalar
                        values of the metrics. Refer to the DefaultEvaluator behavior section
                        for what metrics will be returned based on the type of model (i.e.
                        classifier or regressor).
                    artifacts_dir:
                        A temporary directory path that can be used by the custom artifacts
                        function to temporarily store produced artifacts. The directory will be
                        deleted after the artifacts are logged.
                Returns:
                    A dictionary that maps artifact names to artifact objects
                    (e.g. a Matplotlib Figure) or to artifact paths within ``artifacts_dir``.
                """
                ...
        Object types that artifacts can be represented as:
            - A string uri representing the file path to the artifact. MLflow will infer the
              type of the artifact based on the file extension.
            - A string representation of a JSON object. This will be saved as a .json artifact.
            - Pandas DataFrame. This will be resolved as a CSV artifact.
            - Numpy array. This will be saved as a .npy artifact.
            - Matplotlib Figure. This will be saved as an image artifact. Note that
              ``matplotlib.pyplot.savefig`` is called behind the scene with default
              configurations.
              To customize, either save the figure with the desired configurations and return
              its file path or define customizations through environment variables in
              ``matplotlib.rcParams``.
            - Other objects will be attempted to be pickled with the default protocol.
        .. code-block:: python
            :caption: Example usage of custom artifacts
            import mlflow
            import matplotlib.pyplot as plt
            def scatter_plot(eval_df, builtin_metrics, artifacts_dir):
                plt.scatter(eval_df["prediction"], eval_df["target"])
                plt.xlabel("Targets")
                plt.ylabel("Predictions")
                plt.title("Targets vs. Predictions")
                plt.savefig(os.path.join(artifacts_dir, "example.png"))
                plt.close()
                return {"pred_target_scatter": os.path.join(artifacts_dir, "example.png")}
            def pred_sample(eval_df, _builtin_metrics, _artifacts_dir):
                return {"pred_sample": pred_sample.head(10)}
            mlflow.evaluate(..., custom_artifacts=[scatter_plot, pred_sample])
    validation_thresholds: (Optional) A dictionary of metric name to
        :py:class:`mlflow.models.MetricThreshold` used for model validation. Each metric name
        must either be the name of a builtin metric or the name of a metric defined in the
        ``extra_metrics`` parameter.
        .. code-block:: python
            :caption: Example of Model Validation
            from mlflow.models import MetricThreshold
            thresholds = {
                "accuracy_score": MetricThreshold(
                    # accuracy should be &gt;=0.8
                    threshold=0.8,
                    # accuracy should be at least 5 percent greater than baseline model accuracy
                    min_absolute_change=0.05,
                    # accuracy should be at least 0.05 greater than baseline model accuracy
                    min_relative_change=0.05,
                    greater_is_better=True,
                ),
            }
            with mlflow.start_run():
                mlflow.evaluate(
                    model=your_candidate_model,
                    data,
                    targets,
                    model_type,
                    dataset_name,
                    evaluators,
                    validation_thresholds=thresholds,
                    baseline_model=your_baseline_model,
                )
        See :ref:`the Model Validation documentation &lt;model-validation&gt;`
        for more details.
    baseline_model: (Optional) A string URI referring to an MLflow model with the pyfunc
        flavor. If specified, the candidate ``model`` is compared to this
        baseline for model validation purposes.
    env_manager: Specify an environment manager to load the candidate ``model`` and
        ``baseline_model`` in isolated Python environments and restore their
        dependencies. Default value is ``local``, and the following values are
        supported:
        - ``virtualenv``: (Recommended) Use virtualenv to restore the python
          environment that was used to train the model.
        - ``conda``:  Use Conda to restore the software environment that was used
          to train the model.
        - ``local``: Use the current Python environment for model inference, which
          may differ from the environment used to train the model and may lead to
          errors or invalid predictions.
    model_config: the model configuration to use for loading the model with pyfunc. Inspect
        the model's pyfunc flavor to know which keys are supported for your
        specific model. If not indicated, the default model configuration
        from the model is used (if any).
    baseline_config: the model configuration to use for loading the baseline
        model. If not indicated, the default model configuration
        from the baseline model is used (if any).
Returns:
    An :py:class:`mlflow.models.EvaluationResult` instance containing
    metrics of candidate model and baseline model, and artifacts of candidate model.
<span class="ansi-red-fg">File:</span>      /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/mlflow/models/evaluation/base.py
<span class="ansi-red-fg">Type:</span>      function</pre>
</div>
</div>
</div>
<div id="bb1c2e49-43a8-4ed2-9c04-80393d46f03c" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>mlflow.data.from_pandas?</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">Signature:</span>
mlflow<span class="ansi-blue-fg">.</span>data<span class="ansi-blue-fg">.</span>from_pandas<span class="ansi-blue-fg">(</span>
    df<span class="ansi-blue-fg">:</span> pandas<span class="ansi-blue-fg">.</span>core<span class="ansi-blue-fg">.</span>frame<span class="ansi-blue-fg">.</span>DataFrame<span class="ansi-blue-fg">,</span>
    source<span class="ansi-blue-fg">:</span> Union<span class="ansi-blue-fg">[</span>str<span class="ansi-blue-fg">,</span> mlflow<span class="ansi-blue-fg">.</span>data<span class="ansi-blue-fg">.</span>dataset_source<span class="ansi-blue-fg">.</span>DatasetSource<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    targets<span class="ansi-blue-fg">:</span> Optional<span class="ansi-blue-fg">[</span>str<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    name<span class="ansi-blue-fg">:</span> Optional<span class="ansi-blue-fg">[</span>str<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    digest<span class="ansi-blue-fg">:</span> Optional<span class="ansi-blue-fg">[</span>str<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    predictions<span class="ansi-blue-fg">:</span> Optional<span class="ansi-blue-fg">[</span>str<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
<span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">-&gt;</span> mlflow<span class="ansi-blue-fg">.</span>data<span class="ansi-blue-fg">.</span>pandas_dataset<span class="ansi-blue-fg">.</span>PandasDataset
<span class="ansi-red-fg">Docstring:</span>
Constructs a :py:class:`PandasDataset &lt;mlflow.data.pandas_dataset.PandasDataset&gt;` instance from
a Pandas DataFrame, optional targets, optional predictions, and source.
Args:
    df: A Pandas DataFrame.
    source: The source from which the DataFrame was derived, e.g. a filesystem
        path, an S3 URI, an HTTPS URL, a delta table name with version, or
        spark table etc. ``source`` may be specified as a URI, a path-like string,
        or an instance of
        :py:class:`DatasetSource &lt;mlflow.data.dataset_source.DatasetSource&gt;`.
        If unspecified, the source is assumed to be the code location
        (e.g. notebook cell, script, etc.) where
        :py:func:`from_pandas &lt;mlflow.data.from_pandas&gt;` is being called.
    targets: An optional target column name for supervised training. This column
        must be present in the dataframe (``df``).
    name: The name of the dataset. If unspecified, a name is generated.
    digest: The dataset digest (hash). If unspecified, a digest is computed
        automatically.
    predictions: An optional predictions column name for model evaluation. This column
        must be present in the dataframe (``df``).
.. code-block:: python
    :test:
    :caption: Example
    import mlflow
    import pandas as pd
    x = pd.DataFrame(
        [["tom", 10, 1, 1], ["nick", 15, 0, 1], ["juli", 14, 1, 1]],
        columns=["Name", "Age", "Label", "ModelOutput"],
    )
    dataset = mlflow.data.from_pandas(x, targets="Label", predictions="ModelOutput")
<span class="ansi-red-fg">File:</span>      /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/mlflow/data/pandas_dataset.py
<span class="ansi-red-fg">Type:</span>      function</pre>
</div>
</div>
</div>
<div id="7215a031-64d5-4239-923e-ab7a5479d057" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>pd_dataset.digest</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>'d4a94990'</code></pre>
</div>
</div>
</section>
</section>
<section id="try" class="level2">
<h2 class="anchored" data-anchor-id="try">Try</h2>
<ul>
<li>create_experiment followed by set_experiment</li>
</ul>
<p>https://mlflow.org/docs/latest/tracking.html#tracking-runs</p>
<p>https://learn.microsoft.com/en-us/azure/machine-learning/how-to-log-view-metrics?view=azureml-api-2&amp;tabs=interactive#log-images</p>
<p>https://www.databricks.com/notebooks/gallery/MLflowLoggingAPIPythonQuickstart.html</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/JaumeAmoresDS\.github\.io\/home\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="beatrizmilz/blog-en" data-repo-id="R_kgDOHc0DoQ" data-category="General" data-category-id="DIC_kwDOHc0Doc4CPeUR" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Copyright 2023 Jaume Amores.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>