{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13ba0d15-39de-4d1d-a5dc-b85c7b2eb26d",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f581906-f364-44b7-9b35-3d455fc606d5",
   "metadata": {},
   "source": [
    "## simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d5948b-c3f5-467c-9cff-0b49a39465cc",
   "metadata": {},
   "source": [
    "### Things to change wrt hello-world"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b610504b-c9fc-4975-b925-1b8247d7b9be",
   "metadata": {},
   "source": [
    "- conda env includes current module, with setup.py and settings.ini from nbdev\n",
    "- The pipeline is run from a `python_scripts` folder. \n",
    "- The conda env is in root folder.\n",
    "- The config in a `configs` folder. \n",
    "- The components are in `mylib/aml` folder.\n",
    "- We add a docker file that copies files such as setup.py, settings.ini, data, wheels, etc. (everything needed by the component scripts, which is basically everything copied to simulation folder, except for the json file used for config of how the pipeline is built (e.g., the one indicating the name of the environment, etc.)\n",
    "   - We are going to try two methods: one based on python image, and another based on aml image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1240fed1-8e4a-4a55-a707-f3c437af0eec",
   "metadata": {},
   "source": [
    "### Copying files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e371ed4-64ee-4dda-97c9-0199a44eeef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "os.makedirs ('simulation/python_scripts', exist_ok=True)\n",
    "os.makedirs ('simulation/my_lib/aml', exist_ok=True)\n",
    "os.makedirs ('simulation/configs', exist_ok=True)\n",
    "os.makedirs ('simulation/data', exist_ok=True)\n",
    "\n",
    "# shutil.copy ('./hello_world.yml', 'simulation') => a different one will be created \n",
    "shutil.copy ('./pipeline_input.json', 'simulation/configs')\n",
    "\n",
    "shutil.copy ('preprocessing/preprocessing.py', 'simulation/my_lib/aml')\n",
    "shutil.copy ('training/training.py', 'simulation/my_lib/aml')\n",
    "shutil.copy ('inference/inference.py', 'simulation/my_lib/aml')\n",
    "\n",
    "shutil.copy ('data/dummy_input.csv', 'simulation/data')\n",
    "shutil.copy ('data/dummy_test.csv', 'simulation/data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc85e549-7f38-44d9-92cd-dfa795f73795",
   "metadata": {},
   "source": [
    "### Copying settings.ini and setup.py from nbdev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1323fe53-e178-47c4-83a2-0f09bbabf889",
   "metadata": {},
   "source": [
    "```bash\n",
    "cd ../../..\n",
    "git clone https://github.com/fastai/nbdev.git\n",
    "cp nbdev/settings.ini home/posts/data_science/simulation\n",
    "cp nbdev/setup.py home/posts/data_science/simulation\n",
    "cd home/posts/data_science/simulation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93d09ca-37e5-42be-9923-af479704b1e8",
   "metadata": {},
   "source": [
    "### C|hanging settings.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec2d589-8d8c-450d-aa24-f0e559f66510",
   "metadata": {},
   "source": [
    "Edit the `settings.ini` file and replace the following entries as follows:\n",
    "\n",
    "```\n",
    "lib_name = my_lib\n",
    "repo = simulation\n",
    "requirements = pandas\n",
    "               scikit-learn\n",
    "               numpy\n",
    "dev_requirements = joblib\n",
    "                   azure-ai-ml\n",
    "lib_path = my_lib\n",
    "```\n",
    "\n",
    "Then remove the following entries:\n",
    "    \n",
    "```\n",
    "pip_requirements\n",
    "conda_requirements\n",
    "dev_requirements\n",
    "console_scripts\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b4fd07-a362-46dc-8a02-c3c88851ced4",
   "metadata": {},
   "source": [
    "### Changing the conda environment file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ef3f057-2d9a-498f-8e09-0c059ebdc7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting simulation/hello_world.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile simulation/hello_world.yml\n",
    "name: hello_world\n",
    "dependencies:\n",
    "    - python=3.10\n",
    "    - pip\n",
    "    - pip:\n",
    "        - -e .[dev]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143669cb-aac7-4b54-bd29-94f75c40b4ac",
   "metadata": {},
   "source": [
    "### Using custom docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc721d-8be5-44c1-8f95-496952d27e31",
   "metadata": {
    "tags": []
   },
   "source": [
    "I just searched in the list of curated environments present in my workspace, using the keyword `sklearn` in the search text box. At the time of writing this tutorial, the environment found is `sklearn-1.1:30`. By clicking on it, and then on the `Context` tab, we can read its dockerfile, which indicates, in the first line, the base docker image used: `FROM mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20240415.v1`.\n",
    "\n",
    "I found documentation about the docker image to be used by googling \"docker mcr.microsoft.com/azureml/openmpi4.1.0\", which provided the following URL:\n",
    "`https://hub.docker.com/_/microsoft-azureml`. In there, we can find additional links:\n",
    "- https://github.com/Azure/AzureML-Containers => contains docker files for each image\n",
    "    - Note: SDK code contained in this link is v1. \n",
    "\n",
    "We can also explore this docker image by running it in interactive mode (see a cheat sheet of docker commands in [here](https://docs.docker.com/get-started/docker_cheatsheet.pdf) and [here](https://dockerlabs.collabnix.com/docker/cheatsheet/)):\n",
    "```bash\n",
    "docker pull mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20240415.v1\n",
    "docker run -it --entrypoint bash mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20240415.v1\n",
    "```\n",
    "\n",
    "\n",
    "In [this tutorial](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-custom-container?view=azureml-api-2&tabs=cli) we can see how to test this docker image :\n",
    "\n",
    "```\n",
    "docker run --rm -d -v $PWD/$BASE_PATH:$MODEL_BASE_PATH -p 8501:8501 \\\n",
    " -e MODEL_BASE_PATH=$MODEL_BASE_PATH -e MODEL_NAME=$MODEL_NAME \\\n",
    " --name=\"tfserving-test\" docker.io/tensorflow/serving:latest\n",
    "sleep 10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2c881d-fdd0-4ce3-9c49-f996ad9c9b62",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Possibilities\n",
    "\n",
    "Either copy the dockerfile and replace the line:\n",
    "\n",
    "```dockerfile\n",
    "COPY conda_dependencies.yaml .\n",
    "```\n",
    "\n",
    "with:\n",
    "\n",
    "```dockerfile\n",
    "RUN mkdir -p data\n",
    "COPY <MY-CONDA-ENV-YAML> .\n",
    "COPY settings.ini .\n",
    "COPY setup.py .\n",
    "COPY data/dummy_input.csv data/\n",
    "COPY data/dummy_test.csv data/\n",
    "COPY data/dummy_test.csv data/\n",
    "```\n",
    "\n",
    "and change the name `conda_dependencies.yaml` with <MY-CONDA-ENV-YAML> everywhere else in the file...\n",
    "\n",
    "or name your conda env file `conda_dependencies.yaml`:\n",
    "```bash\n",
    "mv simulation/hello_world.yml simulation/conda_dependencies.yaml\n",
    "```\n",
    "\n",
    "and use the curated docker image as base image in your dockerfile:\n",
    "\n",
    "```dockerfile\n",
    "FROM mcr.microsoft.com/azureml/curated/sklearn-1.1:30\n",
    "\n",
    "RUN mkdir -p data\n",
    "COPY settings.ini .\n",
    "COPY setup.py .\n",
    "COPY data/dummy_input.csv data/\n",
    "COPY data/dummy_test.csv data/\n",
    "COPY data/dummy_test.csv data/\n",
    "```\n",
    "    \n",
    "Note that we can dedicate a specific folder for all the files that need to be copied and used in the Dockerfile, including `conda_dependencies.yaml`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3947ec3f-4c03-4b4c-a7ab-9a763b4cf576",
   "metadata": {},
   "source": [
    "#### Final Dokerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae98c86-124b-4876-91dd-5290c6da915d",
   "metadata": {},
   "source": [
    "From the two possibilities mentioned above, we use the first one, which is more modular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a6d27a7-754c-4322-a4fd-0e8e1ebe4cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/jaumecpu/code/Users/jau.m/home/posts/data_science\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab92b7d9-38a2-49fb-b5da-3011fdfe1f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat 'simulation/hello_world.yml': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "mv simulation/hello_world.yml simulation/conda_dependencies.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a280b089-1119-4583-aeb1-75a5f4c3ccf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting simulation/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile simulation/Dockerfile\n",
    "FROM mcr.microsoft.com/azureml/curated/sklearn-1.1:30\n",
    "\n",
    "RUN mkdir -p data\n",
    "COPY settings.ini .\n",
    "COPY setup.py .\n",
    "COPY data/dummy_input.csv data/\n",
    "COPY data/dummy_test.csv data/\n",
    "COPY data/dummy_test.csv data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29eedc64-9665-464a-9b0e-2aa61f03e3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile simulation/Dockerfile\n",
    "FROM mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20240415.v1\n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "ENV CONDA_PREFIX=/azureml-envs/sklearn-1.1\n",
    "ENV CONDA_DEFAULT_ENV=$CONDA_PREFIX\n",
    "ENV PATH=$CONDA_PREFIX/bin:$PATH\n",
    "\n",
    "# This is needed for mpi to locate libpython\n",
    "ENV LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "# Create conda environment\n",
    "RUN mkdir -p data\n",
    "COPY hello_world.yml .\n",
    "COPY settings.ini .\n",
    "COPY setup.py .\n",
    "COPY data/dummy_input.csv data/\n",
    "COPY data/dummy_test.csv data/\n",
    "COPY data/dummy_test.csv data/\n",
    "RUN conda env create -p $CONDA_PREFIX -f conda_dependencies.yaml -q && \\\n",
    "    rm conda_dependencies.yaml && \\\n",
    "    conda run -p $CONDA_PREFIX pip cache purge && \\\n",
    "    conda clean -a -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23df602-693e-4e20-85f9-f01ee0b3a81a",
   "metadata": {},
   "source": [
    "#### Testing docker "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41b3fde-dd8e-4b88-ac00-7bf277e891a1",
   "metadata": {},
   "source": [
    "```bash\n",
    "#docker pull mcr.microsoft.com/azureml/curated/sklearn-1.1:30\n",
    "docker build -t hello_world .\n",
    "docker run -v ~/cloudfiles/code/Users/jau.m/home/posts/data_science/simulation/:/host_dir -it --entrypoint bash hello_world\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750c3056-c729-4850-a986-8028c9db7a32",
   "metadata": {},
   "source": [
    "Let's try running the first job of the pipeline function: `preprocessing_training_job`.For this, we first look how the script needs to be run from command line, as indicated in the `command` call of the `preprocessing` component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b01224f-283f-4160-a12a-3ecf8ac69d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "        command=\"python preprocessing.py \"\n",
    "            \"--input_file ${{inputs.input_file}} \"\n",
    "            \"-x ${{inputs.x}} \"\n",
    "            \"--output_folder ${{outputs.output_folder}} \"\n",
    "            \"--output_filename ${{inputs.output_filename}}\","
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5428039-e421-46b1-a422-838708a8431d",
   "metadata": {},
   "source": [
    "Then, in order to see what the inputs `inputs.input_file`, `ìnputs.x` and `inputs.output_filename` are, we look at how the `preprocessing_training_job` is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc4635a-47e6-4c98-9663-151517fb11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    preprocessing_training_job = preprocessing_component(\n",
    "        input_file=preprocessing_training_input_file,\n",
    "        #output_folder: automatically determined\n",
    "        output_filename=preprocessing_training_output_filename,\n",
    "        x=x,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e3f06d-bd97-4113-8db8-348af98c79f3",
   "metadata": {},
   "source": [
    "and, in order to fill in those values we look at the ones passed to the pipeline function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b629994d-b9f9-4fbb-90e5-08041bf771af",
   "metadata": {},
   "outputs": [],
   "source": [
    "    three_components_pipeline_object = three_components_pipeline(\n",
    "        # first preprocessing component\n",
    "        preprocessing_training_input_file=Input(type=\"uri_file\", path=config.preprocessing_training_input_file),\n",
    "        preprocessing_training_output_filename=config.preprocessing_training_output_filename,\n",
    "        x=config.x,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b057617-0845-4812-bdeb-3a91cbf9dcc9",
   "metadata": {},
   "source": [
    "If we just replace those, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da625f2c-d338-41f9-b338-83ae480acee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "python preprocessing.py --input config.preprocessing_training_input_file config.preprocessing_training_output_filename -x config.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3c8a5d-1359-4477-a7ab-9efe2ef23aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "These values are given in the config file, so we visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a79d8963-ad99-4396-a589-344861a09dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"preprocessing_training_input_file\": \"./data/dummy_input.csv\",\n",
      "    \"preprocessing_training_output_filename\":\"preprocessed_training_data.csv\",\n",
      "    \"x\": 10,\n",
      "    \"preprocessing_test_input_file\": \"./data/dummy_test.csv\",\n",
      "    \"preprocessing_test_output_filename\": \"preprocessed_test_data.csv\",\n",
      "    \"training_output_filename\": \"model.pk\",\n",
      "    \"inference_output_filename\": \"inference_results.csv\",\n",
      "    \"experiment_name\": \"e2e_three_components_in_script\",\n",
      "    \"compute_name\": \"jaumecpu\",\n",
      "    \"image\": \"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n",
      "    \"conda_file\": \"./hello_world.yml\",\n",
      "    \"name_env\": \"hello-world\",\n",
      "    \"description_env\": \"Hello World\",\n",
      "    \"docker_context_path\": \".\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!cat configs/pipeline_input.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b332bd82-7d6f-4f0e-93cd-4dbc8259ee3d",
   "metadata": {},
   "source": [
    "With all this, we can put the pieces togeher. The names of the ouput folders are automatically generated by AML, and the folders automatically created. In our case, we will the name the ouput folder as `preprocessing_training_ouput_folder`, and create it before hand. We also have to copy the script `my_lib/aml/preprocessing.py` to current folder. Putting all together, we run the following in command line inside the docker container:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5601056c-b5ba-43f0-851e-5c887df8bb1d",
   "metadata": {},
   "source": [
    "```bash\n",
    "cp host_dir/my_lib/aml/preprocessing.py .\n",
    "mkdir preprocessing_training_ouput_folder\n",
    "python preprocessing.py --input ./data/dummy_input.csv --output_folder preprocessing_training_ouput_folder --output_filename preprocessed_training_data.csv -x 10 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a641de-3a53-4b7c-91a7-c018f66f9737",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Change aml_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4a8aeb-c093-49a2-8f1c-5468fefb6e1c",
   "metadata": {},
   "source": [
    "In order to use a custom docker image, we need to use a different way of creating the environment:\n",
    "    \n",
    "```python\n",
    "env = Environment(\n",
    "    build=BuildContext(path=docker_context_path),\n",
    "    name=name_env,\n",
    "    description=description_env,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c3f977-81e3-47e5-82d1-e9a0195067f7",
   "metadata": {},
   "source": [
    "This change affects the function `create_env` in `aml_utils.py`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e95cf8-f83e-406a-9253-632bed125c86",
   "metadata": {},
   "source": [
    "```python\n",
    "def create_env (\n",
    "    ml_client,\n",
    "    image: str=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n",
    "    conda_file: str=\"./pipeline.yml\",\n",
    "    name_env: str=\"pipeline\",\n",
    "    description_env: str=\"Pipeline environment\",\n",
    "    docker_context_path=None,\n",
    "):\n",
    "    if docker_context_path is None:\n",
    "        \"Creates environment in AML workspace\"\n",
    "        env = Environment (\n",
    "            image=image,\n",
    "            conda_file=conda_file,\n",
    "            name=name_env,\n",
    "            description=description_env,\n",
    "        )\n",
    "    else:\n",
    "        env = Environment(\n",
    "            build=BuildContext(path=docker_context_path),\n",
    "            name=name_env,\n",
    "            description=description_env,\n",
    "        )\n",
    "    ml_client.environments.create_or_update (env)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8623ec4-cc49-4fba-bfdd-bdff70a1566a",
   "metadata": {},
   "source": [
    "The change also affects functions that call `create_env` (`connect_setup_and_run` in `aml_utils.py`, and `run_pipeline` in `hello_world_pipeline.py`), since they need to pass the additional parameter `docker_context_path`. We also need to import `BuildContext` from `azure.ai.ml.entities`.\n",
    "With these changes, the complete `aml_utils.py` file is as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca34680b-70a4-49e3-bd1d-a8af36116f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting aml_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile aml_utils.py\n",
    "# Standard imports\n",
    "import json\n",
    "\n",
    "# Third-party imports\n",
    "from sklearn.utils import Bunch\n",
    "\n",
    "# AML imports\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import Environment, BuildContext\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "def connect ():\n",
    "    \"\"\"Connects to Azure ML workspace and returns a handle to use it.\"\"\"\n",
    "    # authenticate\n",
    "    credential = DefaultAzureCredential()\n",
    "\n",
    "    # Get a handle to the workspace\n",
    "    ml_client = MLClient.from_config (\n",
    "        credential=credential,\n",
    "    )\n",
    "    return ml_client\n",
    "\n",
    "def create_env (\n",
    "    ml_client,\n",
    "    image: str=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n",
    "    conda_file: str=\"./pipeline.yml\",\n",
    "    name_env: str=\"pipeline\",\n",
    "    description_env: str=\"Pipeline environment\",\n",
    "    docker_context_path=None,\n",
    "):\n",
    "    if docker_context_path is None:\n",
    "        \"Creates environment in AML workspace\"\n",
    "        env = Environment (\n",
    "            image=image,\n",
    "            conda_file=conda_file,\n",
    "            name=name_env,\n",
    "            description=description_env,\n",
    "        )\n",
    "    else:\n",
    "        env = Environment(\n",
    "            build=BuildContext(path=docker_context_path),\n",
    "            name=name_env,\n",
    "            description=description_env,\n",
    "        )\n",
    "    ml_client.environments.create_or_update (env)\n",
    "    \n",
    "def connect_setup_and_run (\n",
    "    pipeline_object, \n",
    "    experiment_name: str=\"pipeline experiment\",\n",
    "    compute_name: str=\"jaumecpu\",\n",
    "    image: str=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n",
    "    conda_file: str=\"./pipeline.yml\",\n",
    "    name_env: str=\"pipeline\",\n",
    "    description_env: str=\"Pipeline environment\",\n",
    "    docker_context_path=None,\n",
    "):\n",
    "    \"\"\"Does all the setup required to run the pipeline.\n",
    "    \n",
    "    This includes: connecting, creating environment, indicating our compute instance,\n",
    "    creating and running the pipeline.\n",
    "    \"\"\"\n",
    "    # connect\n",
    "    ml_client = connect ()\n",
    "\n",
    "    # create env\n",
    "    create_env (\n",
    "        ml_client,\n",
    "        image=image,\n",
    "        conda_file=conda_file,\n",
    "        name_env=name_env,\n",
    "        description_env=description_env,\n",
    "        docker_context_path=docker_context_path,\n",
    "    )\n",
    "\n",
    "    # compute\n",
    "    pipeline_object.settings.default_compute = compute_name \n",
    "\n",
    "    # create pipeline and run\n",
    "    pipeline_job = ml_client.jobs.create_or_update(\n",
    "        pipeline_object,\n",
    "        # Project's name\n",
    "        experiment_name=experiment_name,\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # Pipeline running\n",
    "    # ----------------------------------------------------\n",
    "    ml_client.jobs.stream(pipeline_job.name)\n",
    "\n",
    "def read_config (config_path: str):\n",
    "    # Read config json file\n",
    "    with open (config_path,\"rt\") as config_file:\n",
    "        config = json.load (config_file)\n",
    "\n",
    "    config = Bunch (**config)\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "354498bd-4f7a-402f-b2a5-7d5d5cec1290",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp aml_utils.py simulation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184f8eb0-710a-446c-921b-c456c997f29c",
   "metadata": {},
   "source": [
    "[AML documentation](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-environments-v2?view=azureml-api-2&tabs=python#create-an-environment-from-a-docker-build-context)\n",
    "\n",
    "[Tutorial](https://medium.com/@luisdmonge/azure-dp-100-prep-hands-on-with-pytorch-and-azure-ml-sdk-v2-8ab9497eb88f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b5d75-f828-4697-bd7f-3e9d3c74e1a3",
   "metadata": {},
   "source": [
    "### Change config file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86b94e4-d4c2-4357-ad67-fb1143a703e9",
   "metadata": {},
   "source": [
    "Add the following line to the previous config file: `\"docker_context_path\": \".\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04f74c30-3631-4db6-9a86-332915c9e1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting configs/pipeline_input.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile configs/pipeline_input.json\n",
    "{\n",
    "    \"preprocessing_training_input_file\": \"./data/dummy_input.csv\",\n",
    "    \"preprocessing_training_output_filename\":\"preprocessed_training_data.csv\",\n",
    "    \"x\": 10,\n",
    "    \"preprocessing_test_input_file\": \"./data/dummy_test.csv\",\n",
    "    \"preprocessing_test_output_filename\": \"preprocessed_test_data.csv\",\n",
    "    \"training_output_filename\": \"model.pk\",\n",
    "    \"inference_output_filename\": \"inference_results.csv\",\n",
    "    \"experiment_name\": \"e2e_three_components_in_script\",\n",
    "    \"compute_name\": \"jaumecpu\",\n",
    "    \"image\": \"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n",
    "    \"conda_file\": \"./hello_world.yml\",\n",
    "    \"name_env\": \"hello-world\",\n",
    "    \"description_env\": \"Hello World\",\n",
    "    \"docker_context_path\": \".\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "847cab78-613f-455d-8ab2-ac55fa2af5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'configs/untitled.txt': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "rm configs/untitled.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b870f98-ab14-4cfa-a2b3-46d011606122",
   "metadata": {},
   "source": [
    "No need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b584c17-cff3-44f4-83e9-b629c175334a",
   "metadata": {},
   "source": [
    "### Change hello_world_pipeline.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1383e8-8407-4f61-8f44-9e4df988b3b9",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc5155d-7668-4d40-8c1f-e98099b31096",
   "metadata": {},
   "source": [
    "Same imports section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19d524ee-98f0-4dcb-b3c7-f306a805fe47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hello_world_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile simulation/hello_world_pipeline.py\n",
    "# Standard imports\n",
    "import argparse\n",
    "\n",
    "# AML imports\n",
    "from azure.ai.ml import (\n",
    "    command,\n",
    "    dsl,\n",
    "    Input,\n",
    "    Output,\n",
    ")\n",
    "\n",
    "# Utility functions\n",
    "from aml_utils import (\n",
    "    connect,\n",
    "    create_env,\n",
    "    connect_setup_and_run,\n",
    "    read_config,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6332ade4-f1b6-4c21-8b69-5674f4f1146a",
   "metadata": {},
   "source": [
    "#### Pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ddc3356-1888-4c51-8708-552679a3021c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to hello_world_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a simulation/hello_world_pipeline.py\n",
    "@dsl.pipeline(\n",
    "    description=\"Simulation hello-world\",\n",
    ")\n",
    "def three_components_pipeline(\n",
    "    # Preprocessing component parameters, first component:\n",
    "    preprocessing_training_input_file: str,\n",
    "    preprocessing_training_output_filename: str,\n",
    "    x: int,\n",
    "    \n",
    "    # Preprocessing component parameters, second component:\n",
    "    preprocessing_test_input_file: str,\n",
    "    preprocessing_test_output_filename: str,\n",
    "    \n",
    "    # Training component parameters:\n",
    "    training_output_filename: str, \n",
    "    \n",
    "    # Inference component parameters:\n",
    "    inference_output_filename: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Third pipeline: preprocessing, training and inference.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    preprocessing_training_input_file: str\n",
    "        Path to file containing training data to be preprocessed.\n",
    "    preprocessing_training_output_filename: str\n",
    "        Name of file containing the preprocessed, training data.\n",
    "    x: int\n",
    "        Number to add to input data for preprocessing it.\n",
    "    preprocessing_test_input_file: str\n",
    "        Path to file containing test data to be preprocessed.\n",
    "    preprocessing_test_output_filename: str\n",
    "        Name of file containing the preprocessed, test data.\n",
    "    training_output_filename: str\n",
    "        Name of file containing the trained model.\n",
    "    inference_output_filename: str\n",
    "        Name of file containing the output data with inference results.\n",
    "    \"\"\"\n",
    "        \n",
    "    # -------------------------------------------------------------------------------------\n",
    "    # Preprocessing\n",
    "    # -------------------------------------------------------------------------------------\n",
    "    # Interface\n",
    "    preprocessing_component = command(\n",
    "        inputs=dict(\n",
    "            input_file=Input (type=\"uri_file\"),\n",
    "            x=Input (type=\"number\"),\n",
    "            output_filename=Input (type=\"string\"),\n",
    "        ),\n",
    "        outputs=dict(\n",
    "            output_folder=Output (type=\"uri_folder\"),\n",
    "        ),\n",
    "        code=f\"./my_lib/aml/\",  # location of source code: in this case, the root folder\n",
    "        command=\"python preprocessing.py \"\n",
    "            \"--input_file ${{inputs.input_file}} \"\n",
    "            \"-x ${{inputs.x}} \"\n",
    "            \"--output_folder ${{outputs.output_folder}} \"\n",
    "            \"--output_filename ${{inputs.output_filename}}\",\n",
    "        environment=\"hello-world@latest\",\n",
    "        display_name=\"Pre-processing\",\n",
    "    )\n",
    "\n",
    "    # Instantiation\n",
    "    preprocessing_training_job = preprocessing_component(\n",
    "        input_file=preprocessing_training_input_file,\n",
    "        #output_folder: automatically determined\n",
    "        output_filename=preprocessing_training_output_filename,\n",
    "        x=x,\n",
    "    )\n",
    "    preprocessing_test_job = preprocessing_component(\n",
    "        input_file=preprocessing_test_input_file,\n",
    "        #output_folder: automatically determined\n",
    "        output_filename=preprocessing_test_output_filename,\n",
    "        x=x,\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------------------\n",
    "    # Training component\n",
    "    # -------------------------------------------------------------------------------------\n",
    "    # Interface\n",
    "    training_component = command(\n",
    "        inputs=dict(\n",
    "            input_folder=Input (type=\"uri_folder\"),\n",
    "            input_filename=Input (type=\"string\"),\n",
    "            output_filename=Input (type=\"string\"),\n",
    "        ),\n",
    "        outputs=dict(\n",
    "            output_folder=Output (type=\"uri_folder\"),\n",
    "        ),\n",
    "        code=f\"./my_lib/aml/\",  # location of source code: in this case, the root folder\n",
    "        command=\"python training.py \"\n",
    "            \"--input_folder ${{inputs.input_folder}} \"\n",
    "            \"--input_filename ${{inputs.input_filename}} \"\n",
    "            \"--output_folder ${{outputs.output_folder}} \"\n",
    "            \"--output_filename ${{inputs.output_filename}}\",\n",
    "        environment=\"hello-world@latest\",\n",
    "        display_name=\"Training\",\n",
    "    )\n",
    "\n",
    "    # Instantiation\n",
    "    training_job = training_component(\n",
    "        input_folder=preprocessing_training_job.outputs.output_folder,\n",
    "        input_filename=preprocessing_training_output_filename,\n",
    "        #output_folder: automatically determined\n",
    "        output_filename=training_output_filename,\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------------------\n",
    "    # Inference\n",
    "    # -------------------------------------------------------------------------------------\n",
    "    # Interface\n",
    "    inference_component = command(\n",
    "        inputs=dict(\n",
    "            preprocessed_input_folder=Input (type=\"uri_folder\"),\n",
    "            preprocessed_input_filename=Input (type=\"string\"),\n",
    "            model_input_folder=Input (type=\"uri_folder\"),\n",
    "            model_input_filename=Input (type=\"string\"),\n",
    "            output_filename=Input (type=\"string\"),\n",
    "        ),\n",
    "        outputs=dict(\n",
    "            output_folder=Output (type=\"uri_folder\"),\n",
    "        ),\n",
    "        code=f\"./my_lib/aml/\",  # location of source code: in this case, the root folder\n",
    "        command=\"python inference.py \" \n",
    "            \"--preprocessed_input_folder ${{inputs.preprocessed_input_folder}} \"\n",
    "            \"--preprocessed_input_filename ${{inputs.preprocessed_input_filename}} \"\n",
    "            \"--model_input_folder ${{inputs.model_input_folder}} \"\n",
    "            \"--model_input_filename ${{inputs.model_input_filename}} \"\n",
    "            \"--output_folder ${{outputs.output_folder}} \"\n",
    "            \"--output_filename ${{inputs.output_filename}} \",\n",
    "\n",
    "        environment=\"hello-world@latest\",\n",
    "        display_name=\"inference\",\n",
    "    )\n",
    "\n",
    "    # Instantiation\n",
    "    inference_job = inference_component(\n",
    "        preprocessed_input_folder=preprocessing_test_job.outputs.output_folder,\n",
    "        preprocessed_input_filename=preprocessing_test_output_filename,\n",
    "        model_input_folder=training_job.outputs.output_folder,\n",
    "        model_input_filename=training_output_filename,\n",
    "        #output_folder: automatically determined\n",
    "        output_filename=inference_output_filename,\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af80497f-93db-4ee0-be17-3a757f6dcab7",
   "metadata": {},
   "source": [
    "#### Create and run pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30de813-47bd-40e3-a254-90558033f1aa",
   "metadata": {},
   "source": [
    "Next we define a function that both creates and runs the pipeline implemented above. This function performs all the steps implemented so far: it reads a config file, instantiates a pipeline object by calling our `three_components_pipeline` function, and finally performs the pipeline set-up and runs it by calling `connect_setup_and_run`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce64c95d-979a-4293-85d4-7c1dd1d2e24f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to hello_world_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a simulation/hello_world_pipeline.py\n",
    "def run_pipeline (\n",
    "    config_path: str=\"./pipeline_input.json\",\n",
    "    experiment_name=\"hello-world-experiment\",\n",
    "):\n",
    "    # read config\n",
    "    config = read_config (config_path)\n",
    "\n",
    "    # Build pipeline \n",
    "    three_components_pipeline_object = three_components_pipeline(\n",
    "        # first preprocessing component\n",
    "        preprocessing_training_input_file=Input(type=\"uri_file\", path=config.preprocessing_training_input_file),\n",
    "        preprocessing_training_output_filename=config.preprocessing_training_output_filename,\n",
    "        x=config.x,\n",
    "        \n",
    "        # second preprocessing component\n",
    "        preprocessing_test_input_file=Input(type=\"uri_file\", path=config.preprocessing_test_input_file),\n",
    "        preprocessing_test_output_filename=config.preprocessing_test_output_filename,\n",
    "        \n",
    "        # Training component parameters:\n",
    "        training_output_filename=config.training_output_filename,\n",
    "        \n",
    "        # Inference component parameters:\n",
    "        inference_output_filename=config.inference_output_filename,\n",
    "    )\n",
    "\n",
    "    connect_setup_and_run (\n",
    "        three_components_pipeline_object, \n",
    "        experiment_name=experiment_name,\n",
    "        compute_name=config.compute_name,\n",
    "        image=config.image,\n",
    "        conda_file=config.conda_file,\n",
    "        name_env=config.name_env,\n",
    "        description_env=config.description_env,\n",
    "        docker_context_path=config.docker_context_path,\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3576a083-ab55-4ef9-94e2-052ee1b55956",
   "metadata": {},
   "source": [
    "#### Parsing arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "895da1f5-3599-4157-a77c-854453e244ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to hello_world_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a simulation/hello_world_pipeline.py\n",
    "def parse_args ():\n",
    "    \"\"\"Parses input arguments\"\"\"\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument (\n",
    "        \"--config-path\", \n",
    "        type=str, \n",
    "        default=\"configs/pipeline_input.json\",\n",
    "        help=\"Path to config file specifying pipeline input parameters.\",\n",
    "    )\n",
    "    parser.add_argument (\n",
    "        \"--experiment-name\", \n",
    "        type=str, \n",
    "        default=\"simulation\",\n",
    "        help=\"Name of experiment.\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print (\"Running hello-world pipeline with args\", args)\n",
    "    \n",
    "    return args\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a07141c-4da1-436b-ae7b-8532dd581309",
   "metadata": {},
   "source": [
    "#### Main section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ca9ab74-0b06-48d6-879d-a58e0aaacaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to hello_world_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a simulation/hello_world_pipeline.py\n",
    "def main ():\n",
    "    \"\"\"Parses arguments and runs pipeline\"\"\"\n",
    "    args = parse_args ()\n",
    "    run_pipeline (\n",
    "        args.config_path,\n",
    "        args.experiment_name,\n",
    "    )\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# -------------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbad6c2b-b87d-4275-8228-b6ab6cc14d3b",
   "metadata": {},
   "source": [
    "### Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbb2fa08-883f-4a1a-bff0-ab3d81c7a680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/jaumecpu/code/Users/jau.m/home/posts/data_science/simulation\n"
     ]
    }
   ],
   "source": [
    "cd simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2184344d-dd29-42fc-8c34-6ab70e54f869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running hello-world pipeline with args Namespace(config_path='configs/pipeline_input.json', experiment_name='simulation')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n",
      "\u001b[32mUploading simulation (0.04 MBs): 100%|██████████| 43370/43370 [00:01<00:00, 38468.84it/s] \n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: upbeat_leg_dmp9bcvs9y\n",
      "Web View: https://ml.azure.com/runs/upbeat_leg_dmp9bcvs9y?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n",
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2024-04-22 16:10:08Z] Submitting 2 runs, first five are: b47b7c2a:96c70098-5988-4f87-82e5-533cf367757a,f0c6ed40:8447c9f9-a361-4615-961a-f9362407c3fe\n",
      "[2024-04-22 16:20:46Z] Completing processing run id 96c70098-5988-4f87-82e5-533cf367757a.\n",
      "[2024-04-22 16:20:46Z] Completing processing run id 8447c9f9-a361-4615-961a-f9362407c3fe.\n",
      "[2024-04-22 16:20:47Z] Submitting 1 runs, first five are: 0ef51f82:68e226d8-0c7a-4d40-ab80-df94e1eae12e\n",
      "[2024-04-22 16:21:09Z] Completing processing run id 68e226d8-0c7a-4d40-ab80-df94e1eae12e.\n",
      "[2024-04-22 16:21:10Z] Submitting 1 runs, first five are: 005c2297:9906cb00-9ecf-4b37-9a83-58942197aef9\n",
      "[2024-04-22 16:21:33Z] Completing processing run id 9906cb00-9ecf-4b37-9a83-58942197aef9.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: upbeat_leg_dmp9bcvs9y\n",
      "Web View: https://ml.azure.com/runs/upbeat_leg_dmp9bcvs9y?wsid=/subscriptions/6af6741b-f140-48c2-84ca-027a27365026/resourcegroups/helloworld/workspaces/helloworld\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run hello_world_pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c6c116-a519-4903-b065-66e5b37e5a0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf694155-11e3-4675-88e8-8e74f430b935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
